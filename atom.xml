<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Future Tech Blog - フューチャーアーキテクト</title>
  <subtitle>フューチャーアーキテクト開発者ブログ</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://future-architect.github.io/"/>
  <updated>2019-04-25T05:06:54.367Z</updated>
  <id>https://future-architect.github.io/</id>
  
  <author>
    <name>Future Architect Consultants</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Qiita Jobsを始めました</title>
    <link href="https://future-architect.github.io/articles/20190425/"/>
    <id>https://future-architect.github.io/articles/20190425/</id>
    <published>2019-04-25T04:43:19.000Z</published>
    <updated>2019-04-25T05:06:54.367Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Qiita-Jobsとは"><a href="#Qiita-Jobsとは" class="headerlink" title="Qiita Jobsとは"></a>Qiita Jobsとは</h1><p><a href="https://qiita.com/" target="_blank" rel="noopener">Qiita</a> で有名なIncrements社が2019年4月に発表した、エンジニアに特化した転職支援サービスです。<br>以下のようなコンセプトのようです。</p>
<blockquote>
<p>転職のミスマッチは、なぜ起こるのだろう。<br>同じ会社でも開発チームが違えば、一緒に働く人も、使う技術も、積める経験も違う。<br>ミスマッチは、会社ではなくチームで起こる。<br>だから Qiita Jobs では、会社ではなくチームを探す。採用担当ではなくチームメンバーと話す。人事ではなく自分が配属チームを決める。<br>自分に最適のチームで、最高の仕事をしよう。<br>(引用) <a href="https://jobs.qiita.com/" target="_blank" rel="noopener">https://jobs.qiita.com/</a></p>
</blockquote>
<h1 id="チーム紹介"><a href="#チーム紹介" class="headerlink" title="チーム紹介"></a>チーム紹介</h1><p>2019.04.25時点でフューチャーからは2チーム掲載しています✨<br>他にも、社内開発チームが準備を行っているので増えていく予定です。<br>ちなみに2チームとも、トップ画像は急遽準備したものなので今後はもっといい写真に差し替える予定です。</p>
<h2 id="コアテクノロジーチーム"><a href="#コアテクノロジーチーム" class="headerlink" title="コアテクノロジーチーム"></a>コアテクノロジーチーム</h2><p>フューチャー社内でトップレベルの技術集団です。<br>OSS活動も活発に行なっているので、興味がある方はぜひチェックください。</p>
<p><a href="https://jobs.qiita.com/employers/future/development_teams/57" target="_blank" rel="noopener">https://jobs.qiita.com/employers/future/development_teams/57</a></p>
<p><img src="/images/20190425/photo_20190425_01.png" style="border:solid 1px #CFD8DC"></p>
<h2 id="FutureIoTチーム"><a href="#FutureIoTチーム" class="headerlink" title="FutureIoTチーム"></a>FutureIoTチーム</h2><p>フューチャー社内でも、特にIoTやDX（Digital Transformation）のコンサルティングサービスを行っているチームです。<br>社内でも実績がない技術をドンドン採用するチームなので、チャレンジしたい人はぜひ連絡ください！</p>
<p><a href="https://jobs.qiita.com/employers/future/development_teams/109" target="_blank" rel="noopener">https://jobs.qiita.com/employers/future/development_teams/109</a></p>
<p><img src="/images/20190425/photo_20190425_02.png" style="border:solid 1px #CFD8DC"></p>
<h1 id="フューチャーとQiita"><a href="#フューチャーとQiita" class="headerlink" title="フューチャーとQiita"></a>フューチャーとQiita</h1><p>余談ですが、フューチャーとQiitaの関わりについて簡単にまとめました。</p>
<p>遡ること、<a href="https://qiita.com/organizations/future" target="_blank" rel="noopener">フューチャーのQiita Organization</a> を作ったのは2015年です。<br>当時のボスに「Organization を作って良い？」って聞いたら、「いいよ」ってさくっと回答がきたのが懐かしいです。<br>（一応、メンバーリストをGitで管理したり、リーガルとWikiに投稿規約をまとめたり、会社の広報に利用して良い会社ロゴをもらったり事務手続きはちょっとだけ面倒でした）</p>
<p>Organizationを作る前からQiitaでアウトプットしている人が何人もいたため、作成した途端20名を超えるメンバーを一気に追加した覚えがあります。<br>これにより開発チームが異なると、みんなが何に興味を持っているかも分からなかったりすることが多かったですが、Qiitaを通して社内のつながりが強化され面白い！ってみんなで話していました。</p>
<p>現在は60名を超えるメンバーがOrganizationに属しており、50名程度のメンバーが何かしらの記事をQiitaに投稿するなど活発にQiitaを利用しています。<br>チームメンバーが今興味があること、困ったりしていることが分かるので、とても良い流れができていると思います。笑</p>
<p>また、年末はITエンジニア界隈で恒例のAdvent Calendarを毎年のお祭り気分で参加しています。<br>年末のフューチャーは社内イベントもたくさんあり大変ですが、12月は毎日みんながどんな記事を投稿したかでちょっとした話題になっています。</p>
<ul>
<li><a href="https://qiita.com/advent-calendar/2018/future" target="_blank" rel="noopener">Future Advent Calendar 2017</a></li>
<li><a href="https://qiita.com/advent-calendar/2017/future" target="_blank" rel="noopener">Future Advent Calendar 2017</a></li>
<li><a href="https://qiita.com/advent-calendar/2016/future" target="_blank" rel="noopener">Future Advent Calendar 2016</a></li>
<li><a href="https://qiita.com/advent-calendar/2015/future" target="_blank" rel="noopener">Future Advent Calendar 2015</a></li>
</ul>
<p>2015年のAdvent Calendarでの謎の一体感から、その年明けにいくつかネタを見繕ってLT大会しよう！となって実施したのが<a href="https://future-architect.github.io/categories/Culture/page/2/">これ</a>になります。</p>
<p>色々変化を起こすと繋がってきますね！</p>
<p>今後はQiita Jobsを通しても、よりオープンに情報を発信し、自分たちの文化や技術力を洗練させ良い循環に繋げたいと思っています。</p>
<p>興味がある方はぜひランチ🍝や、カフェでコーヒー☕でも飲みながら話しましょう♫</p>
<p><strong><a href="http://www.future.co.jp/recruit/" target="_blank" rel="noopener">We are hiring engineers!!</a></strong></p>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><ul>
<li>フューチャー、Qiita Jobsを始めたってよ</li>
<li>今は2チームだけど今後も増やしていって情報をオープンに出すよ</li>
<li>今年もフューチャーでAdvent Calendarをやるので応援してね</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Qiita-Jobsとは&quot;&gt;&lt;a href=&quot;#Qiita-Jobsとは&quot; class=&quot;headerlink&quot; title=&quot;Qiita Jobsとは&quot;&gt;&lt;/a&gt;Qiita Jobsとは&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://qiita.com/&quot; t
    
    </summary>
    
      <category term="Culture" scheme="https://future-architect.github.io/categories/Culture/"/>
    
    
      <category term="Qiita" scheme="https://future-architect.github.io/tags/Qiita/"/>
    
  </entry>
  
  <entry>
    <title>LT Free Style Battle（VS リクルートテクノロジーズ）を開催しました</title>
    <link href="https://future-architect.github.io/articles/20190422/"/>
    <id>https://future-architect.github.io/articles/20190422/</id>
    <published>2019-04-22T01:12:28.000Z</published>
    <updated>2019-04-19T08:26:05.190Z</updated>
    
    <content type="html"><![CDATA[<p>フューチャー TIG所属の真野です。</p>
<p>2018年10月14日に弊社オフィスにて株式会社リクルートテクノロジーズ様と合同でLT FREE STYLE BATTLEを開催しました。リクルートテクノロジーズ様の皆さま。弊社までお越しいただき、本当にありがとうございます。とても感謝しております！</p>
<blockquote>
<p>リクルートテクノロジーズ様の記事はこちらです<br><a href="https://recruit-tech.co.jp/blog/2019/02/14/lt_free_style_battle/" target="_blank" rel="noopener">https://recruit-tech.co.jp/blog/2019/02/14/lt_free_style_battle/</a></p>
</blockquote>
<p>LT Free Style Battle の形式はシンプル、両者で5名ずつ好きな技術をテーマに熱く語ることです。勝敗は会場の拍手という非常に定量的かつ無慈悲な判定により下されます。さて、結果はどうだったでしょうか？</p>
<h2 id="料理"><a href="#料理" class="headerlink" title="料理"></a>料理</h2><p>お腹が空いてLTに集中できない！という最悪の事態を防止するため、軽食を用意しました。中には不届き者な腹ペコ社員がいて、ご飯を食べてさっと会場を後にした人もいたとかいないとか。</p>
<p><img src="/images/20190422/photo_20190422_01.jpeg"></p>
<h2 id="会場の様子"><a href="#会場の様子" class="headerlink" title="会場の様子"></a>会場の様子</h2><p>親睦を深めたいということで、セミナールームではなく交流ルームで開催しました。<br>ゆったり空間を取れてよかったと思います。</p>
<p><img src="/images/20190422/photo_20190422_02.jpeg"></p>
<h2 id="発表資料"><a href="#発表資料" class="headerlink" title="発表資料"></a>発表資料</h2><p>会場で発表された資料です。<br>まずはフューチャー側から。</p>
<h3 id="1-筒井悠平さん"><a href="#1-筒井悠平さん" class="headerlink" title="1. 筒井悠平さん"></a>1. 筒井悠平さん</h3><p>趣味と実益を兼ねた電子工作についての発表でした。<br>あいにくデモ機が満員電車で破損したという悲しい事件が発覚しましたが、部品調達や工作コツなど楽しさが伝わりました。<br>なお、テルミンは後日、無事修理されたとのことです。</p>
<script async class="speakerdeck-embed" data-id="34ea6254ec2b40eba9b57bca7fe3c419" data-ratio="1.41436464088398" src="//speakerdeck.com/assets/embed.js"></script>

<h3 id="2-田中駿さん"><a href="#2-田中駿さん" class="headerlink" title="2. 田中駿さん"></a>2. 田中駿さん</h3><p>日々の悩み、課題をIT技術の腕力で解決するという姿勢、見習いたいと思いました。<br>そして学習データの大事さを学ぶこともできました。チャットボット、良いですね。</p>
<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/3pbm6RAcaKbQlt" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/cbutters400/tarasinn" title="あなたの心のそばに、チャットボットtarasinn" target="_blank">あなたの心のそばに、チャットボットtarasinn</a> </strong> from <strong><a href="https://www.slideshare.net/cbutters400" target="_blank">Shun Tanaka</a></strong> </div></p>
<h3 id="3-塚本祥太さん"><a href="#3-塚本祥太さん" class="headerlink" title="3. 塚本祥太さん"></a>3. 塚本祥太さん</h3><p>競技プログラミング愛にあふれる発表でした。<br>会社のリソースを上手く使い、自社開催のHack To The Future（競技プログラミングコンテスト）に至る経緯を熱く伝えられていました。</p>
<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/tfJHvsWU56Yg4g" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/ShotaTsukamoto/future-vs-recruit-ltbattletsukammo" title="Future vs RECRUIT LT-Battle_tsukammo" target="_blank">Future vs RECRUIT LT-Battle_tsukammo</a> </strong> from <strong><a href="https://www.slideshare.net/ShotaTsukamoto" target="_blank">Shota Tsukamoto</a></strong> </div></p>
<h3 id="4-山本力世さん"><a href="#4-山本力世さん" class="headerlink" title="4. 山本力世さん"></a>4. 山本力世さん</h3><p>タイトルから全く予期できない発表。<br>5分の枠で絶対デモが収まらないだろうと思っていたら、案の定、見たい人はこの後でという流れでした。実はわたしと山本さんは席が近いのですが、本当によくルンバで遊んでいる（仕事をされている）姿を見ます。</p>
<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/b4pLnFTbL32b4q" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/rkyymmt/ss-123391761" title="ザビエル" target="_blank">ザビエル</a> </strong> from <strong><a href="https://www.slideshare.net/rkyymmt" target="_blank">力世 山本</a></strong> </div></p>
<h3 id="5-渋川よしきさん"><a href="#5-渋川よしきさん" class="headerlink" title="5. 渋川よしきさん"></a>5. 渋川よしきさん</h3><p>Real World HTTPの次版のネタをいくつか楽しく聞くことができました。<br>中々著者本人からこういった話を聞くことはわたしの経験の上では無かったので貴重でした。</p>
<script async class="speakerdeck-embed" data-id="c353292856a24fafa657f06778477ee1" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>


<p>続いて、リクルートテクノロジーズ様の発表資料です。</p>
<h3 id="1-辻健人さん"><a href="#1-辻健人さん" class="headerlink" title="1. 辻健人さん"></a>1. 辻健人さん</h3><script async class="speakerdeck-embed" data-id="d5ab96dae96e4d65acbc3472c10f2cfd" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>

<p>React/Reduxでパフォーマンス計測を上手く行う手法についての発表です。<br>弊社のフロントエンド勢が興味深く聞いていたのが印象的でした。</p>
<h3 id="2-與那城有さん"><a href="#2-與那城有さん" class="headerlink" title="2. 與那城有さん"></a>2. 與那城有さん</h3><script async class="speakerdeck-embed" data-id="4ae900b790ca4db39444b29f7babe217" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>

<p>個人的には一番楽しく発表を聞くことができ、最高に怪作だと思いました。<br>コンテナビルドに特化した話がこんなにも楽しいとは。<br>もともとの発表は30分超えである内容を絞ったので密度が濃く、勢いに圧倒されっぱなしでした。</p>
<h3 id="3-伊藤瑛さん"><a href="#3-伊藤瑛さん" class="headerlink" title="3. 伊藤瑛さん"></a>3. 伊藤瑛さん</h3><script async class="speakerdeck-embed" data-id="043d0b5eb1884a798d5e4fa62862ff56" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>

<p>WebAPIインターフェースにおける課題解決についてのお話。<br>弊社でもBFFパターンでアーキテクチャを設計することが多く、個人的に普通に導入したいと思った次第です。</p>
<h3 id="4-mizchiさん"><a href="#4-mizchiさん" class="headerlink" title="4. mizchiさん"></a>4. mizchiさん</h3><script async class="speakerdeck-embed" data-id="e692953706e642978aad89debaa5c8e5" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>

<p>Service Workerを上手く活用してServer Side Renderingをしていこうという発表でした。<br>アイデアだではなく、それをすぐ実装するといったスピード感にしびれました。</p>
<h3 id="5-古川陽介さん"><a href="#5-古川陽介さん" class="headerlink" title="5. 古川陽介さん"></a>5. 古川陽介さん</h3><script async class="speakerdeck-embed" data-id="031c1b2c2545495e901891e746194888" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>

<p>フューチャー、、、フィーチャー、、、似てますね！<br>Feature Policyそのものを恥ずかしながらあまり知らなかったので勉強になりました。<br>そしてまた自作のライブラリも実装されていてスゴイです。</p>
<h2 id="懇親会"><a href="#懇親会" class="headerlink" title="懇親会"></a>懇親会</h2><p>発表が終わった後はざっくばらんに懇親会。<br>厳正な審査が行われ、リクルートテクノロジーズ様がバトルを制されました！<br>おめでとうございます！</p>
<p>このような楽しい話を一緒に作っていいただけた、リクルートテクノロジーズ様、ありがとうございました。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;フューチャー TIG所属の真野です。&lt;/p&gt;
&lt;p&gt;2018年10月14日に弊社オフィスにて株式会社リクルートテクノロジーズ様と合同でLT FREE STYLE BATTLEを開催しました。リクルートテクノロジーズ様の皆さま。弊社までお越しいただき、本当にありがとうございま
    
    </summary>
    
      <category term="Culture" scheme="https://future-architect.github.io/categories/Culture/"/>
    
    
      <category term="LT" scheme="https://future-architect.github.io/tags/LT/"/>
    
  </entry>
  
  <entry>
    <title>Real World HTTPミニ版リリース記念勉強会の実施報告</title>
    <link href="https://future-architect.github.io/articles/20190310/"/>
    <id>https://future-architect.github.io/articles/20190310/</id>
    <published>2019-03-10T02:00:00.000Z</published>
    <updated>2019-04-10T00:51:57.342Z</updated>
    
    <content type="html"><![CDATA[<p>こんにちは、フーチャーのTIG所属、真野です。<br>パネルディスカッションとして参加したらとても良い経験になったため、その流れで開催報告記事を担当させていただくことになりました。</p>
<h1 id="勉強会の背景"><a href="#勉強会の背景" class="headerlink" title="勉強会の背景"></a>勉強会の背景</h1><p>TIG所属の<a href="https://twitter.com/shibu_jp" target="_blank" rel="noopener">渋川さん</a>が、<a href="https://www.oreilly.co.jp/community/blog/2019/03/real-world-http-mini-released.html" target="_blank" rel="noopener">無料の電子書籍『Real World HTTPミニ版』をリリース</a>したこともあり、せっかくなのでみんなで勉強会しようということでイベントを企画しました。</p>
<p>その名も…</p>
<p>『<a href="https://future.connpass.com/event/123910" target="_blank" rel="noopener">Real World HTTPミニ版リリース記念勉強会</a>』です！</p>
<p>そのままですね。笑</p>
<p>Real World HTTPミニ版は、若手エンジニアの勉強会や、企業の新人研修で使われることも想定されているということで、じゃぁ新人研修に関するパネルディスカッションも一緒に行おうということになりました。<br>折しも時期は3月、ちょうど企業側が新人研修のコンテンツに苦慮している時期なため、ちょうどよいのでは？という目論見もあります。</p>
<p>当日のスケジュールは以下のような時間配分です。<br>来てくださった皆様、本当にありがとうございます！</p>
<table>
<thead>
<tr>
<th>No</th>
<th>Time</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>30min</td>
<td>　著者の渋川による発表</td>
</tr>
<tr>
<td>2</td>
<td>30min</td>
<td>　新人教育に関するパネルディスカッション<br>（リクルートテクノロジーズ 古川陽介さん、DeNA karupaneruraさん、フューチャー真野、司会渋川）</td>
</tr>
<tr>
<td>3</td>
<td>45min</td>
<td>　懇親会</td>
</tr>
</tbody>
</table>
<h1 id="著者発表"><a href="#著者発表" class="headerlink" title="著者発表"></a>著者発表</h1><p>渋川さんによるReal World HTTPの最新情報です。<br>ミニ版をなぜ公開したかといった背景の説明や、さらにReal World HTTPに追加するネタもいくつか紹介。<br>アカデミックなプロトコルの話ではなく、あくまで現実世界で使われているようなWebFrameworkの仕様のようなネタが紹介され、個人的には非常に勉強になりました。</p>
<p><img src="/images/20190410/shibukawa.jpg"></p>
<h2 id="内容について"><a href="#内容について" class="headerlink" title="内容について"></a>内容について</h2><p>簡単ではありますが、聞いていてすごいなーと思った点をまとめました</p>
<ul>
<li>ミニ版はHTTPの基礎だけを取り出した本！<ul>
<li>著作権を調整したので、社内教育用に引用できる！</li>
</ul>
</li>
<li>そもそもReal World HTTPを書いた背景<ul>
<li>フレームワークを学ぶ、というのはあるが裏でどう動いてる、というのを体系的に学べる機会をつくる</li>
<li>Webの一次情報は見るのは良いが、RFCはそれだけだと単独機能の説明のため全体像がつかみにくく、読み解くにはスキルが必要</li>
<li>本は出版すると古くなるが、基礎的な知識があれば新しいこともわかるようになるのでキッカケにしたい</li>
</ul>
</li>
<li>無料にした理由<ul>
<li>無料にすると多くの人の目に触れる。中身を知らないものは売れないという時代もある</li>
<li>同じだけの知識量を、次の世代の人がより短時間で学べるようにするため</li>
<li>同じ苦労をして同じ量しか学べないのであれば上の世代としては失格</li>
</ul>
</li>
<li>ミニ版の内容<ul>
<li>HTTPの基本的内容</li>
</ul>
</li>
<li>初版出版後から追加したい内容をGitHub Issueで管理していた<ul>
<li>これから書きたいネタについて（膨大なネタが…!!!ぜひ続編に期待しましょう!!!)</li>
</ul>
</li>
</ul>
<h1 id="パネルディスカション"><a href="#パネルディスカション" class="headerlink" title="パネルディスカション"></a>パネルディスカション</h1><p>フューチャー含めた3社でReal World HTTPや、それに絡めた新人研修についてディスカッションです。<br>パネラーは以下の方々！司会は安定の著者渋川さんです。</p>
<ul>
<li>リクルートテクノロジーズ 古川陽介さん</li>
<li>DeNA karupaneruraさん</li>
<li>フューチャー真野、司会:渋川さん</li>
</ul>
<p><img src="/images/20190410/panel.jpg"></p>
<p>新人研修はそれぞれ各社の特色が出ていて、話していて面白かったです。</p>
<h2 id="ディスカッション内容"><a href="#ディスカッション内容" class="headerlink" title="ディスカッション内容"></a>ディスカッション内容</h2><p>以下のようなテーマで話していました。<br>どこも優秀そうな新米エンジニアな方を、さらに強くするために色々と工夫されていてスゴイと思いました。<br>また、新人研修について参加者の方からいくつか質問をいただきました！</p>
<p>話したテーマ（ネタ）抜粋：</p>
<ul>
<li>新人教育において工夫してる点<ul>
<li>低レイヤーやパフォーマンスを意識させるためISUCONを活用しているとの意見もあり</li>
<li>某A社では、HTTP Requestをスクラッチで実装させたりするらしい</li>
<li>某B社ではWiresharkのようなアプリをスクラッチで実装させているらしい（恐ろしい..）<ul>
<li>HTTPはサーバよりクライアントを実装してみるほうが理解しやすいとか</li>
</ul>
</li>
</ul>
</li>
<li>入社する人の背景は様々。研修のレベルはどう合わせてるか？</li>
<li>Real World HTTPは本当の初学者にはぶっちゃ敷居が高くないかという話<ul>
<li>Login後Redirect遷移などの理解がピンとこない人向けにレファレンスとして使わせてる</li>
<li>迷子になったときの指針書として良い</li>
</ul>
</li>
<li>フューチャー社で渋川さんに質問すると、だいたい自著をページ番号付きで引用してくる件</li>
<li>HTTPヘッダーについて詳しく書いてある本が少ないので、Real World HTTP ヘッダー版を出版してくれないか話</li>
<li>新人研修の規模感と、研修のスタイルについて</li>
<li>新人研修で最近の情報系はどの程度、現場の技術を知っているものなのか</li>
<li>単なる技術ではなく、ITエンジニアとしての考え方、振る舞いをどうやって身につかせるか<br>などなど</li>
</ul>
<p>書ききれないですが、非常に濃密な30分だったと思います。</p>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>HTTPの通信周りや、ブラウザの挙動で悩んだ時に読んでいた Real World HTTPという技術書ですが、その背景にある渋川さんの思いに触れられて刺激をもらえる良い会でした！<br>各社の新人研修もとても良いですね。可能であれば年齢含めて何もかも偽って受講したいと思いました。笑<br>また、こういったイベントでお互いの知見を交換しあえる機会を作りたい思います！</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;こんにちは、フーチャーのTIG所属、真野です。&lt;br&gt;パネルディスカッションとして参加したらとても良い経験になったため、その流れで開催報告記事を担当させていただくことになりました。&lt;/p&gt;
&lt;h1 id=&quot;勉強会の背景&quot;&gt;&lt;a href=&quot;#勉強会の背景&quot; class=&quot;h
    
    </summary>
    
      <category term="Event" scheme="https://future-architect.github.io/categories/Event/"/>
    
    
      <category term="Network" scheme="https://future-architect.github.io/tags/Network/"/>
    
      <category term="Education" scheme="https://future-architect.github.io/tags/Education/"/>
    
  </entry>
  
  <entry>
    <title>第2回Future開発合宿</title>
    <link href="https://future-architect.github.io/articles/20190102/"/>
    <id>https://future-architect.github.io/articles/20190102/</id>
    <published>2019-01-02T04:50:16.000Z</published>
    <updated>2019-01-07T03:21:37.365Z</updated>
    
    <content type="html"><![CDATA[<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>あけまして、おめでとうございます🐗<br>またお会いましたね。フューチャー歴2年目の谷村です。<br>今回は時空を越えて、2017年12月16-17日に開催した第2回の開発合宿レポートをお届けします。</p>
<ul>
<li>第1回の記事は<a href="https://future-architect.github.io/articles/20171217/">こちら</a></li>
</ul>
<p>Future開発合宿はプライベートでおのおのが作りたいものを作るというテーマのもと、有志活動として行っています。</p>
<h1 id="準備"><a href="#準備" class="headerlink" title="準備"></a>準備</h1><p>前回合宿させていただいた土善旅館さんに11月頭に連絡を取ると、12月は合う日程がないとのこと。<br>さすが最高の旅館ですね。</p>
<p>というわけで様々な情報を探しましたが、同条件で合宿できるところは中々ありません。<br>最終的に、私が別件の合宿で宿泊したことのある<a href="http://www.ito-yamaki.jp" target="_blank" rel="noopener">山喜旅館さん</a>にお願いしました。</p>
<p>今回の旅のしおりはこちらです。<br><a href="https://gist.github.com/tng527/1d30de9ea7eaeaa5d34251e187e04319" target="_blank" rel="noopener">https://gist.github.com/tng527/1d30de9ea7eaeaa5d34251e187e04319</a></p>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>前回のブログ同様、先にまとめておきます。</p>
<ul>
<li>伊東市の温泉がスゴイ！</li>
<li>海産物が美味しい！</li>
<li>海が近い！砂浜で遊べる！</li>
<li>建物に重厚感が有りお寺みたい！</li>
<li>お風呂はぬるめ</li>
<li>開発部屋では飲食制限あり</li>
<li><a href="http://www.draftkeg.co.jp" target="_blank" rel="noopener">draftkeg</a>はテンション爆上げ！</li>
<li>NW弱い、遅い…</li>
</ul>
<p><img src="/images/20190102/photo_20190102_01.jpeg"></p>
<h1 id="当日"><a href="#当日" class="headerlink" title="当日"></a>当日</h1><p>山喜旅館は、静岡県の伊東市(熱海の南、伊豆半島)にあります。<br>都内から電車で2時間くらい。土善旅館より近いですね。</p>
<p>伊東市は観光地として発展しており、スーパーや飲食店、コンビニなどの商業施設があり快適です。</p>
<p><img src="/images/20190102/photo_20190102_02.png" class="img-middle-size" style="border:solid 1px #CFD8DC"><br>早速遅刻者が出ました。…主催の私です。ごめんなさいごめんなさい。</p>
<p><img src="/images/20190102/photo_20190102_03.png" class="img-middle-size" style="border:solid 1px #CFD8DC"><br>さらに電車の乗り換えをミスりました。ごめんなさいごめんなさい。</p>
<p><img src="/images/20190102/photo_20190102_04.jpeg"><br>定時組は魚楽亭さんでご飯を食べつつ、</p>
<p><img src="/images/20190102/photo_20190102_05.jpeg"><br>山喜旅館に到着！</p>
<p><img src="/images/20190102/photo_20190102_06.jpeg"><br>スーパーでお酒を買い出しして、<br>(写真はスーパーで蔵元さんが日本酒を詰めてくれているところ)</p>
<p><img src="/images/20190102/photo_20190102_07.jpeg"><br>乾杯で合宿を始めます。</p>
<p><img src="/images/20190102/photo_20190102_08.jpeg"><br>今回の開発部屋はこんな感じ。畳ではないので寝転べないですが、机がしっかりしています。<br>旅館のご厚意で、小さな会議室の値段で大きな会議室を貸してくださいました。<br>(エアコンが効かず寒かった…)</p>
<p><img src="/images/20190102/photo_20190102_09.jpeg"><br>みかん片手にひたすら開発します。</p>
<p><img src="/images/20190102/photo_20190102_10.jpeg"><br>海が近いので、開発が煮詰まったら波打ち際でチャプチャプします。</p>
<p><img src="/images/20190102/photo_20190102_11.jpeg"><br>眺めの良いお部屋を割り当てていただいたので、<br>チェックイン時間になると、そちらで開発する人もいたようです。</p>
<p><img src="/images/20190102/photo_20190102_12.jpeg"><br>そうこうするうちに夕飯のお時間。<br>お魚と天ぷらと鍋で美味しかったです！<br>別室を用意いただけてありがたいですね。</p>
<p><img src="/images/20190102/photo_20190102_13.jpeg"><br>てっぺん(24時)回ったら集合写真。倒れてる人もいましたが全員で撮ることが出来ました。</p>
<p><img src="/images/20190102/photo_20190102_14.jpeg"><br>25時回ったら<a href="https://tabelog.com/shizuoka/A2205/A220503/22013110/" target="_blank" rel="noopener">くるまやラーメン</a>へ。<br>この旅館に来たら行かねばならぬ！美味しさは普通なのに、25時に外まで行列ができてます。</p>
<p><img src="/images/20190102/photo_20190102_15.jpeg"><br>もうだいぶ眠そう…</p>
<p><img src="/images/20190102/photo_20190102_16.jpeg"><br>2日目も乾杯から始めます。draftkegで美味しいビールを注いで、</p>
<p><img src="/images/20190102/photo_20190102_17.jpeg"><br>乾杯！！</p>
<p><img src="/images/20190102/photo_20190102_18.jpeg"><br>そしてまた開発へ。</p>
<p><img src="/images/20190102/photo_20190102_19.jpeg"><br>成果発表タイム！<br>競プロについて熱く語ったり。</p>
<p><img src="/images/20190102/photo_20190102_20.jpeg"><br>成果挙がらなかったので合宿の幹事やったことが成果だと言い切ってみたり。(私です)</p>
<p><img src="/images/20190102/photo_20190102_21.jpeg"><br>タイトルから伺える進捗感…今回は全体的に反省多めかも。</p>
<p><img src="/images/20190102/photo_20190102_22.jpeg"><br>Haskellについて熱く語ったり。</p>
<p><img src="/images/20190102/photo_20190102_23.jpeg"><br>うさぎの健康管理アプリ作ってみたり。</p>
<p><img src="/images/20190102/photo_20190102_24.jpeg"><br>最後にみんなで投票しておしまい！</p>
<h1 id="参加者アンケート結果"><a href="#参加者アンケート結果" class="headerlink" title="参加者アンケート結果"></a>参加者アンケート結果</h1><h2 id="KEEP"><a href="#KEEP" class="headerlink" title="KEEP"></a>KEEP</h2><ul>
<li>温泉旅館に行けるのが良い</li>
<li>土善旅館にロックインせずに済んだ</li>
<li>海辺で遊べた</li>
<li>もくもく会的な集中環境を作れた</li>
<li>各々が得意分野の知識をシェアできたた</li>
</ul>
<h2 id="PROBLEM"><a href="#PROBLEM" class="headerlink" title="PROBLEM"></a>PROBLEM</h2><ul>
<li>谷村さんが電車に乗り遅れたこと ←ごめんなさい</li>
<li>お酒が多すぎた</li>
<li>NW環境が悪い</li>
<li>お酒がぬるくならない環境を用意したい</li>
</ul>
<h2 id="TRY"><a href="#TRY" class="headerlink" title="TRY"></a>TRY</h2><ul>
<li>二泊三日したいです！</li>
<li>島でやりたい</li>
<li>各自ビジホでサテライト的に。。。</li>
<li>リモート参加の実現</li>
</ul>
<p>本合宿は2017/12/16-17に実施しました。<br>次回は2018/11/17-18を予定しています。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h1&gt;&lt;p&gt;あけまして、おめでとうございます🐗&lt;br&gt;またお会いましたね。フューチャー歴2年目の谷村です。&lt;br&gt;今回は時空を越えて、
    
    </summary>
    
      <category term="Culture" scheme="https://future-architect.github.io/categories/Culture/"/>
    
    
      <category term="Camp" scheme="https://future-architect.github.io/tags/Camp/"/>
    
  </entry>
  
  <entry>
    <title>5TB/日 のデータをAWS Glueでさばくためにやったこと（性能編）</title>
    <link href="https://future-architect.github.io/articles/20181205/"/>
    <id>https://future-architect.github.io/articles/20181205/</id>
    <published>2018-12-05T14:18:17.000Z</published>
    <updated>2018-12-04T14:54:05.811Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://qiita.com/advent-calendar/2018/future" target="_blank" rel="noopener">フューチャー Advent Calendar 2018 Qiita</a> 5日目の記事です。</p>
<p>みなさん、こんにちは。<br>本記事は、AWS Glueについてのサービス概要や開発Tipsを紹介する<a href="https://future-architect.github.io/articles/20180828">5TB/日 のデータをAWS Glueでさばくためにやったこと（概要編）</a>の続編で、Glueの性能検証した内容を共有していきます。</p>
<h2 id="検証について"><a href="#検証について" class="headerlink" title="検証について"></a>検証について</h2><p>Glueの性能がテーマですが、Glueそのものには設定できるパラメータが少ないためチューニングの余地が比較的小さいです。</p>
<p>その中で、Glueの機能に着目すると以下の1~4が挙げられると思います。</p>
<ol>
<li>DPU数<ul>
<li>ジョブに使用されるDPU(Data Processing Unit)の数、Sparkで言うところのworker nodeの数に該当</li>
<li>1DPUは4vCPU、16GBのメモリを持ち、これ以外を選択することは不可</li>
<li>ジョブ実行に使用できるDPU数は最低2つ、最大で100まで設定可能  </li>
</ul>
</li>
<li>データカタログの使用有無<ul>
<li>Glueは、クローリングによりデータカタログというメタデータを自動生成できる</li>
<li>データを読み込む際は、このデータカタログを利用することも、利用しないこともできる</li>
</ul>
</li>
<li>ファイル形式<ul>
<li>Glueではデータの入出力にGlueContextを使用します</li>
<li>様々な形式のファイルに対応していますが、ファイルの形式別にリードライトに性能差がある</li>
<li>以下のファイルフォーマットは入力・出力の両方に対応<ul>
<li>avro</li>
<li>csv</li>
<li>json</li>
<li>orc</li>
<li>parquet</li>
</ul>
</li>
<li>以下のファイルフォーマットは入力のみに対応<ul>
<li>ion</li>
<li>grokLog</li>
<li>xml</li>
</ul>
</li>
</ul>
</li>
<li>ファイルパーティション<ul>
<li>GlueContextを使用してS3へファイルを出力する際に、特定の項目でパーティション分割することが可能</li>
<li>これにより読み込む際に必要なファイルのみを読み込むことが可能</li>
</ul>
</li>
</ol>
<p>今回は、「1.DPU」を除いた、2~4を検証していきます。</p>
<p>※Glueの内部で利用されているSparkそのものの性能についてはスコープ外としてますのでご了承ください。</p>
<h2 id="検証用データ"><a href="#検証用データ" class="headerlink" title="検証用データ"></a>検証用データ</h2><p>検証用ダミーデータには以下のものを使用しました。</p>
<ul>
<li>件数 : 2000万レコード</li>
<li>カラム数 : 4パターンの連番を持つ4カラム</li>
<li>データ　: 1 ~ 1000000までの値</li>
</ul>
<p>データイメージ</p>
<table>
<thead>
<tr>
<th>No</th>
<th>column1</th>
<th>column2</th>
<th>column3</th>
<th>column4</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>…</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>19</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>20</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>21</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>22</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>…</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1999</td>
<td>100</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2000</td>
<td>100</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2001</td>
<td>101</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>2002</td>
<td>101</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>…</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>199999</td>
<td>10000</td>
<td>100</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>200000</td>
<td>10000</td>
<td>100</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>200001</td>
<td>10001</td>
<td>101</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>200002</td>
<td>10001</td>
<td>101</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>…</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>19999999</td>
<td>1000000</td>
<td>10000</td>
<td>100</td>
<td>1</td>
</tr>
<tr>
<td>20000000</td>
<td>1000000</td>
<td>10000</td>
<td>100</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>またデータの形式は以下のものを用意しました。</p>
<table>
<thead>
<tr>
<th>拡張子</th>
<th>ファイルサイズ</th>
<th>備考</th>
</tr>
</thead>
<tbody>
<tr>
<td>csv</td>
<td>334.0MB</td>
<td></td>
</tr>
<tr>
<td>avro</td>
<td>414.1MB</td>
<td></td>
</tr>
<tr>
<td>json.gz</td>
<td>58.5MB</td>
<td>jsonをgzipで圧縮。Glueで対応しており、未解凍で読み込み可能</td>
</tr>
<tr>
<td>json</td>
<td>1.65GB</td>
<td></td>
</tr>
<tr>
<td>snappy.orc</td>
<td>83.5MB</td>
<td>圧縮方式にsnappyを選択したorcバイナリファイル</td>
</tr>
<tr>
<td>snappy.parquet</td>
<td>86.8MB</td>
<td>圧縮方式にsnappyを選択したparquetバイナリファイル</td>
</tr>
</tbody>
</table>
<p>Glueは数百GB、数億カラムレベルのデータサイズも対応可能ですが、今回はこのデータで検証を行います。(データを作るのが大変だったため…)</p>
<h2 id="検証結果"><a href="#検証結果" class="headerlink" title="検証結果"></a>検証結果</h2><p>先に説明したダミーデータを用いて検証します。<br>環境はGlue、S3ともに東京リージョンを利用しています。特に明記しない場合は、ファイル形式はCSVを利用し、検証結果は10回計測した平均値を記載しています。<br><br></p>
<h3 id="検証①-DPU数"><a href="#検証①-DPU数" class="headerlink" title="検証① DPU数"></a>検証① DPU数</h3><p>DPU数を増やすと時間がかかるタスクも高速化が期待できますが、今回の検証からは割愛します。<br><br></p>
<h3 id="検証②-データカタログの使用有無"><a href="#検証②-データカタログの使用有無" class="headerlink" title="検証② データカタログの使用有無"></a>検証② データカタログの使用有無</h3><p>データカタログの利用する、しないで検証しました。<br>しないケースはさらに、2種類のAPIで比較して検証しました。</p>
<ul>
<li>検証結果</li>
</ul>
<table>
<thead>
<tr>
<th>方式</th>
<th>読み込み時間[s]</th>
</tr>
</thead>
<tbody>
<tr>
<td>データカタログ利用</td>
<td>0.3019</td>
</tr>
<tr>
<td>GlueContextのfrom_option利用</td>
<td>0.2461</td>
</tr>
<tr>
<td>Sparkのread.csvを利用</td>
<td>0.7811</td>
</tr>
</tbody>
</table>
<ul>
<li>コメント<ul>
<li>結果にあまり差が出なかったため、実装方式に合わせて任意のものを選択するで良いと思います</li>
</ul>
</li>
</ul>
<p><br></p>
<h3 id="検証③-ファイル形式"><a href="#検証③-ファイル形式" class="headerlink" title="検証③ ファイル形式"></a>検証③ ファイル形式</h3><p>パーティションを切らず、いくつかのファイル形式でS3から読込/書出 を行い検証しました。</p>
<ul>
<li>読込結果</li>
</ul>
<table>
<thead>
<tr>
<th>拡張子</th>
<th>読み込み時間[s]</th>
</tr>
</thead>
<tbody>
<tr>
<td>csv</td>
<td>0.1305</td>
</tr>
<tr>
<td>avro</td>
<td>0.1144</td>
</tr>
<tr>
<td>json.gz</td>
<td>0.1211</td>
</tr>
<tr>
<td>json</td>
<td>0.3420</td>
</tr>
<tr>
<td>snappy.orc</td>
<td>0.4451</td>
</tr>
<tr>
<td>snappy.parquet</td>
<td>0.3794</td>
</tr>
</tbody>
</table>
<ul>
<li>書出結果</li>
</ul>
<table>
<thead>
<tr>
<th>拡張子</th>
<th>書き出し時間[s]</th>
</tr>
</thead>
<tbody>
<tr>
<td>csv</td>
<td>6.9508</td>
</tr>
<tr>
<td>avro</td>
<td>12.8020</td>
</tr>
<tr>
<td>json</td>
<td>15.1127</td>
</tr>
<tr>
<td>snappy.orc</td>
<td>13.9531</td>
</tr>
<tr>
<td>snappy.parquet</td>
<td>11.6287</td>
</tr>
</tbody>
</table>
<ul>
<li>コメント<ul>
<li>読込に関してはほとんど差が見受けられませんでした。10回実施した際の各回にもばらつきがあるため、誤差の範囲内かと思われます</li>
<li>書出には明確な差が出ました<ul>
<li>特にjsonは遅く、変換に時間がかかったものと推測</li>
<li>CSVが一番早いですが、ファイルサイズが大きくなってしまうのがネック</li>
</ul>
</li>
<li>性能やファイルサイズを考えると、avroやparquetといった、バイナリファイルの選択が有力ではないでしょうか<ul>
<li>今回のわたしの在籍していたPJではparquetを採用しました</li>
<li>バイナリファイルは、内容の確認やデータ作成に一手間かかるため、開発時にはCSVを使い、テストや本運用の時のみparquetを利用するという手法が良いかと思います</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="検証④-ファイルパーティション"><a href="#検証④-ファイルパーティション" class="headerlink" title="検証④ ファイルパーティション"></a>検証④ ファイルパーティション</h2><p>パーティション数を変化させてS3から読込/書出 を実施しました。</p>
<ul>
<li>読込結果（5回計測、その平均値）</li>
</ul>
<table>
<thead>
<tr>
<th>拡張子</th>
<th>読み込み時間[s]</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.0793</td>
</tr>
<tr>
<td>100</td>
<td>0.1565</td>
</tr>
<tr>
<td>10000</td>
<td>0.1986</td>
</tr>
</tbody>
</table>
<ul>
<li>書出結果（5回計測、その平均値）</li>
</ul>
<table>
<thead>
<tr>
<th>拡張子</th>
<th>書き出し時間[s]</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>70.1816</td>
</tr>
<tr>
<td>100</td>
<td>58.3604</td>
</tr>
<tr>
<td>10000</td>
<td>410.1765</td>
</tr>
<tr>
<td>1000000</td>
<td>6時間待っても終了しなかったので中断</td>
</tr>
</tbody>
</table>
<ul>
<li>コメント<ul>
<li>読込はあまり差が出ませんでした。（実は読み込まれていないのでは？と不安に思い、別の手段でも確かめましたが同じ結果でした） </li>
<li>書出はパーティション数を増やした場合に著しく性能が落ちました</li>
<li>開発時は細かくパーティションを切った方が目的のファイルにたどり着きやすく、デバッグしやすかったです</li>
<li>パーティション設計はGlue利用時の肝ですが、実アプリへ適用したパーティション設計は、後続処理の内容やサービスに依存するため一言では言い難いです<ul>
<li>例1: 後続がGlueの場合、時間課金ですので出力時にはパーティションは大きくは切らず、短時間で出力し切った方がコストを削減できます。</li>
<li>例2: 後続がAthenaの場合、読み取ったデータサイズに対して課金されますのでパーティションを細かく切り、余計なデータは読まないようにした方がコストは削減できます</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="その他"><a href="#その他" class="headerlink" title="その他"></a>その他</h2><p>ここまで処理時間について検証しましたが、見逃しやすくもネックになりやすいのはGlueのクラスタ構築にかかる時間です。</p>
<p>Glue jobを実行する場合は、必ずクラスタの構築が開始されますが、このクラスタ構築が曲者で、構築に必要なDPUの確保に数分の時間がかかる場合があります。</p>
<p>もちろんjobが数時間レベルのものであれば、数分間は誤差かもしれません。</p>
<p>しかし最悪の場合、時間がかかるだけでなくそもそもDPUの確保に失敗し、job自体が動かないという事象が発生します。</p>
<p>クラスタ構築時間、job失敗率は要求するDPUの数に応じて大きくなります。DPU確保失敗時には設定回数を上限としたリトライも可能ですが、それでも100%実行できるという保証はありません。</p>
<p>残念ながらGlueにはリザーブドの考え方は存在しないため、DPU確保に関する問題を回避する方法は現在のところ存在しません。</p>
<p>2018年11月時点での東京リージョンで触ってみた感覚では、以下のような感じです。</p>
<ul>
<li>10(デフォルト):クラスタ構築に5~10分前後ほどかかる。DPU確保失敗はほぼ起こらない</li>
<li>20:クラスタ構築に15分ほどかかる。DPU確保失敗する割合は五分五分程度</li>
<li>30:DPU確保にほぼ失敗する</li>
</ul>
<p>ただし実行する時間帯も関係するようで、朝5時など利用者が少ないような時間だとDPUの確保できる成功率が高くなるようです。</p>
<h2 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h2><ul>
<li>データの読込は、ファイルの形式やパーティションの切り方に関わらず高速</li>
<li>データの書出は、parquetなどの圧縮形式が高速で、サイズも小さい。またパーティション数が多いほど時間がかかる</li>
<li>ミニサイズのジョブにおいて、性能面で最も考慮すべきはGlueのDPU確保の不安定さ。DPU確保にかかる時間は要注意。最悪、確保できずに実行失敗する。今後のサービスの成熟に期待</li>
</ul>
<h2 id="あとがき"><a href="#あとがき" class="headerlink" title="あとがき"></a>あとがき</h2><p>いかがだったでしょうか？性能編と言っておきながら最後はサービスとしてまだ発展途上という終わり方になってしまいましたが、AWSのサービスの成熟スピードはとても早いため、きっと近い将来に改善されているのではと思います。</p>
<p>実際、Glueを日々使っている中で新しい機能がどんどん追加されていき、便利に使いやすくなっていきました。</p>
<p>開発をしながら機能追加に驚き、裏の改善を行なっているであろうAWSのエンジニアさんのことを想い、一緒に仕事ができているようで尊敬やら激励やらの念を抱いていた当時のことを、記事を書きながら思い出しました。</p>
<p>私も彼らに負けないよう、のっぴきならない人生を乗り越えていくために、さらに切磋琢磨していきたいと想います。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://qiita.com/advent-calendar/2018/future&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;フューチャー Advent Calendar 2018 Qiita&lt;/a&gt; 5日目の記事です。&lt;/p
    
    </summary>
    
      <category term="BigData" scheme="https://future-architect.github.io/categories/BigData/"/>
    
    
      <category term="AWS" scheme="https://future-architect.github.io/tags/AWS/"/>
    
  </entry>
  
  <entry>
    <title>AmazonConnect BootCampセッションでハンズオン受けてきた話</title>
    <link href="https://future-architect.github.io/articles/20181202/"/>
    <id>https://future-architect.github.io/articles/20181202/</id>
    <published>2018-12-02T05:21:59.000Z</published>
    <updated>2018-12-02T06:03:36.058Z</updated>
    
    <content type="html"><![CDATA[<p>本記事は <a href="https://qiita.com/advent-calendar/2018/future" target="_blank" rel="noopener">フューチャー Advent Calendar 2018 2日目</a>の記事です。</p>
<h1 id="まえがき"><a href="#まえがき" class="headerlink" title="まえがき"></a>まえがき</h1><p>近々東京リージョンに提供されることがアナウンスされたAmazonConnectのBootCampセッション@目黒に2018年10月に参加しました。</p>
<p>コールセンターソリューションはGoogleもContact Center AIの開始をアナウンスするなど、混戦が予想されるホットトピックだと思います。</p>
<p>AmazonConnectは海外・日本での導入事例も増えてきていますが、ブログはまだ数が少ないと思いますので、実際に手を動かした作った仕組みと合わせて共有します。</p>
<h2 id="AmazonConnect-Session-day1-2018-11-06"><a href="#AmazonConnect-Session-day1-2018-11-06" class="headerlink" title="AmazonConnect Session#day1 2018.11.06"></a>AmazonConnect Session#day1 2018.11.06</h2><h3 id="セールスと技術担当者からのAmazonConnectの説明"><a href="#セールスと技術担当者からのAmazonConnectの説明" class="headerlink" title="セールスと技術担当者からのAmazonConnectの説明"></a>セールスと技術担当者からのAmazonConnectの説明</h3><p>米AWSから営業のご担当とソリューションアーキテクトの方がいらっしゃって、両名からの講義という形でした。<br>完全に全編英語となりびっくりしましたが、日本AWSの方の通訳があるため安心です。</p>
<p>AWSの広告っぽくなってしまうので、サラッと記載します。</p>
<h2 id="Sales"><a href="#Sales" class="headerlink" title="Sales"></a>Sales</h2><h3 id="AmazonConnectについて"><a href="#AmazonConnectについて" class="headerlink" title="AmazonConnectについて"></a>AmazonConnectについて</h3><ul>
<li>AmazonConnectのサービス自体は1.5年前から開始され、東京リージョンでのサービスインも決定している</li>
<li>今となっては大規模な導入事例もあり、安定した実績がある。（海外では銀行やISP、アパレルなど様々）</li>
<li>もともとはAmazonのEC事業を支えるコールセンター業務向けのソリューション</li>
</ul>
<h3 id="AmazonConnectのウリ"><a href="#AmazonConnectのウリ" class="headerlink" title="AmazonConnectのウリ"></a>AmazonConnectのウリ</h3><ul>
<li>S3に録音データを保存したり、LambdaやSageMaker、Lexなどと連携させることで高度なコールセンターソリューションを提供できる</li>
<li>CRM（顧客管理システム）との連携も容易で、熟達していなくても1, 2時間で連携作業が完了できる</li>
<li>セールスポイントは利用料は使った分だけ。スケーラビリティにも優れているので、自分でスケールを意識しなくていい<ul>
<li>オペレータが5人から5,000人に増えようと、クリスマス商戦があろうと、自動でスケールする</li>
</ul>
</li>
<li>IVRやCTIみたいな大規模設備は不要で、インターネット回線だけあれば使える</li>
<li>おおよその顧客で25%程度の運用費節約ができている</li>
</ul>
<h2 id="Solution-Architect"><a href="#Solution-Architect" class="headerlink" title="Solution Architect"></a>Solution Architect</h2><h3 id="リージョン展開について"><a href="#リージョン展開について" class="headerlink" title="リージョン展開について"></a>リージョン展開について</h3><ul>
<li>東京リージョンのConnectは今現在インプリを行っている状態</li>
<li>近いところではシドニーリージョン。日本の電話番号もここで取得可能</li>
</ul>
<h3 id="AWSマネージドサービスとの連携"><a href="#AWSマネージドサービスとの連携" class="headerlink" title="AWSマネージドサービスとの連携"></a>AWSマネージドサービスとの連携</h3><p>様々なサービスと連携可能で、特に下記のサービスとの連携事例が多いそうです。</p>
<table>
<thead>
<tr>
<th>サービス</th>
<th>概要</th>
</tr>
</thead>
<tbody>
<tr>
<td>Lambda</td>
<td>関数を実行して、DBやEC2などと連携させる。    この子のおかげで柔軟なソリューションを構築可能。</td>
</tr>
<tr>
<td>Lex</td>
<td>対話型インターフェースサービス。    IVRで一番イヤなのは耳に電話当てたり、キーバッド触ったりが頻発するところ。    あと、問い合わせ番号はえてして長いことが多いです。    Lexを使えば、しゃべった言葉を理解して、Lambdaの引数として渡す、とかできます。    ※日本語未対応・・・</td>
</tr>
<tr>
<td>Polly</td>
<td>テキスト読み上げサービス。    自然な感じのイントネーションで文章を読み上げてくれる。    日本語対応済み。（Takumi（男性）とMizuki（女性））    SSMLにも対応しており、やろうと思えばかなり自然にできる。（初音ミク感）</td>
</tr>
<tr>
<td>S3</td>
<td>言わずとしれたストレージサービス。    録音データの保存などに使われます。</td>
</tr>
<tr>
<td>Kinesis Firehose, Streams</td>
<td>ストリーミングデータをリアルタイムに収集、処理するサービス。    ゼロ管理で若干タイムラグが発生してもいいならFirehose、    より高速な処理を求めるならStreamsをおすすめするとのこと。</td>
</tr>
<tr>
<td>Redshift</td>
<td>DWHのサービス。Kinesisなどで取得したログをクエリできるように    保存するために利用します。</td>
</tr>
<tr>
<td>Athena</td>
<td>Redshiftの友達ですが、よりライトなクエリ基盤。</td>
</tr>
<tr>
<td>Glue</td>
<td>ETL（データ準備）サービス。RedshiftやAthenaに必要な情報だけ貯めるために使います。</td>
</tr>
<tr>
<td>Quicksite</td>
<td>情報可視化基盤。ライトにやるならKinesisで取得してGlueで必要情報だけ抜いて    Athenaでクエリ環境を整えてQuicksiteで見る。</td>
</tr>
</tbody>
</table>
<p>例えば、以下のようなデモでAWSマネージドサービスとの連携が説明されました。</p>
<ul>
<li>Kibanaで表示する<a href="https://aws.amazon.com/jp/blogs/big-data/analyzing-amazon-connect-records-with-amazon-athena-aws-glue-and-amazon-quicksight/" target="_blank" rel="noopener">デモ</a>をやってた。※URLではQuicksite</li>
<li>Amazon connectのコンソールでもリアルタイム/オンデマンドのデータを表示できる。（とはいえ可視化できるデータは少なめ）</li>
</ul>
<h3 id="3rd-Party-システムとの連携"><a href="#3rd-Party-システムとの連携" class="headerlink" title="3rd-Party システムとの連携"></a>3rd-Party システムとの連携</h3><p>引用ですが、下記のスライドで説明がありました。<br><img src="/images/20181202/photo_20181202_01.png"></p>
<h3 id="CCP（ソフトフォン）の拡張"><a href="#CCP（ソフトフォン）の拡張" class="headerlink" title="CCP（ソフトフォン）の拡張"></a>CCP（ソフトフォン）の拡張</h3><p>デフォルトのソフトフォンだと、必要なCRMの情報が出力できないなど、昨今の日本のコールセンター向けとしては利用に耐えられないと思われますが、拡張が可能になっています。</p>
<h4 id="Streams-API"><a href="#Streams-API" class="headerlink" title="Streams API"></a>Streams API</h4><ul>
<li>Githubにてオープンソースで提供</li>
<li>CCP（ソフトフォン）を操作するAPI群</li>
<li>CCPの機能拡張や表示項目の追加などができる</li>
<li>iFrame with hidden divとか使うとCCPは非表示のまま、拡張された機能のみ利用可能</li>
<li>connect integration whitelisting: URLフィルタを使って、CCPにアクセスできるURLをフィルタリング可能</li>
<li>こんな<a href="https://www.connectdemo.com" target="_blank" rel="noopener">サイト</a> もある。（AWSの中の人作のデモ環境だそうです）</li>
</ul>
<p>長くなりましたが、１日目はこのような流れで終了しました。<br>２日目は実際にハンズオンをしながらAmazonConnectでコールセンタソリューションを構築します。</p>
<h2 id="AmazonConnect-Session-day2-2018-11-07"><a href="#AmazonConnect-Session-day2-2018-11-07" class="headerlink" title="AmazonConnect Session#day2 2018.11.07"></a>AmazonConnect Session#day2 2018.11.07</h2><h3 id="Connectインスタンスの作成"><a href="#Connectインスタンスの作成" class="headerlink" title="Connectインスタンスの作成"></a>Connectインスタンスの作成</h3><p><img src="/images/20181202/photo_20181202_02.png" style="border:solid 1px #000000"></p>
<ul>
<li>通常2つのインスタンスを作ることが多い（本番・検証）</li>
<li>既存のAWS Directory Serviceにリンクしたり、SAML2.0認証も可能</li>
<li>自動で2つのS3Bucketが作成される。（通話履歴、出力されたレポート用と問い合わせフローログ）<ul>
<li>設定カスタマイズをする場合は、録音データの暗号化キー（KMS）の選択や録音データの保存先Bucketの変更が可能</li>
</ul>
</li>
<li>5Stepsでインスタンスに関わる全ての設定が完了する</li>
<li>CloudFormationでの構築も可能</li>
<li>後ほど、AmazonConnect &gt; インスタンスエイリアスから設定変更可能</li>
</ul>
<h3 id="インスタンスの設定"><a href="#インスタンスの設定" class="headerlink" title="インスタンスの設定"></a>インスタンスの設定</h3><p><img src="/images/20181202/photo_20181202_03.png" style="border:solid 1px #000000"></p>
<ul>
<li>テレフォニー：発信・着信をAmazonConnectで処理するか否かを設定</li>
<li>データストレージ：S3/KMSの設定が可能。クレジット情報などを扱う場合は暗号化必須です</li>
<li>データストリーミング：Kinesisに対するアクセス許可設定を有効にできる</li>
<li>アプリケーション統合：CRMやWFM（ワークフォースマネジメント）システムと統合する設定が可能</li>
<li>問い合わせフロー：問い合わせフローの暗号化、Amazon Lexとの統合、問い合わせフローログの設定が可能</li>
</ul>
<p>この画面に「管理者としてログイン」ボタンがありますが、セキュリティ的に通常利用はしないそう。（パスワード忘れのときのみ）AWSのルートアカウントのような扱いですね。<br>また、CloudFrontを経由しているらしく、若干不具合（ロードが遅いなど）が起きました（研修中に403 Error結構出てました…）</p>
<h3 id="1-電話番号の取得"><a href="#1-電話番号の取得" class="headerlink" title="1. 電話番号の取得"></a>1. 電話番号の取得</h3><p>ここからはAmazonConnectの管理画面にログインして操作をします。<br><img src="/images/20181202/photo_20181202_04.png"></p>
<ul>
<li>Direct Call（通常回線）とFree Dial（フリーダイヤル）が選択可能<ul>
<li>気になったのが、通常回線の050はいいとして、フリーダイヤルの0800は馴染みが無いですね</li>
</ul>
</li>
<li>電話番号の管理から電話番号の取得/削除及び番号ごとのIVRフローの変更を行うことができる</li>
<li>日本の電話番号が取れない事象も起こり、そこのリソース枯渇もスコープに入れ、事前に確保しておくと無難です</li>
</ul>
<h3 id="2-電話応対時間の設定"><a href="#2-電話応対時間の設定" class="headerlink" title="2. 電話応対時間の設定"></a>2. 電話応対時間の設定</h3><p><img src="/images/20181202/photo_20181202_05.png"></p>
<ul>
<li>オペレーション時間（Hours of operation）から実際に電話が通じる時間を決められる<ul>
<li>TimezoneをAsia/Tokyoにすること！</li>
<li>作成された段階ではキューがアタッチされていない状況になる。時間の適用を行う際はキューにアタッチをすること</li>
</ul>
</li>
</ul>
<h3 id="3-キューの作成"><a href="#3-キューの作成" class="headerlink" title="3. キューの作成"></a>3. キューの作成</h3><p><img src="/images/20181202/photo_20181202_06.png"></p>
<ul>
<li>先程作成したオペレーション時間（Hours of operation）との紐付けを行う</li>
<li>Outbound caller ID nameで通知される際の表示文字を決められる</li>
<li>Outbound caller ID numberで通知される電話番号を決められる</li>
<li>Outbound whisper flowでキューごとに最初の音声を変えられる</li>
<li>Maximum contacts in queueでキューに何人待たせられるかを決められる。ここは最大99になっているが、AWSへの緩和申請で上限を変えられる</li>
<li>Quick connectsで他のキューに飛ばしたり、転送したりが可能</li>
</ul>
<h3 id="4-プロンプトの作成"><a href="#4-プロンプトの作成" class="headerlink" title="4. プロンプトの作成"></a>4. プロンプトの作成</h3><p><img src="/images/20181202/photo_20181202_07.png"></p>
<ul>
<li>Create new promptにて相手に聞かせる音声の録音やファイルのアップロード（.wav）が可能</li>
<li>問い合わせフロー内でPollyに喋らせることも可能なので、フロー作成の際はそちらで手軽に代用も可能です</li>
<li>自然な人間の声や音楽が必要な際はここから登録して利用が必要です</li>
</ul>
<h3 id="5-問い合わせフローの作成"><a href="#5-問い合わせフローの作成" class="headerlink" title="5. 問い合わせフローの作成"></a>5. 問い合わせフローの作成</h3><p><img src="/images/20181202/photo_20181202_08.png" style="border:solid 1px #000000"></p>
<ul>
<li>サンプルのフローが20近くあり、それらを修正しても良いかも</li>
<li>基本はブロックをつないでいけばIVRが簡単に構築できます<ul>
<li>Lambdaやその他連携ツールへの理解があれば、現場のCS担当でも高度な改善ができるかも</li>
</ul>
</li>
<li>Contact Flows<ul>
<li>IVRで使用されるフローをグラフィカルに作成可能</li>
<li>8つのFlowを定義可能<ul>
<li>customer queue flow: キューに入ったときのフロー（ユースケース：品質改善のため録音しています、みたいな案内や録音処理、ロギング開始とか）</li>
<li>customer hold flow: 保留時にカスタマーが体験するフロー</li>
<li>customer whisper flow: エージェントが受信してから、実際に通話を初める直前にカスタマーが体験するフロー</li>
<li>outbound whisper flow: コールバックをエージェントが処理して、コールバック先が電話を取った時コールバックを受けた相手が体験するフロー（ユースケース：顧客情報を渡したり、引き継ぎ情報を渡したりできる）</li>
<li>agent hold flow: 保留時にエージェントが体験するフロー</li>
<li>agent whisper flow: エージェントが受信してから、実際に通話を初める直前にエージェントが体験するフロー（ユースケース：顧客情報を電話番号からDBに取得に行ってそれを表示するとか）</li>
<li>transfer to agent flow: あるエージェントが、別のエージェントにクイックコネクトで転送するときにあるエージェントが体験するフロー</li>
<li>transfer to queue flow: あるエージェントが、別のキューにクイックコネクトで転送するときにあるエージェントが体験するフロー</li>
</ul>
</li>
<li>これらのフローを使って、フロー全体が大きくなりすぎないよう、フローをつないで利用するのがベストプラクティスとのこと</li>
</ul>
</li>
</ul>
<h3 id="Contact-flow-designer"><a href="#Contact-flow-designer" class="headerlink" title="Contact flow designer"></a>Contact flow designer</h3><p><img src="/images/20181202/photo_20181202_09.png"></p>
<ul>
<li>GUIのドラッグ・アンド・ドロップでフローを構築可能</li>
<li>複雑になりがちなので、拡大縮小機能あり</li>
<li>上述しましたが、ベストプラクティスとしては小さいフローを構築し、それらを結合するのが良い</li>
<li>以下のSetやInteractはFlowの種類によって若干使えるものが違う。（例えばhold系のFlowではLoopで音楽を流すみたいな機能が使える）</li>
</ul>
<h4 id="Interact"><a href="#Interact" class="headerlink" title="Interact"></a>Interact</h4><ul>
<li>ユーザ入力などのインタラクティブな処理を行う</li>
<li>ユーザ入力の取得、保存、保留、プロンプトの再生が可能</li>
<li>すべてのコネクタを接続する必要があり、AWSで用意されているのは<ul>
<li>Error: Errorになったのをみたことは無いらしい</li>
<li>Default: 設定した番号以外をユーザが押した場合</li>
<li>Timeout: ユーザが何もしなかったとき</li>
</ul>
</li>
</ul>
<h4 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h4><ul>
<li>working queueを使ってLambdaと連携し、Dynamoと連携することで大量のキューを捌くなどのユースケースがある</li>
<li>set contact attributesを使うことでキューの状態や3rd-PartyのAPIから取得したデータを使って顧客の氏名を取得・利用したり、LambdaでDBにアクセスしてその返り値を利用したりできる</li>
<li>Routing ProfileでQueueに対するPriorityを変更できたが、IVRの選択や属性、時間を利用してContact flow designerにおいて呼単位でPriorityを変えることが可能。（キューでの滞留時間の長いものを優先的にする、など）</li>
<li>Call back numberを利用することで、キューの滞留が解消されたらシステムから電話するようなシステムフローを構築することも可能。ユーザは一旦電話を切ることができるし、その間は課金が発生しない</li>
<li>WebページにAPIを使ってCall backやCall meの機能を使うことが可能。もちろんAttributeも利用可能。</li>
</ul>
<h4 id="Branch"><a href="#Branch" class="headerlink" title="Branch"></a>Branch</h4><ul>
<li>ユーザ属性やキューの滞留などを基にフローを分けるなんてことも可能</li>
</ul>
<h4 id="Integrate"><a href="#Integrate" class="headerlink" title="Integrate"></a>Integrate</h4><ul>
<li>Lambdaを使って様々な外部サービスと接続できる</li>
<li>通常Lambdaはcoldからの起動になるので、Timeoutはデフォルトの3秒から最大の8秒に修正しましょう（特に最初のLambdaの起動）<ul>
<li>CloudWatchで30分間隔くらいでPingすれば常にWarmにしておける。</li>
</ul>
</li>
</ul>
<h4 id="Terminate-Transfer"><a href="#Terminate-Transfer" class="headerlink" title="Terminate / Transfer"></a>Terminate / Transfer</h4><ul>
<li>Transfer flowを使うことで他のフローに転送することができ、フローを小さく保つことが可能</li>
<li>Transfer Queueを使うことで後でかけ直す機能が作成できる。</li>
<li>Transfer to phone numberを使うことで固定電話や携帯電話に転送することができる。もちろん、IP電話でもOK</li>
</ul>
<h4 id="エラー処理"><a href="#エラー処理" class="headerlink" title="エラー処理"></a>エラー処理</h4><ul>
<li>適切なフローに投げるか接続を切るか、の処理を行い、通話の最初に戻る、みたいなことはLoopを起こすのでやめましょう。</li>
</ul>
<h4 id="6-ルーティングプロファイルの作成"><a href="#6-ルーティングプロファイルの作成" class="headerlink" title="6. ルーティングプロファイルの作成"></a>6. ルーティングプロファイルの作成</h4><p><img src="/images/20181202/photo_20181202_10.png"></p>
<ul>
<li>Routing profilesはキューのグループになる -&gt; 呼をどのようにエージェントに振り分けるかのグループ</li>
<li>キューごとにプライオリティを決めることが可能<ul>
<li>優先度（Priority）は小さい方が優先される</li>
<li>遅延（Delay）はキューに転送開始するまでの最低待ち時間</li>
</ul>
</li>
<li>キューの割当により、いわゆるスキルベース（エージェントのスキルによって割当を変える）のルーティングも可能</li>
</ul>
<h4 id="7-ユーザー管理"><a href="#7-ユーザー管理" class="headerlink" title="7. ユーザー管理"></a>7. ユーザー管理</h4><ul>
<li>ユーザーマネジメントからユーザの追加/削除が可能<ul>
<li>追加する場合は手動入力 or CSVに入力してアップロードすることも可能。</li>
<li>またはAPIから登録することも可能</li>
<li>Security Profilesからは4つのロールが選択可能（自分で自作することも可能）</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>ロール</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Agent</td>
<td>管理者用。全リソースへのアクセスと操作ができる。</td>
</tr>
<tr>
<td>CallCenterManager</td>
<td>エージェント用。Contact Control Panel (CCP)へのアクセスのみ可能。</td>
</tr>
<tr>
<td>Admin</td>
<td>コンタクトセンター管理者用。ユーザーとアクセス権限、メトリクスおよび品質、ルーティングへのアクセスが可能。</td>
</tr>
<tr>
<td>QualityAnalyst</td>
<td>スーパーバイザー、コンタクトセンター管理者用。メトリクスおよび品質へのアクセスのみ可能。</td>
</tr>
</tbody>
</table>
<h4 id="3rd-partyへのストリーム"><a href="#3rd-partyへのストリーム" class="headerlink" title="3rd-partyへのストリーム"></a>3rd-partyへのストリーム</h4><p>レイテンシは当然気になりますよね？<br>顧客 -&gt; Stream -&gt; Speech to Textサービスのような形で日本語で話した内容を解釈してマネージドサービスと連携できるいい方法がないか知りたかったのですが、現時点（2018/11/07）では無いようです。<br>ただ、今後、以下は予定されているとのことです。</p>
<ul>
<li>Kinesis video streamを利用可能になる予定</li>
<li>LEXの日本語対応はまだ先（ロードマップにはある）</li>
</ul>
<h2 id="実際に作ってみた"><a href="#実際に作ってみた" class="headerlink" title="実際に作ってみた"></a>実際に作ってみた</h2><p>ECサイトで注文情報の変更・キャンセルを行う自動応答システムをイメージして、コールフローを作成しました。<br>前提として、注文番号とかお問い合わせ番号は長すぎるので、ダイアルプッシュでなく音声認識で受付ができるようにしました。</p>
<h3 id="フロー"><a href="#フロー" class="headerlink" title="フロー"></a>フロー</h3><ol>
<li>電話をかけるとDynamoDBから最近の注文情報を電話番号を基に取得して、「最近〇〇をご購入いただきましたが、これに関するご質問ですか？」と音声を流します</li>
<li>ここはIVRで分岐としました<ol>
<li>AIボットで注文情報の変更　→　「Change shipping date.」「Day after tomorrow」とか言うとDynamoDBにLambda経由で更新</li>
<li>キャンセル　→　DynamoDBにLambda経由でキャンセルフラグを更新</li>
<li>オペレータに電話転送</li>
</ol>
</li>
<li>上記1と2の場合はSMSもLambda経由で送るようにしました</li>
</ol>
<h4 id="全体図"><a href="#全体図" class="headerlink" title="全体図"></a>全体図</h4><p><img src="/images/20181202/photo_20181202_11.png" style="border:solid 1px #000000"></p>
<h3 id="手順"><a href="#手順" class="headerlink" title="手順"></a>手順</h3><p>前提として、電話番号の取得などの基本的な設定は完了しているものとして進めます。</p>
<h4 id="先ずはDynamoDBにデモ用のデータを入れます。"><a href="#先ずはDynamoDBにデモ用のデータを入れます。" class="headerlink" title="先ずはDynamoDBにデモ用のデータを入れます。"></a>先ずはDynamoDBにデモ用のデータを入れます。</h4><ul>
<li>JustinさんがNorth Faceのジャケットを買った的なデータです。</li>
</ul>
<figure class="highlight sh"><figcaption><span>デモデータ投入</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ aws dynamodb put-item --table-name CustomerTable --item <span class="string">'&#123;</span></div><div class="line">    "orderId": &#123;"S": "100"&#125;,</div><div class="line">    "orderedItem": &#123;"S": "The North Face Summit L3 Ventrix 2.0 Hoodie"&#125;,</div><div class="line">    "shippingDate": &#123;"S": "2018-11-12"&#125;,</div><div class="line">    "customerName": &#123;"S": "Justin"&#125;,</div><div class="line">    "CallerID": &#123;"S": "+819012345678"&#125;,</div><div class="line">    "isCancel": &#123;"S": "False"&#125;</div><div class="line">&#125;'</div></pre></td></tr></table></figure>
<h4 id="Lambdaを作成します。今回は下記の３つを作成しました。"><a href="#Lambdaを作成します。今回は下記の３つを作成しました。" class="headerlink" title="Lambdaを作成します。今回は下記の３つを作成しました。"></a>Lambdaを作成します。今回は下記の３つを作成しました。</h4><p>エラーチェックとか不要な変数とか目を瞑ってください・・・。<br>キャンセルTrueでも配送日変更できるなどツッコミどころ満載です。</p>
<pre><code>1. 注文情報の読み込み（直近の注文履歴参照用）
2. キャンセル書き込み用
3. 配送情報変更用
</code></pre><h5 id="注文情報の読み込み（直近の注文履歴参照用）"><a href="#注文情報の読み込み（直近の注文履歴参照用）" class="headerlink" title="注文情報の読み込み（直近の注文履歴参照用）"></a>注文情報の読み込み（直近の注文履歴参照用）</h5><figure class="highlight javascript"><figcaption><span>OrderTableRead</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> AWS = <span class="built_in">require</span>(<span class="string">"aws-sdk"</span>);</div><div class="line"><span class="keyword">var</span> docClient = <span class="keyword">new</span> AWS.DynamoDB.DocumentClient();</div><div class="line"></div><div class="line">exports.handler = <span class="function">(<span class="params">event, context, callback</span>) =&gt;</span> &#123;</div><div class="line">  <span class="keyword">var</span> CallerID = event.Details.Parameters.CallerID;</div><div class="line">  <span class="keyword">var</span> paramsQuery = &#123;</div><div class="line">    <span class="attr">TableName</span>: <span class="string">'CustomerTable'</span>,</div><div class="line">    <span class="attr">KeyConditionExpression</span>: <span class="string">"CallerID = :varNumber"</span>,</div><div class="line">    <span class="attr">ExpressionAttributeValues</span>: &#123; <span class="string">":varNumber"</span>: CallerID &#125;</div><div class="line">  &#125;;</div><div class="line"></div><div class="line">  docClient.query(paramsQuery, <span class="function"><span class="keyword">function</span>(<span class="params">err, data</span>) </span>&#123;</div><div class="line">    <span class="keyword">if</span> (err) &#123;</div><div class="line">      <span class="built_in">console</span>.log(err);</div><div class="line">      callback(<span class="literal">null</span>, buildResponse(<span class="literal">false</span>));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="built_in">console</span>.log(<span class="string">"DynamoDB Query Results:"</span> + <span class="built_in">JSON</span>.stringify(data));</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (data.Items.length === <span class="number">0</span>) &#123;</div><div class="line">        <span class="built_in">console</span>.log(<span class="string">"Customer not Found in CustomerTable"</span>);</div><div class="line">        <span class="keyword">var</span> recordFound = <span class="string">"False"</span>;</div><div class="line">        callback(<span class="literal">null</span>, buildResponse(<span class="literal">true</span>, recordFound));</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">var</span> recordFound = <span class="string">"True"</span></div><div class="line">        <span class="keyword">var</span> customerName = data.Items[<span class="number">0</span>].customerName;</div><div class="line">        <span class="keyword">var</span> orderId = data.Items[<span class="number">0</span>].orderId;</div><div class="line">        <span class="keyword">var</span> orderedItem = data.Items[<span class="number">0</span>].orderedItem;</div><div class="line">        <span class="keyword">var</span> shippingDate = data.Items[<span class="number">0</span>].shippingDate;</div><div class="line">        <span class="keyword">var</span> isCancel = data.Items[<span class="number">0</span>].isCancel;</div><div class="line">        callback(<span class="literal">null</span>, buildResponse(<span class="literal">true</span>, recordFound, customerName, orderId, orderedItem, shippingDate, isCancel));</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;);</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">buildResponse</span>(<span class="params">isSuccess, recordFound, customerName, orderId, orderedItem, shippingDate, isCancel</span>) </span>&#123;</div><div class="line">  <span class="keyword">if</span> (isSuccess) &#123;</div><div class="line">        <span class="keyword">return</span> &#123;</div><div class="line">        <span class="attr">recordFound</span>: recordFound,</div><div class="line">        <span class="attr">customerName</span>: customerName,</div><div class="line">        <span class="attr">orderId</span>: orderId,</div><div class="line">        <span class="attr">orderedItem</span>: orderedItem,</div><div class="line">        <span class="attr">shippingDate</span>: shippingDate,</div><div class="line">        <span class="attr">isCancel</span>: isCancel,</div><div class="line">        <span class="attr">lambdaResult</span>: <span class="string">"Success"</span></div><div class="line">        &#125;;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    <span class="built_in">console</span>.log(<span class="string">"Lambda returned error to Connect"</span>);</div><div class="line">    <span class="keyword">return</span> &#123; <span class="attr">lambdaResult</span>: <span class="string">"Error"</span> &#125;;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="キャンセル書き込み用"><a href="#キャンセル書き込み用" class="headerlink" title="キャンセル書き込み用"></a>キャンセル書き込み用</h5><figure class="highlight javascript"><figcaption><span>CancelOrder</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> AWS = <span class="built_in">require</span>(<span class="string">"aws-sdk"</span>);</div><div class="line"><span class="keyword">var</span> docClient = <span class="keyword">new</span> AWS.DynamoDB.DocumentClient();</div><div class="line"><span class="keyword">var</span> sns = <span class="keyword">new</span> AWS.SNS();</div><div class="line"></div><div class="line">exports.handler = <span class="function">(<span class="params">event, context, callback</span>) =&gt;</span> &#123;</div><div class="line">  <span class="comment">// 発信電話番号の取得</span></div><div class="line">  <span class="keyword">var</span> CallerID = event.Details.ContactData.CustomerEndpoint.Address;</div><div class="line">  <span class="comment">// 商品名, ID</span></div><div class="line">  <span class="keyword">var</span> orderedItem = event.Details.Parameters.orderedItem;</div><div class="line">  <span class="keyword">var</span> orderId = event.Details.Parameters.orderId;</div><div class="line"></div><div class="line">  <span class="keyword">var</span> params = &#123;</div><div class="line">    <span class="attr">TableName</span>: <span class="string">"CustomerTable"</span>,</div><div class="line">    <span class="attr">Key</span>:&#123;</div><div class="line">        <span class="string">"CallerID"</span>: CallerID</div><div class="line">    &#125;,</div><div class="line">    <span class="attr">UpdateExpression</span>: <span class="string">"set isCancel = :c"</span>,</div><div class="line">    <span class="attr">ExpressionAttributeValues</span>: &#123;</div><div class="line">        <span class="string">":c"</span>: <span class="string">"True"</span></div><div class="line">    &#125;,</div><div class="line">    <span class="attr">ReturnValues</span>: <span class="string">"UPDATED_NEW"</span></div><div class="line">  &#125;;</div><div class="line"></div><div class="line">  docClient.update(params, <span class="function"><span class="keyword">function</span>(<span class="params">err, data</span>) </span>&#123;</div><div class="line">    <span class="keyword">if</span> (err) &#123;</div><div class="line">      <span class="built_in">console</span>.log(err);</div><div class="line">      callback(<span class="literal">null</span>, buildResponse(<span class="literal">false</span>));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="built_in">console</span>.log(<span class="string">"DynamoDB record updated:"</span> + params);</div><div class="line">        <span class="keyword">var</span> SMSparams = &#123;</div><div class="line">                    <span class="attr">Message</span>: <span class="string">'We changed state to cancel.\nItem Name: '</span> + orderedItem + <span class="string">'\nOrder ID: '</span> + orderId,</div><div class="line">                    <span class="attr">MessageStructure</span>: <span class="string">'string'</span>,</div><div class="line">                    <span class="attr">PhoneNumber</span>: CallerID</div><div class="line">                    &#125;;</div><div class="line">            sns.publish(SMSparams, <span class="function"><span class="keyword">function</span>(<span class="params">err, data</span>) </span>&#123;</div><div class="line">                    <span class="keyword">if</span> (err) <span class="built_in">console</span>.log(err, err.stack); <span class="comment">// an error occurred</span></div><div class="line">                    <span class="keyword">else</span>     <span class="built_in">console</span>.log(data);           <span class="comment">// successful response</span></div><div class="line">                    callback(<span class="literal">null</span>, buildResponse(<span class="literal">true</span>));</div><div class="line">                    &#125;);</div><div class="line">            &#125;</div><div class="line">  &#125;);</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">buildResponse</span>(<span class="params">isSuccess</span>) </span>&#123;</div><div class="line">  <span class="keyword">if</span> (isSuccess) &#123;</div><div class="line">    <span class="keyword">return</span> &#123;</div><div class="line">      <span class="attr">lambdaResult</span>: <span class="string">"Success"</span></div><div class="line">    &#125;;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    <span class="built_in">console</span>.log(<span class="string">"Lambda returned error to Connect"</span>);</div><div class="line">    <span class="keyword">return</span> &#123; <span class="attr">lambdaResult</span>: <span class="string">"Error"</span> &#125;;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h5 id="配送日変更用"><a href="#配送日変更用" class="headerlink" title="配送日変更用"></a>配送日変更用</h5><figure class="highlight javascript"><figcaption><span>ChangeOrderDate</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">var</span> AWS = <span class="built_in">require</span>(<span class="string">"aws-sdk"</span>);</div><div class="line"><span class="keyword">var</span> docClient = <span class="keyword">new</span> AWS.DynamoDB.DocumentClient();</div><div class="line"><span class="keyword">var</span> sns = <span class="keyword">new</span> AWS.SNS();</div><div class="line"></div><div class="line">exports.handler = <span class="function">(<span class="params">event, context, callback</span>) =&gt;</span> &#123;</div><div class="line">  <span class="comment">// 発信電話番号の取得</span></div><div class="line">  <span class="keyword">var</span> CallerID = event.Details.ContactData.CustomerEndpoint.Address;</div><div class="line">  <span class="comment">// 商品名, ID</span></div><div class="line">  <span class="keyword">var</span> orderedItem = event.Details.Parameters.orderedItem;</div><div class="line">  <span class="keyword">var</span> orderId = event.Details.Parameters.orderId;</div><div class="line">  <span class="comment">// 発送日</span></div><div class="line">  <span class="keyword">var</span> shippingDate = event.Details.Parameters.shippingDate;</div><div class="line"></div><div class="line">  <span class="keyword">var</span> params = &#123;</div><div class="line">    <span class="attr">TableName</span>: <span class="string">"CustomerTable"</span>,</div><div class="line">    <span class="attr">Key</span>:&#123;</div><div class="line">        <span class="string">"CallerID"</span>: CallerID</div><div class="line">    &#125;,</div><div class="line">    <span class="attr">UpdateExpression</span>: <span class="string">"set shippingDate = :s"</span>,</div><div class="line">    <span class="attr">ExpressionAttributeValues</span>: &#123;</div><div class="line">        <span class="string">":s"</span>: shippingDate</div><div class="line">    &#125;,</div><div class="line">    <span class="attr">ReturnValues</span>: <span class="string">"UPDATED_NEW"</span></div><div class="line">  &#125;;</div><div class="line"></div><div class="line">  docClient.update(params, <span class="function"><span class="keyword">function</span>(<span class="params">err, data</span>) </span>&#123;</div><div class="line">    <span class="keyword">if</span> (err) &#123;</div><div class="line">      <span class="built_in">console</span>.log(err);</div><div class="line">      callback(<span class="literal">null</span>, buildResponse(<span class="literal">false</span>));</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="built_in">console</span>.log(<span class="string">"DynamoDB record updated:"</span> + params);</div><div class="line">        <span class="keyword">var</span> SMSparams = &#123;</div><div class="line">                    <span class="attr">Message</span>: <span class="string">'We changed shipping date to '</span> + shippingDate + <span class="string">'\nItem Name: '</span> + orderedItem + <span class="string">'\nOrder ID: '</span> + orderId,</div><div class="line">                    <span class="attr">MessageStructure</span>: <span class="string">'string'</span>,</div><div class="line">                    <span class="attr">PhoneNumber</span>: CallerID</div><div class="line">                    &#125;;</div><div class="line">            sns.publish(SMSparams, <span class="function"><span class="keyword">function</span>(<span class="params">err, data</span>) </span>&#123;</div><div class="line">                    <span class="keyword">if</span> (err) <span class="built_in">console</span>.log(err, err.stack); <span class="comment">// an error occurred</span></div><div class="line">                    <span class="keyword">else</span>     <span class="built_in">console</span>.log(data);           <span class="comment">// successful response</span></div><div class="line">                    callback(<span class="literal">null</span>, buildResponse(<span class="literal">true</span>));</div><div class="line">                    &#125;);</div><div class="line">            &#125;</div><div class="line">  &#125;);</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">buildResponse</span>(<span class="params">isSuccess</span>) </span>&#123;</div><div class="line">  <span class="keyword">if</span> (isSuccess) &#123;</div><div class="line">    <span class="keyword">return</span> &#123;</div><div class="line">      <span class="attr">lambdaResult</span>: <span class="string">"Success"</span></div><div class="line">    &#125;;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    <span class="built_in">console</span>.log(<span class="string">"Lambda returned error to Connect"</span>);</div><div class="line">    <span class="keyword">return</span> &#123; <span class="attr">lambdaResult</span>: <span class="string">"Error"</span> &#125;;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="AmazonConnectからLambdaを呼び出すためのパーミッションの設定をします。（現状GUIからは設定不可）"><a href="#AmazonConnectからLambdaを呼び出すためのパーミッションの設定をします。（現状GUIからは設定不可）" class="headerlink" title="AmazonConnectからLambdaを呼び出すためのパーミッションの設定をします。（現状GUIからは設定不可）"></a>AmazonConnectからLambdaを呼び出すためのパーミッションの設定をします。（現状GUIからは設定不可）</h4><ul>
<li>併せてLambdaからDynamoDB/SNSへのFullAccessも許可しておきます（デモなのでセキュリティはご愛嬌で・・・）</li>
<li>本来はアクセスすべきリソースだけ指定してください<ul>
<li>読み込みだけのLambdaならDynamoDBへのReadOnlyなど</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><figcaption><span>AmazonConnectからLambdaを呼び出すためのパーミッションの設定</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">aws lambda add-permission --function-name <span class="keyword">function</span>:&lt;Lambda Function名&gt; \</div><div class="line"> --statement-id 1 --principal connect.amazonaws.com  \</div><div class="line"> --action lambda:InvokeFunction --source-account &lt;AWSアカウントID&gt; \</div><div class="line"> --source-arn &lt;AmazonConnect ARN&gt;</div></pre></td></tr></table></figure>
<blockquote>
<p>※ ARNはAWSコンソール &gt; AmazonConnect &gt; インスタンスエイリアス名　から確認できます。<br><img src="/images/20181202/photo_20181202_12.png" style="border:solid 1px #000000"></p>
</blockquote>
<h4 id="Lexとの連携を作成します。"><a href="#Lexとの連携を作成します。" class="headerlink" title="Lexとの連携を作成します。"></a>Lexとの連携を作成します。</h4><p>先ずはLexチャットボットを用意しましょう。<br>右端の<code>Test ChatBot</code>からチャットボットのテストができます。</p>
<p>今回は下記しか設定してません。</p>
<ul>
<li>Sample utterances：ここでの言葉に反応してLexがSlotを呼びます。</li>
<li>Slots：”Sure, when?”とLexが聞いて来るので、その後の言葉をAMAZON.DATE型として認識します。</li>
</ul>
<p><img src="/images/20181202/photo_20181202_13.png" style="border:solid 1px #000000"></p>
<h5 id="Amazon-LexとConnectを接続します。"><a href="#Amazon-LexとConnectを接続します。" class="headerlink" title="Amazon LexとConnectを接続します。"></a>Amazon LexとConnectを接続します。</h5><p>下図の通り、Lexボットを作成したリージョンとボット名を選択して追加します。</p>
<p><img src="/images/20181202/photo_20181202_14.png" style="border:solid 1px #000000"></p>
<h4 id="最後にフローを作成します。"><a href="#最後にフローを作成します。" class="headerlink" title="最後にフローを作成します。"></a>最後にフローを作成します。</h4><p><img src="/images/20181202/photo_20181202_15.png" style="border:solid 1px #000000"></p>
<p>かいつまんで説明すると、吹き出し部分が重要なところで、データの受け渡しで若干躓くところです。<br>個別に見ていきましょう。</p>
<h5 id="電話番号の取得"><a href="#電話番号の取得" class="headerlink" title="電話番号の取得"></a>電話番号の取得</h5><p>電話をかけてきた相手の電話番号はシステムの値で取得可能です。<br>宛先キーは使いやすい名称を設定すれば良いと思います。（今回はCallerIDでいきます）<br><img src="/images/20181202/photo_20181202_16.png" class="img-middle-size" style="border:solid 1px #000000"></p>
<h5 id="Lambdaの呼び出し"><a href="#Lambdaの呼び出し" class="headerlink" title="Lambdaの呼び出し"></a>Lambdaの呼び出し</h5><p>先程取得したCallerID（発信者電話番号）を引数としてセットします。<br>呼び出すLambdaのARNは関数画面から取得してきましょう。<br><img src="/images/20181202/photo_20181202_17.png" class="img-middle-size" style="border:solid 1px #000000"></p>
<h5 id="値のチェック"><a href="#値のチェック" class="headerlink" title="値のチェック"></a>値のチェック</h5><p>Lambda関数の戻り値をチェックできます。<br>ここでは発信者電話番号をキーとした注文情報レコードの有無を確認することを想定しています。<br><img src="/images/20181202/photo_20181202_18.png" class="img-middle-size" style="border:solid 1px #000000"></p>
<h5 id="ダイヤルプッシュを取得（IVR）"><a href="#ダイヤルプッシュを取得（IVR）" class="headerlink" title="ダイヤルプッシュを取得（IVR）"></a>ダイヤルプッシュを取得（IVR）</h5><p>IVRの機能を枠ひとつでできてしまいます！<br>ここは特につまずくことなく設定可能かなと思います。<br><img src="/images/20181202/photo_20181202_19.png" style="border:solid 1px #000000"></p>
<h5 id="キャンセル関数"><a href="#キャンセル関数" class="headerlink" title="キャンセル関数"></a>キャンセル関数</h5><p>複数の引数を持つLambda関数にもちゃんとデータを渡すことができます。<br><img src="/images/20181202/photo_20181202_20.png" style="border:solid 1px #000000"></p>
<h5 id="Lexで顧客の入力を取得"><a href="#Lexで顧客の入力を取得" class="headerlink" title="Lexで顧客の入力を取得"></a>Lexで顧客の入力を取得</h5><p>前項の「ダイヤルプッシュを取得（IVR）」と同様のことをLexを用いて実装可能です。<br>ボットを選択するだけなので、とても簡単に追加できます。<br><img src="/images/20181202/photo_20181202_21.png" style="border:solid 1px #000000"></p>
<h5 id="配送日変更関数の呼び出し"><a href="#配送日変更関数の呼び出し" class="headerlink" title="配送日変更関数の呼び出し"></a>配送日変更関数の呼び出し</h5><p>以前までと違うのが、Lexで取得してきた値を引数としたい点です。<br>素晴らしいことにこれもGUIで簡単に取れてしまいます。<br>（Lexの部分だけ抜き取っています。）<br><img src="/images/20181202/photo_20181202_22.png" class="img-middle-size" style="border:solid 1px #000000"></p>
<h5 id="完成！"><a href="#完成！" class="headerlink" title="完成！"></a>完成！</h5><p>これでフローの完成です！長丁場お疲れ様でした！<br>右上の保存・・・を押しても反映されないので、保存の右の▼をクリック→保存して発行を実行してください。</p>
<p>あとはしばらくして電話をかければ動作を確認できます！<br>多少クセはありますがとっても簡単ですね。</p>
<h2 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h2><p>とっても長くなってしまいましたが、２日間非常に濃い内容のセッションでした。<br>やはりLambdaとの連携がとても強力で、基幹のシステムと連携して様々な活用が想像できますね！<br>AmazonConnectならではの強みを活かしてEC2インスタンスの再起動や、運用監視向けのシステムを作られている方もいて、非常に夢が広がるツールだと思いました。</p>
<p>この実習での作品が発想勝ちだったのか、AmazonConnectサンプルコールセンター対決で優勝をいただきましてechospotをもらえました！（ありがとうございます！）<br><img src="/images/20181202/photo_20181202_23.jpeg"></p>
<h2 id="参考資料"><a href="#参考資料" class="headerlink" title="参考資料"></a>参考資料</h2><ul>
<li><a href="https://aws.amazon.com/blogs/contact-center/" target="_blank" rel="noopener">Amazon Connect Contact Center Blog Channel</a></li>
<li><a href="https://aws.amazon.com/jp/blogs/contact-center/building-an-automated-ai-experience-with-amazon-connect-and-salesforce-service-cloud/" target="_blank" rel="noopener">Salesforce Connector information</a></li>
<li><a href="https://aws.amazon.com/blogs/contact-center/use-amazon-connect-data-in-real-time-with-elasticsearch-and-kibana/?nc1=b_rp" target="_blank" rel="noopener">Using Kibana to visualize contact center data</a></li>
<li><a href="https://aws.amazon.com/blogs/big-data/analyzing-amazon-connect-records-with-amazon-athena-aws-glue-and-amazon-quicksight/" target="_blank" rel="noopener">Using Athena and Quicksight to analyze Amazon Connect Data</a></li>
<li><a href="https://read.acloud.guru/how-to-keep-your-lambda-functions-warm-9d7e1aa6e2f0" target="_blank" rel="noopener">Keeping your Lambda functions warm</a></li>
<li><a href="https://blogs.perficient.com/2017/12/20/building-an-amazon-connect-holiday-calendar-in-4-easy-steps/" target="_blank" rel="noopener">Building an automated holiday table</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本記事は &lt;a href=&quot;https://qiita.com/advent-calendar/2018/future&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;フューチャー Advent Calendar 2018 2日目&lt;/a&gt;の記事です。&lt;/p&gt;
    
    </summary>
    
      <category term="Cloud" scheme="https://future-architect.github.io/categories/Cloud/"/>
    
    
      <category term="AWS" scheme="https://future-architect.github.io/tags/AWS/"/>
    
      <category term="AmazonConnect" scheme="https://future-architect.github.io/tags/AmazonConnect/"/>
    
  </entry>
  
  <entry>
    <title>その問い合わせ、AIが解決します！～Redmineチケットレコメンドシステムのご紹介～</title>
    <link href="https://future-architect.github.io/articles/20181031/"/>
    <id>https://future-architect.github.io/articles/20181031/</id>
    <published>2018-10-31T00:54:57.000Z</published>
    <updated>2018-11-05T03:23:44.245Z</updated>
    
    <content type="html"><![CDATA[<p>こんにちは、フューチャーアーキテクト2017年4月入社、TIG（Technology Innovation Group）所属の竹林です。<br>大学では主にIoTの研究をしており、趣味で作ったArduinoベースのIoTラジコンカーを<a href="https://future-architect.github.io/articles/20160718/">入社前にフューチャーのLT大会で発表したり</a>していました。</p>
<p>研修終了後のOJTではフューチャーの社内インフラ構築・運用に関する業務を担当・経験し、現在はQA(Quality Assurance)チームとして、フューチャーにおける各プロジェクト成果物の品質保証全般に関わるツールの改善業務やサポート業務などを主に行っております。</p>
<h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><h3 id="作ったもの"><a href="#作ったもの" class="headerlink" title="作ったもの"></a>作ったもの</h3><ul>
<li>AIコンシェルジュの<a href="https://future-architect.github.io/articles/20171005/"><strong>あいちゃん</strong></a>がパワーアップして帰ってきました！<ul>
<li>新たに「<strong>チケットレコメンド</strong>業務」をこなしてもらえるようになりました</li>
</ul>
</li>
<li>何が出来るのか<ul>
<li>社内ヘルプデスクで使われているチケット管理OSS「<a href="https://www.redmine.org" target="_blank" rel="noopener">Redmine</a>」に起票された問い合わせチケットの内容と過去の同様事例チケットを自動検索します</li>
<li>検索結果として得られた類似度の高いチケット(以下、<strong>類似チケット</strong>)を、関連チケットへの紐付けによりユーザに提供します</li>
</ul>
</li>
</ul>
<h4 id="デモ"><a href="#デモ" class="headerlink" title="デモ"></a>デモ</h4><ul>
<li><p>手順1. 社内問い合わせ窓口のRedmineにチケットを起票<br><img src="/images/20181031/1.png" class="img-middle-size"></p>
</li>
<li><p>手順2. 起票してから<strong>10~20秒後</strong>、画面をリロードする</p>
<ul>
<li>類似チケットが自動で紐付けられます！</li>
<li>紐付けられたチケットの順番は類似度の高い順でソート済み</li>
</ul>
<p><img src="/images/20181031/2.png" class="img-middle-size"></p>
</li>
</ul>
<h2 id="メリットはなにか"><a href="#メリットはなにか" class="headerlink" title="メリットはなにか"></a>メリットはなにか</h2><p>本システムを使うことにより、起票者(質問者)と回答者の双方が過去の回答チケットを検索する手間を省略することができます。</p>
<ul>
<li>チケットクローズまでの<strong>回答コストを削減</strong></li>
<li>削減により生まれた時間をメインタスクに割り当て、生産性を向上できる</li>
</ul>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://www.future.co.jp/" target="_blank" rel="noopener">フューチャー</a>では社内問い合わせのみならず、各プロジェクトの管理としてRedmineを活用していますが、改善要望の声が上がっていました。</p>
<ul>
<li>「過去チケットの検索に時間が掛かるので、何とかしてほしい」</li>
<li>「障害発生時に、過去の同様の事例を参照できる仕組みがほしい」 などなど…</li>
</ul>
<p>フューチャーでは、RedmineでのAI活用実績として既に、萩原さんによる<a href="https://future-architect.github.io/articles/20171005/">チケットカテゴリ振り分けシステム</a>があります。<br>今回、AI活用の次なるステップとして「<strong>全文検索アルゴリズム</strong>」技術に注目し、これがチケット検索の改善に活かせるのではないかと考え開発に踏み切りました。</p>
<h2 id="システムについて"><a href="#システムについて" class="headerlink" title="システムについて"></a>システムについて</h2><h3 id="使用したソフトウェア・ライブラリ"><a href="#使用したソフトウェア・ライブラリ" class="headerlink" title="使用したソフトウェア・ライブラリ"></a>使用したソフトウェア・ライブラリ</h3><ul>
<li>チケットレコメンドのアルゴリズムはPythonで実装しました。<ul>
<li>Redmineチケット分類・スコア化<ul>
<li>Python (3.5.2)<ul>
<li>mecab-python3 (0.7)</li>
<li>scikit-learn (0.19.0)</li>
<li>gensim(3.1.0)</li>
<li>Flask (0.12.2)</li>
</ul>
</li>
<li>Elasticsearch (5.6.10)</li>
</ul>
</li>
<li>ジョブ管理<ul>
<li>Jenkins (2.60.3)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="システム構成図"><a href="#システム構成図" class="headerlink" title="システム構成図"></a>システム構成図</h3><p>システムは大きく分けて、既存システムの「Redmine」グループと、今回新規構築する「チケットレコメンド」の2つのグループから構成されます。(下図点線枠部分)</p>
<p><img src="/images/20181031/3.png" class="img-middle-size"></p>
<p>「Redmine」グループでは、既に構築済みのRedmine APに向けて新たにRedmine拡張プラグインを開発・実装を実施しました。<br>拡張プラグインに実装した機能は以下の2つです。</p>
<ul>
<li>関連チケットへ「類似チケット」の表示・追加機能</li>
<li><a href="http://www.redmine.org/projects/redmine/wiki/Hooks_List" target="_blank" rel="noopener">Redmine Plugin Hook</a>を用いたチケット起票時でのJenkinsジョブキック＆類似チケットの自動付与機能</li>
</ul>
<p>チケットレコメンドグループ内のサーバは、ES(Elasticsearch)を除きすべてECS(ElasticContainerService)として新規構築しました。<br>ESは、AWSマネージドの<a href="https://aws.amazon.com/jp/elasticsearch-service/" target="_blank" rel="noopener">ElasticsearchService</a>を使用し、保守・運用コストを抑えることにしました。</p>
<hr>
<ul>
<li>補足: ECSの起動タイプについて<ul>
<li>ECSの起動タイプは「Fargate」と「EC2」の2種類から選べるが、今回はEC2を利用</li>
<li>Dockerイメージは社内のプライベートリポジトリで管理する必要があり、その場合だとEC2しか選択できない<ul>
<li>Fargate起動タイプは、AWS ECR又はDocker Hubリポジトリのみをサポートのため利用不可のため。</li>
<li>参照: <a href="https://docs.aws.amazon.com/ja_jp/AmazonECS/latest/developerguide/task_definition_parameters.html#standard_container_definition_params" target="_blank" rel="noopener">https://docs.aws.amazon.com/ja_jp/AmazonECS/latest/developerguide/task_definition_parameters.html#standard_container_definition_params</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>チケットレコメンドグループは、更に細かく分けると「ジョブ管理」「レコメンドAPIサーバ群」の2つから構成されます(下図オレンジ色枠線部分)</p>
<p><img src="/images/20181031/4.png" class="img-middle-size"></p>
<p>ジョブ管理の仕組みとして、Jenkinsを使用しました。Jenkinsでは、RedmineからWebhookとして飛んでくるリアルタイムジョブ、日次実行ジョブなどを管理します。<br>レコメンドAPIサーバ群では、入力として検索対象チケット本文を受け取り、出力として類似チケットの番号とスコアのセットを返します。</p>
<p>Redmine,ECS→ECS間のHTTPS/HTTP通信を実現するため、ALB(Application Load Balancer)において以下の振り分けルール設定を行っています。</p>
<ul>
<li>パスパターンが<code>/jenkins/*</code> → ECSのJenkinsサーバにリクエスト転送</li>
<li>パスパターンが<code>/flask-scdv/*</code> → ECSのSCDVサーバにリクエスト転送</li>
<li>パスパターンが<code>/flask-score/*</code> → ECSのスコア合算サーバにリクエスト転送</li>
</ul>
<p>参考: <a href="https://docs.aws.amazon.com/ja_jp/elasticloadbalancing/latest/application/load-balancer-listeners.html#path-conditions" target="_blank" rel="noopener">https://docs.aws.amazon.com/ja_jp/elasticloadbalancing/latest/application/load-balancer-listeners.html#path-conditions</a></p>
<h2 id="検索アルゴリズムについて"><a href="#検索アルゴリズムについて" class="headerlink" title="検索アルゴリズムについて"></a>検索アルゴリズムについて</h2><blockquote>
<p>※本セクションは、SAIG小池さんにより執筆していただきました。</p>
</blockquote>
<p>検索アルゴリズムは、「類似文書検索」と「キーワード検索」のハイブリッド手法を用いることにより、より精度を向上させる試みを行いました。</p>
<p>類似文書検索は、機械学習のトップカンファレンス<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> で発表された<strong>SCDV</strong>(sparse Composite Document Vector)と呼ばれるEmbedding手法を用い、キーワード検索は現在有力とされている<strong>BM25</strong>を用いました。</p>
<p>本システムのもっとも肝な部分は、SCDV（文書検索）×BM25（キーワード検索）の<strong>ハイブリッドアルゴリズム</strong>を実装した点にあります。詳細は下記で説明します。</p>
<p><br></p>
<h3 id="SCDVについて"><a href="#SCDVについて" class="headerlink" title="SCDVについて"></a>SCDVについて</h3><h4 id="SCDVの概要"><a href="#SCDVの概要" class="headerlink" title="SCDVの概要"></a>SCDVの概要</h4><p>SCDV(Sparse Composite Document Vectors)とは、簡単にいってしまえば文書（本ブログではチケット）を<strong>ベクトルに変換する技術</strong>です。<br>文書をベクトル化することによって、各文書の類似度を測ることができます。また、ベクトルに変換することにより、分類問題やクラスタリングなどさまざまなタスクに応用することができます。</p>
<p>文書のベクトル化手法は多く提案されていますが、個人的に分類タスクにてかなり精度が良い印象でした。<br>SCDVのアルゴリズムについてはここでは詳しくは述べませんが、<a href="https://arxiv.org/pdf/1612.06778.pdf" target="_blank" rel="noopener">論文</a>は非常にわかりやすいので、参考にしてもらえればと思います。</p>
<h4 id="SCDVの選定理由"><a href="#SCDVの選定理由" class="headerlink" title="SCDVの選定理由"></a>SCDVの選定理由</h4><p>みなさん今流行りのword2vecはご存知でしょうか。<br>word2vevは現在自然言語処理分野において最も注目されている技術の1つで、言葉通り単語をベクトルに変換する技術のことです。</p>
<p>word2vecの良い点は、<strong>教師なし学習</strong>で類似単語を獲得できる点にあります。<br>つまり、「PC」と「コンピューター」といった単語ベクトルの類似度は大きくなり、「PC」と「鉛筆」といった単語ベクトルの類似度は小さくなります。<br>w2vでは、表現が揺れていても（例、「PC」と「コンピューター」）、似たような単語ベクトルの獲得が可能です。</p>
<p>さて、前置きが長くなりましたが、SCDVを用いるメリットを説明します。</p>
<p>SCDVはword2vecを元に文書ベクトルを獲得します。したがって、上記のword2vecの表現の揺れが吸収でき、なおかつ<strong>文書同士の類似度を測ることができる</strong>のがSCDVのメリットと言えるでしょう。<br>選定理由としては、このような単語の揺れを吸収できる点・個人的に精度が良い印象だったという2点から、SCDVを選定しました。</p>
<p><br></p>
<h3 id="BM25について"><a href="#BM25について" class="headerlink" title="BM25について"></a>BM25について</h3><h4 id="BM25の概要"><a href="#BM25の概要" class="headerlink" title="BM25の概要"></a>BM25の概要</h4><p>BM25<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>は、キーワード検索アルゴリズムの1種です。</p>
<p>キーワード検索といっても、単純にキーワードで引っかかった文書を提示するようなものではなく、各キーワードに重みを付けて、<strong>キーワードと文書のマッチ度をスコア化する</strong>ことができます。<br>BM25は、キーワード検索においてかなりの威力を発揮しています。</p>
<p>今回はElastic Search(ES)にBM25が実装されていたので、<strong>ESを使えば簡単に利用することができます</strong>。</p>
<h3 id="BM25の選定理由"><a href="#BM25の選定理由" class="headerlink" title="BM25の選定理由"></a>BM25の選定理由</h3><p>キーワード検索のアルゴリズムは多くあります。その中で、BM25をなぜ選んだかというと、実験の結果BM25が最も検索精度がよかったのが理由です。多くのキーワード検索アルゴリズムを試し、ハイパーパラメーターがあるものはグリッドサーチ的なことを行った結果、BM25が<strong>最も高精度</strong>でした。</p>
<p><br></p>
<h3 id="SCDV×BM25のハイブリッド"><a href="#SCDV×BM25のハイブリッド" class="headerlink" title="SCDV×BM25のハイブリッド"></a>SCDV×BM25のハイブリッド</h3><p>さて、チケット検索の具体的なアルゴリズムについて話していきたいと思います。<br>SCDVを用いて全チケットをベクトル化します。クエリーとなるチケットを<code>q</code>、任意のチケットを<code>p</code>とおくと、SCDVによるチケットp,q間の類似度は<code>SCDVscore(p,q)</code>で表すことができます。</p>
<p>BM25ではチケットqの文書を形態素解析（単語に分解）し、ストップワーズの除去（「て」、「に」、「を」、「は」等多く頻出するが意味のない単語）を行ったあとに残ったワードをキーワードとして突っ込みます。<br>そうすると、チケットp,qの類似度として<code>BM25score(q,p)</code>が算出することができます。<br>SCDVscore(q,p)は、単純にコサイン類似度としました。</p>
<p>この2つのスコアに対して正規化を行い、重みwをかけ、線形和をとり最終的なスコアとして算出します。</p>
<p>$$Score(q,p) = BM25score(p,q)  + w * SCDVscore(p,q)$$</p>
<p>ここでの<code>w</code>は、どちらの<strong>重み</strong>を重視するかといったパラメーターで、<strong>教師あり学習</strong>を用いて決定します。<br>つまり、あらかじめ検索チケットと見つかってほしいチケットのデータセットを用意しておき、それに従って最適なwを見つけることをしています。今回は損失関数を定義するまでもないので、wの最適化にはバイナリーサーチを用いて、ある程度最適なwの更新にとどめています。</p>
<p>上記式は、BM25とSCDVの「おいしいところ取り」をしたい意図があります。</p>
<p>BM25とSCDVでは、検索結果に見つかってほしいチケットに差が生じました。<br>上記式では、適切な重み<code>w</code>を選ぶことにより、同じチケットでもSCDVでは見つかっているのに、BM25では見つけられないチケットが存在するような場合も検索することができるようになりました。まさに「<strong>おいしいところ取り</strong>」です😁</p>
<p>本アルゴリズムでのチケット検索実験の結果、最もよかった<strong>BM25の最高精度を8%上回る結果</strong>となりました。</p>
<h2 id="システムの解説"><a href="#システムの解説" class="headerlink" title="システムの解説"></a>システムの解説</h2><h3 id="システムの流れについて"><a href="#システムの流れについて" class="headerlink" title="システムの流れについて"></a>システムの流れについて</h3><p>ここからは、システムの処理の流れに沿って、本システムの解説を行います。<br>まず、本システムの処理フローについて説明します。</p>
<ol>
<li>(夜間処理)RedmineチケットをES(Elasticsearch)に格納する</li>
<li>(夜間処理)SCDVモデルを最新化する</li>
<li>(日中処理)Redmine拡張プラグインが、チケット起票タイミングでJenkinsをキック</li>
<li>(日中処理)SCDVモデル・ESのスコアをマージし、類似チケット候補のフィルターを実行</li>
<li>(日中処理)類似チケットセットをRedmine関連チケットに紐付ける</li>
</ol>
<p>処理フロー1～5を図で示すと以下のようになります。<br><img src="/images/20181031/5.png" class="img-middle-size"></p>
<p>以下、個々のフローの解説になります。</p>
<h3 id="1-夜間処理-RedmineチケットをES-Elasticsearch-に格納する"><a href="#1-夜間処理-RedmineチケットをES-Elasticsearch-に格納する" class="headerlink" title="1. (夜間処理)RedmineチケットをES(Elasticsearch)に格納する"></a>1. (夜間処理)RedmineチケットをES(Elasticsearch)に格納する</h3><p>まずは、類似チケットを検索するためのインデックスやモデルの最新化処理について説明します。<br>平日夜間(3:00)に、Jenkinsの定期実行機能をトリガとしてESインデックス内ドキュメント(※)の更新処理を実施するところからスタートします。</p>
<p>※インデックス: RDBにおける「テーブル」に相当。今回は、RedmineプロジェクトのIdentifier(識別子,RedmineサーバURLの<code>/projects/XXXX</code>の<code>XXXX</code>部分に相当)をインデックス名として使用<br>※ドキュメント: RDBにおける「レコード」に相当</p>
<p>まずは<a href="http://www.redmine.org/projects/redmine/wiki/Rest_Issues" target="_blank" rel="noopener">Redmine Issues API</a>より対象Redmineプロジェクトの全量チケットを取得します(下図①)。<br><img src="/images/20181031/6.png" class="img-middle-size"></p>
<p>ポイントとして、Issues APIでは一度に取得可能なチケット数は100までとなっています。<br>ですので、「<code>総チケット数/100</code>回分のリクエストを送信し、レスポンスのJSONデータをマージし保存する」ための実装をする必要があります。</p>
<p>続いて、取得できたチケット全量をESに送信し、ドキュメントとして格納します(下図①’)。<br><img src="/images/20181031/7.png" class="img-middle-size"></p>
<p>先程得たJSON形式のチケットデータを、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html" target="_blank" rel="noopener">Elasticsearch Bulk API</a>のフォーマットへと変換後、バルクインサートを実行します。<br>変換前のJSONフォーマットと変換後のバルクインサート用データバイナリのイメージは以下のとおりです。</p>
<figure class="highlight js"><figcaption><span>チケット変換前.json</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="string">"issues"</span>: [</div><div class="line">    &#123;</div><div class="line">      <span class="string">"id"</span>: <span class="number">1</span>,</div><div class="line">      <span class="string">"start_date"</span>: <span class="string">"201X-YY-ZZ"</span>,</div><div class="line">      <span class="comment">// 省略</span></div><div class="line">      <span class="string">"project"</span>: &#123;</div><div class="line">        <span class="string">"id"</span>: <span class="number">1</span>,</div><div class="line">        <span class="string">"name"</span>: <span class="string">"社内問い合わせプロジェクト"</span></div><div class="line">      &#125;,</div><div class="line">      <span class="string">"subject"</span>: <span class="string">"～～～のDB接続について"</span>,</div><div class="line">      <span class="string">"description"</span>: <span class="string">"～～～のDBコネクションが時間が立つと切れてしまっているようです。\r\nプロセス自体は上がったままなのですが、添付ファイルのログのように～～～とメッセージが出ます。\r\n\r\nお手数ですがよろしくお願い致します。\r\n"</span>,</div><div class="line">      <span class="comment">// 省略</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="string">"id"</span>: <span class="number">2</span>,</div><div class="line">      <span class="comment">// 省略</span></div><div class="line">　　&#125;,</div><div class="line">    <span class="comment">// 省略</span></div><div class="line">  ],</div><div class="line">  <span class="string">"total"</span>: <span class="number">3930</span>,</div><div class="line">  <span class="string">"max_id"</span>: <span class="number">4248</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight js"><figcaption><span>チケット変換後.dat</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&#123; <span class="string">"index"</span> : &#123;<span class="string">"_id"</span>: <span class="string">"1"</span> &#125; &#125;</div><div class="line">&#123;<span class="string">"id"</span>: <span class="number">1</span>, <span class="string">"start_date"</span>: <span class="string">"201X-YY-ZZ"</span>, <span class="comment">/*省略*/</span> <span class="string">"project"</span>: &#123;<span class="string">"id"</span>: <span class="number">1</span>, <span class="string">"name"</span>: <span class="string">"社内問い合わせプロジェクト"</span>&#125;, <span class="string">"subject"</span>: <span class="string">"～～～のDB接続について"</span>, <span class="string">"description"</span>: <span class="string">"～～～のDBコネクションが時間が立つと切れてしまっているようです。\r\nプロセス自体は上がったままなのですが、添付ファイルのログのように～～～とメッセージが出ます。\r\n\r\nお手数ですがよろしくお願い致します。\r\n"</span>, <span class="comment">/*省略*/</span>&#125;</div><div class="line">&#123; <span class="string">"index"</span> : &#123;<span class="string">"_id"</span>: <span class="string">"2"</span> &#125; &#125;</div><div class="line"><span class="comment">/*省略*/</span></div></pre></td></tr></table></figure>
<p>上記のバルクインサート用データへ変換したら、まずは既存のESドキュメントを一度消去します。</p>
<figure class="highlight sh"><figcaption><span>ES(5系)ドキュメント全消去.sh</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -X POST <span class="string">"<span class="variable">$&#123;ELASTICSEARCH_URL&#125;</span>/<span class="variable">$&#123;REDMINE_PROJECT_IDENTIFIER&#125;</span>/issues/_delete_by_query"</span></div></pre></td></tr></table></figure>
<p>続いて、以下のようにESに向けてバルクインサートを実行します。</p>
<figure class="highlight sh"><figcaption><span>ES(5系)バルクインサート実行例.sh</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -X POST <span class="string">"<span class="variable">$&#123;ELASTICSEARCH_URL&#125;</span>/<span class="variable">$&#123;REDMINE_PROJECT_IDENTIFIER&#125;</span>/issues/_bulk"</span> --data-binary @ target/<span class="variable">$&#123;REDMINE_PROJECT_IDENTIFIER&#125;</span>.dat</div></pre></td></tr></table></figure>
<h3 id="2-夜間処理-SCDVモデルを最新化する"><a href="#2-夜間処理-SCDVモデルを最新化する" class="headerlink" title="2. (夜間処理)SCDVモデルを最新化する"></a>2. (夜間処理)SCDVモデルを最新化する</h3><p>続いて、SCDVモデルの更新を行います。(下図②)<br><img src="/images/20181031/8.png" class="img-middle-size"></p>
<p>SCDVサーバはFlaskによるAPIサーバとして稼働しており、下記のURLにPOSTリクエストを送信することで、SCDVモデルの更新が可能となる仕組みです。</p>
<ul>
<li>リクエストURL: <code>http://[SCDVサーバのアドレス]/flask-scdv/v1/model/{redmine_project_identifier}</code></li>
</ul>
<p>SCDVサーバがリクエストを受信すると、ESからチケット取得・モデルの最新化を実行します。(下図②’)<br><img src="/images/20181031/9.png" class="img-middle-size"></p>
<p>SCDVはPythonを用いて実装していますが、こちらのソースコードの解説はボリュームが多めとなるため今回の記事では割愛とさせていただきます。</p>
<p>これで、夜間処理としての各種モデル最新化のための処理が完了しました。</p>
<h3 id="3-日中処理-Redmine拡張プラグインが、チケット起票タイミングでJenkinsをキック"><a href="#3-日中処理-Redmine拡張プラグインが、チケット起票タイミングでJenkinsをキック" class="headerlink" title="3. (日中処理)Redmine拡張プラグインが、チケット起票タイミングでJenkinsをキック"></a>3. (日中処理)Redmine拡張プラグインが、チケット起票タイミングでJenkinsをキック</h3><p>ここからは、ユーザが起票したチケットに対し類似チケットの付与をするまでの処理の解説となります。<br>まず、ユーザが問い合わせチケットを起票したタイミングでJenkinsのジョブを自動実行する仕組みを見てみましょう。</p>
<p>本レコメンドシステム用に実装したRedmine拡張プラグインにより、ユーザのチケット起票のタイミング(下図③)でJenkinsへのキックを可能とします(下図③’)。<br><img src="/images/20181031/10.png" class="img-middle-size"></p>
<p>こちらの仕組みの詳細は、<a href="https://future-architect.github.io/articles/20171005/#Redmine-Pluginの作成">過去の萩原さんの記事</a>を併せて参照下さい。</p>
<h3 id="4-日中処理-SCDVモデル・Elasticsearchのスコアをマージし、類似チケット候補のフィルターを実行"><a href="#4-日中処理-SCDVモデル・Elasticsearchのスコアをマージし、類似チケット候補のフィルターを実行" class="headerlink" title="4. (日中処理)SCDVモデル・Elasticsearchのスコアをマージし、類似チケット候補のフィルターを実行"></a>4. (日中処理)SCDVモデル・Elasticsearchのスコアをマージし、類似チケット候補のフィルターを実行</h3><p>続いて、Redmineよりチケット起票の通知を受け取ったJenkinsがスコア合算サーバに向けてリクエストを送信します。(下図④)<br><img src="/images/20181031/11.png" class="img-middle-size"></p>
<p>スコア合算サーバもSCDVサーバと同様、FlaskによるAPIサーバとして稼働しています。以下のようなPOSTリクエストを送信することで、レンスポンスとしてレコメンドチケットセットを得ることが出来ます。</p>
<ul>
<li>リクエストURL: <code>http://[スコア合算サーバのアドレス]/flask-score/v1/recommended_issue/{redmine_project_identifier}/{issue_id}</code></li>
<li>リクエストボディ: 下記JSON参照</li>
</ul>
<figure class="highlight json"><figcaption><span>スコア合算サーバ_レコメンドチケットセット取得APIリクエスト送信例</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"text"</span>: <span class="string">"～～～のDB接続について ～～～のDBコネクションが時間が立つと切れてしまっているようです。 プロセス自体は上がったままなのですが、添付ファイルのログのように～～～とメッセージが出ます。 お手数ですがよろしくお願い致します。"</span> // チケットのタイトル＋本文(説明欄)をクエリとして送信</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight json"><figcaption><span>スコア合算サーバ_レコメンドチケットセット取得APIレスポンス例</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"input_id"</span>: <span class="string">"1"</span>,</div><div class="line">  <span class="attr">"output_list"</span>: [ // スコアが高い順にソート済み</div><div class="line">    &#123;</div><div class="line">        <span class="attr">"id"</span>: <span class="string">"1559"</span>,</div><div class="line">        <span class="attr">"score"</span>: <span class="number">0.98765</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">        <span class="attr">"id"</span>: <span class="string">"11"</span>,</div><div class="line">        <span class="attr">"score"</span>: <span class="number">0.87654</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">        <span class="attr">"id"</span>: <span class="string">"2485"</span>,</div><div class="line">        <span class="attr">"score"</span>: <span class="number">0.65432</span></div><div class="line">    &#125;,</div><div class="line">    ...</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>スコア合算サーバでは、SCDV及びESより得たチケットセット・スコアを合算し(下図④’)、合算結果をレスポンスとして返却します。<br><img src="/images/20181031/12.png" class="img-middle-size"></p>
<p>SCDVとESのスコアのマージ方法ですが、片方のスコアを正規化した上で、お互いのスコアを合算させるという方式を取りました。</p>
<p>まずは、SCDVとESのスコアの仕様について比較してみましょう。</p>
<ul>
<li>SCDV<ul>
<li>最小値は0、最大値は1</li>
<li>スコアの例: 0.9198</li>
</ul>
</li>
<li>ES(Elasticsearch)<ul>
<li>最小値は0、最大値は不定</li>
<li>スコアの例: 613.9819</li>
</ul>
</li>
</ul>
<p>ESの最大値が不定となっていることが分かります。このため、ここではES側のスコアを正規化した上でSCDVのスコアと合算させるようにしました。</p>
<p>具体的には、ESスコアを1～0の範囲となるようESスコアの最大値<code>ESmax</code>と重み値<code>w</code>を使って正規化をした上で、1～0の値の範囲を取るSCDVのスコアとマージし、更にスコア順で再ソートを実行します(下図参照)。<br><img src="/images/20181031/13.png" class="img-middle-size"></p>
<p>上図の例で<code>チケットID=1111</code>が重複して2回出ているように、ESとSCDVのスコアを合算するためチケットの重複が発生します。<br>このため実際のプログラムでは、スコアの合算後にチケット重複分を取り除く処理を入れてから、レスポンスとしてチケットIDの一覧を返すようにしています。</p>
<h3 id="5-日中処理-類似チケットセットをRedmine関連チケットに紐付ける"><a href="#5-日中処理-類似チケットセットをRedmine関連チケットに紐付ける" class="headerlink" title="5. (日中処理)類似チケットセットをRedmine関連チケットに紐付ける"></a>5. (日中処理)類似チケットセットをRedmine関連チケットに紐付ける</h3><p>最後に、受け取ったレコメンドチケットセットを<a href="https://www.redmine.org/projects/redmine/wiki/Rest_IssueRelations" target="_blank" rel="noopener">Redmine Issue Relations API</a>に向けて送信すれば紐付けが完了します。(下図⑤)<br><img src="/images/20181031/14.png" class="img-middle-size"></p>
<p>今回は起票された1チケットにつき、スコアの高い順に4件(件数固定)の類似チケットの紐付けを行うようにします。<br>Issue Relations APIは1リクエストに付き1件の関連チケット紐付けを行いますので、リクエストを4回、紐付け先IDを変えながら送信します。</p>
<p>送信するPOSTリクエストは、以下のようになります。</p>
<ul>
<li>リクエストURL: <code>https://[Redmineサーバのアドレス]/redmine/issues/XX/relations.json</code></li>
<li>リクエストボディ: 下記JSON参照</li>
</ul>
<figure class="highlight json"><figcaption><span>Redmine_Issue_Relations_APIリクエスト送信例</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"relation"</span>: &#123;</div><div class="line">    <span class="attr">"issue_to_id"</span>: YY,</div><div class="line">    <span class="attr">"relation_type"</span>: <span class="string">"recommends"</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上記リクエストボディには、<code>relation_type: recommends</code>という独自の関連チケットタイプを用いております。<br>こちらについて、<code>relation_type: recommends</code>を含むリクエスト送信デフォルト環境のRedmineに向けて実行すると、下記のようにバリデーションエラーが発生してしまいます。</p>
<figure class="highlight json"><figcaption><span>relation_type</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"errors"</span>: [</div><div class="line">        <span class="string">"relation_type は一覧にありません。"</span></div><div class="line">    ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>そのため、<code>relation_type: recommends</code>をバリデータに許可してもらうためにモンキーパッチを作る必要があります。</p>
<p>関連チケットのモデルは<a href="https://www.redmine.org/projects/redmine/repository/entry/branches/3.4-stable/app/models/issue_relation.rb" target="_blank" rel="noopener">app/models/issue_relation.rb</a>で管理しているため、こちらのソースコードで定義されたvalidatorに対するモンキーパッチを当てることで対応しました。</p>
<h3 id="工夫ポイント-Docker-ECSを使ってサービスのコンテナ化を実現"><a href="#工夫ポイント-Docker-ECSを使ってサービスのコンテナ化を実現" class="headerlink" title="工夫ポイント: Docker+ECSを使ってサービスのコンテナ化を実現"></a>工夫ポイント: Docker+ECSを使ってサービスのコンテナ化を実現</h3><p>EC2などを使ってOS上に直接Pythonプログラムを載せるのではなく、機能単位でFlask APIサーバ化＆Dockerイメージ化をしました。<br>これにより、AWS ECSの使用が可能となり、多くの恩恵を受けることが出来ました。</p>
<ul>
<li>ST環境、本番環境の<strong>構築コストが大幅に削減</strong><ul>
<li>やることは「DockerイメージのPush」「タスク定義の作成」「サービス定義の作成」「クラスタの作成」のみ</li>
</ul>
</li>
<li>ST環境、本番環境の<strong>運用コストも大幅に削減</strong><ul>
<li>アップデート作業も「DockerイメージのPush」「タスク定義の更新」「サービス定義の更新」の3ステップで完了</li>
<li>スケールアップやスケールアウトもECSのWebコンソール画面から操作可能</li>
</ul>
</li>
</ul>
<h2 id="結果と今後の展望"><a href="#結果と今後の展望" class="headerlink" title="結果と今後の展望"></a>結果と今後の展望</h2><h3 id="導入した結果"><a href="#導入した結果" class="headerlink" title="導入した結果"></a>導入した結果</h3><ul>
<li>類似チケットの精度について<ul>
<li>運用・保守フェーズを担当する1プロジェクトにご協力頂き、「チケットに紐付けられた類似チケットが参考になるか？」をPJメンバの皆様に見てもらいました。<ul>
<li>サンプリング対象: 26チケット</li>
<li>チケットで扱う内容: クライアントからの問い合わせ、サーバメンテナンス・障害連絡など(定常業務・非定常業務の双方含む)</li>
</ul>
</li>
<li>結果<ul>
<li>26件中、22件(<strong>84.6%</strong>)のチケットに対して「いずれか1つ以上存在の類似チケットが<strong>参考になった</strong>」との回答をいただきました。</li>
</ul>
</li>
</ul>
</li>
<li>システムの応答時間について<ul>
<li>類似チケットの探索時間は、検索対象チケット数に比例して増加します。</li>
<li>チケット起票から類似チケットがRedmineに紐づくまでの時間は、対象チケット<strong>4000件</strong>のRedmineプロジェクトにおいて平均<strong>9.39秒</strong>という結果でした。</li>
</ul>
</li>
</ul>
<h3 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h3><ul>
<li>想像以上にいい精度が出てよかった<ul>
<li>開発着手前は、参考になる類似チケットが含まれるのはせいぜい5割程度と見積もっていましたが、予想以上の結果(8割超)を得ることが出来ました。</li>
</ul>
</li>
<li>少人数・短期間で、簡単に構築することができた<ul>
<li>設計・開発は私一人がメインで、レビューやAI技術に関するアドバイスとしてTIG(Technology Innovation Group)・SAIG(Strategic AI Group)の先輩方にご協力をいただきました。</li>
<li>ECSを用いることで、サーバ管理が<strong>かなり楽</strong>になりました。<ul>
<li>DockerイメージとECSタスク定義さえ作れば、EC2のような初期構築作業が一切不要</li>
<li>メモリリソース不足等でサーバダウンの際は自動で再起動してくれる</li>
</ul>
</li>
</ul>
</li>
<li>紙とペンを使った、手書きのアウトプットの有用性に気付かされた<ul>
<li>システムの設計段階においてサーバ構成やデータ処理の流れを考える必要がありましたが、うまくイメージがまとまらず、今までのように「頭の中で考えて、イメージが固まってからパワポ等で作る」だけでは通用しないことが分かりました。</li>
<li>そこで、ノートをアウトプットの土台とし「まずは頭に浮かんだ個々のイメージをノートに吐き出す」→「出来上がった全体像を俯瞰して、おかしい部分を修正する」という作業に落とし込むことで、スムーズに設計作業を進めることが出来ました。</li>
</ul>
</li>
</ul>
<h3 id="苦労したところ・ハマりポイント"><a href="#苦労したところ・ハマりポイント" class="headerlink" title="苦労したところ・ハマりポイント"></a>苦労したところ・ハマりポイント</h3><ul>
<li>AWS ElasticsearchServiceでは一度に送信できる<strong>バルクインサートのサイズに制限あり</strong><ul>
<li>1度のPOSTで挿入できるデータ量は<strong>10MB(※)まで</strong> ※インスタンスタイプにより異なる<ul>
<li>参考: <a href="https://docs.aws.amazon.com/ja_jp/elasticsearch-service/latest/developerguide/aes-limits.html#network-limits" target="_blank" rel="noopener">https://docs.aws.amazon.com/ja_jp/elasticsearch-service/latest/developerguide/aes-limits.html#network-limits</a></li>
<li>およそ1000件以上のチケットを纏めて格納しようとするとエラーとなってしまう</li>
</ul>
</li>
<li>この仕様を後から知ったため、バルク分割のためのスクリプトを追加で開発することになってしまった💦</li>
</ul>
</li>
<li>Elasticsearchの<strong>最新版(6系)では、インデックス内の複数タイプ使用が不可</strong><ul>
<li>参考: <a href="https://dev.classmethod.jp/server-side/elasticsearch/elasticsearch-6-breaking-changes/" target="_blank" rel="noopener">https://dev.classmethod.jp/server-side/elasticsearch/elasticsearch-6-breaking-changes/</a></li>
<li>開発時点では5系をベースとしていた＆6系の変更点を洗い出せていなかったため、5系依存のプログラムを作ってしまった<ul>
<li>「Elasticsearchのインデックス内タイプ」と「RedmineのプロジェクトID(プロジェクト識別子)」を紐付ける仕様としたため、そのままでは6系アップグレードが不可能になってしまった</li>
</ul>
</li>
</ul>
</li>
<li>AWS ECS環境で<strong>メモリリソース不足</strong>によるコンテナ強制終了が頻発<ul>
<li>開発環境(ローカルマシン)から検証環境(AWS)へ移行後、メモリリソース不足によるコンテナ強制終了が頻発した<ul>
<li>開発機環境における検証段階で、APIサーバ単位・合計の消費メモリのチェックをすべきだった</li>
<li>APIサーバごとのメモリ使用量調査など、追加検証が幾つか必要になった</li>
</ul>
</li>
<li>EC2サーバの台数を増やすことで対処できた<ul>
<li>ただし、今もSCDVモデルの書き出し処理の最中で強制終了されることがあり、要改善</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="今後の展望"><a href="#今後の展望" class="headerlink" title="今後の展望"></a>今後の展望</h3><p>今回のシステムは「チケット起票時」に注目したものでしたが、<br>今後はチケットをわざわざ<strong>起票せずとも</strong>システムを使えるよう、以下のUI機能実装を考えています。</p>
<ul>
<li>チケット起票画面で、フォーム入力状況からリアルタイムにレコメンド結果を表示する<strong>インクリメンタルサーチ</strong>機能</li>
<li>対話形式であいちゃんBotと会話することによりリアルタイムにレコメンド結果を得られる<strong>チャットボット</strong>機能</li>
</ul>
<p>また、システム面においてはレコメンドシステムの<strong>サーバレス化</strong>のため、JenkinsをAWS Lambdaに置き換えることを予定しております。</p>
<h3 id="スペシャルサンクス"><a href="#スペシャルサンクス" class="headerlink" title="スペシャルサンクス"></a>スペシャルサンクス</h3><ul>
<li>フューチャーSAIG(Strategic AI Group)所属の<strong>小池さん</strong></li>
</ul>
<p>レコメンド機能の設計・開発のアドバイスに加え、本記事のアルゴリズム解説の執筆担当をしていただきました！<br>ありがとうございます。</p>
<h3 id="おわりに"><a href="#おわりに" class="headerlink" title="おわりに"></a>おわりに</h3><p>PythonライブラリやAWSのクラウド資源を活用することにより、<br>AIを活用したレコメンドシステムを少人数・短期間で構築＆デプロイすることができました。</p>
<p>各種機械学習ライブラリやクラウド資源、Web上のナレッジベースの普及により、AIを使ったシステム開発・構築の<strong>ハードルはかなり下がっている</strong>な、と私自身も実感しました。</p>
<p>皆さんの身近な環境に、「このシステムは使いづらい」「検索などの定常作業を取り除きたい」といった”悩みの種”はありませんか？<br>チケット管理システムなどの<strong>社内で眠ったままの豊富なリソース</strong>と<strong>AI技術</strong>とを結びつけることで、多くの人に恩恵を与えるシステムを作り上げることが出来ますよ！</p>
<p>是非、チャレンジしてみて下さい💪</p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">EMNLP2017</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">okapi BM25（<a href="https://ja.wikipedia.org/wiki/Okapi_BM25）" target="_blank" rel="noopener">https://ja.wikipedia.org/wiki/Okapi_BM25）</a></span><a href="#fnref:2" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;こんにちは、フューチャーアーキテクト2017年4月入社、TIG（Technology Innovation Group）所属の竹林です。&lt;br&gt;大学では主にIoTの研究をしており、趣味で作ったArduinoベースのIoTラジコンカーを&lt;a href=&quot;https://fut
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="Redmine" scheme="https://future-architect.github.io/tags/Redmine/"/>
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>PostgreSQLパーティションプルーニングの動作を確認する</title>
    <link href="https://future-architect.github.io/articles/20181019/"/>
    <id>https://future-architect.github.io/articles/20181019/</id>
    <published>2018-10-19T07:00:00.000Z</published>
    <updated>2018-10-19T05:43:44.695Z</updated>
    
    <content type="html"><![CDATA[<p>PostgreSQL10までのパーティション機能を利用したプロジェクトにおいて、遅延SQLの調査をするなかで以下のような長い長い実行計画を目にすることがありました。</p>
<details><summary>こちらはサンプルテーブルでそれを再現したものです。長いので畳みました。</summary><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">Update on tr_part tgt  (cost=0.83..115.84 rows=4 width=44)</div><div class="line">  Update on tr_part_p_1809_01 tgt_1</div><div class="line">  Update on tr_part_p_1809_02 tgt_2</div><div class="line">  Update on tr_part_p_1809_03 tgt_3</div><div class="line">  Update on tr_part_p_9912_31 tgt_4</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.01 rows=1 width=38)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 tgt_1  (cost=0.42..8.44 rows=1 width=16)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.61 rows=4 width=26)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.01 rows=1 width=38)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 tgt_2  (cost=0.42..8.44 rows=1 width=16)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.61 rows=4 width=26)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.01 rows=1 width=38)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 tgt_3  (cost=0.42..8.44 rows=1 width=16)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.61 rows=4 width=26)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.57..31.81 rows=1 width=64)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.15..17.55 rows=1 width=58)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 tgt_4  (cost=0.15..5.50 rows=1 width=42)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..14.20 rows=4 width=26)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..4.65 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..4.66 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..4.66 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.22 rows=1 width=78)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div></pre></td></tr></table></figure><br><br></details> 


<p>なぜこのようなことになっているのか、仮に性能問題（SQLの遅延）が発生しているとき、どのような対処が考えられるか説明してきます。</p>
<h2 id="PostgreSQL10での確認"><a href="#PostgreSQL10での確認" class="headerlink" title="PostgreSQL10での確認"></a>PostgreSQL10での確認</h2><p>以下のようにパーティションテーブルを用意しました。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">--drop table tr_part;</span></div><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tr_part(</div><div class="line">    part_date <span class="built_in">date</span></div><div class="line">,   <span class="keyword">key</span> <span class="built_in">numeric</span></div><div class="line">,   <span class="keyword">data</span> <span class="built_in">numeric</span></div><div class="line">)</div><div class="line"><span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">range</span>(part_date);</div><div class="line"><span class="comment">--パーティション作成</span></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> tr_part_p_1809_01 <span class="keyword">partition</span> <span class="keyword">of</span>      tr_part <span class="keyword">for</span> <span class="keyword">values</span> <span class="keyword">from</span> ( <span class="keyword">MINVALUE</span> ) <span class="keyword">to</span> (<span class="string">'20180902'</span>);</div><div class="line"><span class="keyword">alter</span>  <span class="keyword">table</span> tr_part_p_1809_01 <span class="keyword">add</span> <span class="keyword">constraint</span> pk_tr_part_p_1809_01 primary <span class="keyword">key</span>(ymd,<span class="keyword">key</span>);</div><div class="line"></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> tr_part_p_1809_02 <span class="keyword">partition</span> <span class="keyword">of</span>      tr_part <span class="keyword">for</span> <span class="keyword">values</span> <span class="keyword">from</span> ( <span class="string">'20180902'</span> ) <span class="keyword">to</span> (<span class="string">'20180903'</span>);</div><div class="line"><span class="keyword">alter</span>  <span class="keyword">table</span> tr_part_p_1809_02 <span class="keyword">add</span> <span class="keyword">constraint</span> pk_tr_part_p_1809_02 primary <span class="keyword">key</span>(ymd,<span class="keyword">key</span>);</div><div class="line"></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> tr_part_p_1809_03 <span class="keyword">partition</span> <span class="keyword">of</span>      tr_part <span class="keyword">for</span> <span class="keyword">values</span> <span class="keyword">from</span> ( <span class="string">'20180903'</span> ) <span class="keyword">to</span> (<span class="string">'20180904'</span>);</div><div class="line"><span class="keyword">alter</span>  <span class="keyword">table</span> tr_part_p_1809_03 <span class="keyword">add</span> <span class="keyword">constraint</span> pk_tr_part_p_1809_03 primary <span class="keyword">key</span>(ymd,<span class="keyword">key</span>);</div><div class="line"></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> tr_part_p_9912_31 <span class="keyword">partition</span> <span class="keyword">of</span>      tr_part <span class="keyword">for</span> <span class="keyword">values</span> <span class="keyword">from</span> ( <span class="string">'20180904'</span> ) <span class="keyword">to</span> (<span class="string">'99991231'</span>);</div><div class="line"><span class="keyword">alter</span>  <span class="keyword">table</span> tr_part_p_9912_31 <span class="keyword">add</span> <span class="keyword">constraint</span> pk_tr_part_p_9912_31 primary <span class="keyword">key</span>(ymd,<span class="keyword">key</span>);</div><div class="line"></div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tr_part <span class="keyword">select</span> <span class="string">'20180901'</span>,generate_series,<span class="keyword">round</span>(generate_series * random() * <span class="number">100</span>) <span class="keyword">from</span> generate_series(<span class="number">1</span>,<span class="number">100000</span>);</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tr_part <span class="keyword">select</span> <span class="string">'20180902'</span>,generate_series,<span class="keyword">round</span>(generate_series * random() * <span class="number">100</span>) <span class="keyword">from</span> generate_series(<span class="number">100001</span>,<span class="number">200000</span>);</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tr_part <span class="keyword">select</span> <span class="string">'20180903'</span>,generate_series,<span class="keyword">round</span>(generate_series * random() * <span class="number">100</span>) <span class="keyword">from</span> generate_series(<span class="number">200001</span>,<span class="number">300000</span>);</div><div class="line"><span class="keyword">analyze</span> tr_part;</div></pre></td></tr></table></figure></p>
<p>データを投入した <code>p_1809_01</code>、 <code>p_1809_02</code>、 <code>p_1809_03</code> のパーティションに注目すると次のようなイメージです。 <code>part_date</code> の値によってレコードがパーティションに振り分けられて格納されています。</p>
<p><img src="/images/20181019/1.png"></p>
<p>続いて以下のような小さなテーブルを用意します。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">--drop table wk_input;</span></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> wk_input(</div><div class="line">	<span class="keyword">key</span> <span class="built_in">numeric</span></div><div class="line">,	related_key <span class="built_in">numeric</span></div><div class="line">,	target_date <span class="built_in">date</span></div><div class="line">)</div><div class="line">;</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> wk_input <span class="keyword">select</span> <span class="number">1</span>,<span class="number">1</span>     ,<span class="string">'20180901'</span>;</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> wk_input <span class="keyword">select</span> <span class="number">2</span>,<span class="number">100001</span>,<span class="string">'20180902'</span>;</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> wk_input <span class="keyword">select</span> <span class="number">3</span>,<span class="number">200001</span>,<span class="string">'20180903'</span>;</div><div class="line"><span class="keyword">analyze</span> wk_input;</div></pre></td></tr></table></figure>
<p>そして、以下のようなselectを実行するとどのような動作となるでしょうか。<br>ポイントはパーティションテーブルのパーティションキーに設定した<code>part_date</code>の列が結合条件としてのみ指定されていることです。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">select</span></div><div class="line">	*</div><div class="line"><span class="keyword">from</span></div><div class="line">	wk_input a</div><div class="line">,	tr_part b</div><div class="line"><span class="keyword">where</span> <span class="number">1</span> = <span class="number">1</span></div><div class="line"><span class="keyword">and</span> a.related_key = b.key</div><div class="line"><span class="keyword">and</span> a.target_date = b.part_date</div><div class="line">;</div></pre></td></tr></table></figure>
<p>このとき期待するのは次図の赤線のような動作でしょう。<br><img src="/images/20181019/2.png"></p>
<p>wk_input の target_date が <code>20180901</code> のレコードに対し、tr_partのpart_dateが <code>20180901</code> のパーティションにアクセスし、<br>wk_input の target_date が <code>20180902</code> のレコードに対し、tr_partのpart_dateが <code>20180902</code> のパーティションにアクセスし、<br>wk_input の target_date が <code>20180903</code> のレコードに対し、tr_partのpart_dateが <code>20180903</code> のパーティションにアクセスする。</p>
<p>ORACLEの場合はまさにそのような動作になります。<br>実行計画でみると以下のようになります。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> <span class="keyword">STATEMENT</span></div><div class="line">    <span class="keyword">NESTED</span> LOOPS</div><div class="line">        <span class="keyword">TABLE</span> <span class="keyword">ACCESS</span> <span class="keyword">FULL</span> WK_INPUT</div><div class="line">        <span class="keyword">PARTITION</span> <span class="keyword">RANGE</span> ITERATOR</div><div class="line">            <span class="keyword">TABLE</span> <span class="keyword">ACCESS</span> <span class="keyword">BY</span> <span class="keyword">ROWID</span> TR_PART</div><div class="line">                <span class="keyword">INDEX</span> <span class="keyword">UNIQUE</span> <span class="keyword">SCAN</span> PK_TR_PART</div></pre></td></tr></table></figure>
<p><code>PARTITION RANGE ITERATOR</code>のところがまさに、WK_INPUTの各行に対応するパーティションへのアクセスを示しています。</p>
<p>では、PostgreSQL10ではどのようになるかというと<code>explain analyze</code>で先のselect文を実行すると以下のような出力になりました。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">Nested Loop  (cost=0.42..93.60 rows=1 width=31) (actual time=66.130..275.215 rows=3 loops=1)</div><div class="line">  -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=15) (actual time=16.125..16.261 rows=3 loops=1)</div><div class="line">  -&gt;  Append  (cost=0.42..30.82 rows=4 width=16) (actual time=69.391..86.303 rows=1 loops=3)</div><div class="line">        -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..8.44 rows=1 width=16) (actual time=48.916..51.817 rows=0 loops=3★)</div><div class="line">              Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..8.44 rows=1 width=16) (actual time=17.012..21.819 rows=0 loops=3★)</div><div class="line">              Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..8.44 rows=1 width=16) (actual time=12.144..12.594 rows=0 loops=3★)</div><div class="line">              Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..5.50 rows=1 width=68) (actual time=0.013..0.013 rows=0 loops=3★)</div><div class="line">              Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">Planning time: 118.634 ms</div><div class="line">Execution time: 275.380 ms</div></pre></td></tr></table></figure>
<p>注目は★マークを付けた4,6,8,10行目の <code>loops=3</code> のところでしょうか。どのパーティションにもwk_inputの3行に対し3回のアクセスがあることが確認できます。</p>
<p>図にすると次のようなイメージです。<br><img src="/images/20181019/3.png"></p>
<p>つまりPostgreSQL10ではクエリ実行時にwk_inputのレコードの値をみて、パーティションプルーニングするような動作はできないことがわかります。<br>これを踏まえたうえで、パーティションテーブルとパーティションテーブルの結合を考えてみます。</p>
<p>冒頭の実行計画は以下のクエリをexplainしたものです。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">update</span> tr_part tgt</div><div class="line"><span class="keyword">set</span></div><div class="line">	<span class="keyword">data</span> = b.data *<span class="number">0.8</span></div><div class="line"><span class="keyword">from</span></div><div class="line">	wk_input a</div><div class="line">,	tr_part b</div><div class="line"><span class="keyword">where</span> <span class="number">1</span> = <span class="number">1</span></div><div class="line"><span class="keyword">and</span> a.related_key = b.key</div><div class="line"><span class="keyword">and</span> a.target_date = b.part_date</div><div class="line"><span class="keyword">and</span> b.key = tgt.key</div><div class="line"><span class="keyword">and</span> b.part_date = tgt.part_date</div><div class="line">;</div></pre></td></tr></table></figure>
<p>冒頭の実行計画の先頭部分を抜き出して以下に貼り付けました。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"> Update on tr_part tgt  (cost=0.83..115.84 rows=4 width=44)</div><div class="line">   Update on tr_part_p_1809_01 tgt_1</div><div class="line">   Update on tr_part_p_1809_02 tgt_2</div><div class="line">   Update on tr_part_p_1809_03 tgt_3</div><div class="line">   Update on tr_part_p_9912_31 tgt_4</div><div class="line">   -&gt;  Nested Loop  (cost=0.83..28.01 rows=1 width=38)</div><div class="line">         Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">         -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32)</div><div class="line">               -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16)</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01★ tgt_1  (cost=0.42..8.44 rows=1 width=16)</div><div class="line">                     Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">         -&gt;  Append  (cost=0.42..1.61 rows=4 width=26)</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01● b  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02● b_1  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03● b_2  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31● b_3  (cost=0.15..0.17 rows=1 width=78)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">...省略</div></pre></td></tr></table></figure>
<p>パーティションテーブルであるtr_partに注目します。<br>10行目にあるtr_partテーブルの <code>p_1809_01★</code> に対し、13~19行目のtr_partテーブルの <code>p_1809_01●</code>、 <code>p_1809_02●</code>、 <code>p_1809_03●</code>、 <code>p_9912_31●</code> が参照されています。</p>
<p>パーティションテーブルtr_partに着目すると、期待する動きは次図ですが、、、<br><img src="/images/20181019/4.png"></p>
<p>実際は次図のようになっているということです。<br><img src="/images/20181019/5.png"></p>
<p>パーティション数が多いと、PostgreSQLのこのような動作がかなりな性能遅延を引き起こします。</p>
<p>PostgreSQLでは1テーブルに100を超えるほどのパーティションを定義することはあまり無いでしょう。<br>しかし、例えば1月分のデータを日次のパーティションで保持している場合の約30パーティションのテーブル同士の結合を想定すると、30×30で900通りのパーティション間の結合を試みることになります。<br>これがどれほど非効率かは想像にかたくありません。</p>
<p>PostgreSQLのこのような動作に起因して性能劣化が見られる場合は、ユーザからアクセスすべきパーティションを教えてあげる必要があります。<br>つまり、この例ではアクセス対象のパーティションはwk_inputに保持されているtarget_dateの値で決まっています。<br>そのため、<code>select distinct target_date from wk_iput</code>のように一度target_dateの一覧を抽出します。<br>そのうえで、以下のようにパーティションキーのpart_dateの値を以下のクエリの<code>/*あらかじめ取得した値*/</code>のところで指定してループ実行します。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">update</span> tr_part tgt</div><div class="line"><span class="keyword">set</span></div><div class="line">	<span class="keyword">data</span> = b.data *<span class="number">0.8</span></div><div class="line"><span class="keyword">from</span></div><div class="line">	wk_input a</div><div class="line">,	tr_part b</div><div class="line"><span class="keyword">where</span> <span class="number">1</span> = <span class="number">1</span></div><div class="line"><span class="keyword">and</span> a.related_key = b.key</div><div class="line"><span class="keyword">and</span> a.target_date = b.part_date</div><div class="line"><span class="keyword">and</span> b.key = tgt.key</div><div class="line"><span class="keyword">and</span> b.part_date = tgt.part_date</div><div class="line"><span class="keyword">and</span> b.part_date = <span class="comment">/*あらかじめ取得した値*/</span></div><div class="line">;</div></pre></td></tr></table></figure>
<p>パーティション数が多く、アクセスが非効率になっているような場合は、このようにパーティションをユーザから特定してあげることで大きな改善がみられる場合があります。</p>
<h2 id="PostgreSQL11での確認"><a href="#PostgreSQL11での確認" class="headerlink" title="PostgreSQL11での確認"></a>PostgreSQL11での確認</h2><p>さて、ある日dockerで環境構築をしていてふとPostgreSQL11（β版）がpullできるようになっていることに気づきましたので、<br>ちょろっと触ってみようと思い上記と同様にパーティションプルーニングの動作を確認してみました。</p>
<details><summary>やはりとても長い実行計画が確認できました。。</summary><div><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">Update on tr_part tgt  (cost=0.83..115.93 rows=4 width=70) (actual time=235.751..235.751 rows=0 loops=1)</div><div class="line">  Update on tr_part_p_1809_01 tgt_1</div><div class="line">  Update on tr_part_p_1809_02 tgt_2</div><div class="line">  Update on tr_part_p_1809_03 tgt_3</div><div class="line">  Update on tr_part_p_9912_31 tgt_4</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.03 rows=1 width=64) (actual time=61.837..72.983 rows=1 loops=1)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32) (actual time=61.790..62.534 rows=1 loops=1)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16) (actual time=17.805..17.811 rows=3 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 tgt_1  (cost=0.42..8.44 rows=1 width=16) (actual time=14.901..14.901 rows=0 loops=3)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.63 rows=4 width=26) (actual time=0.036..10.437 rows=1 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26) (actual time=0.027..10.424 rows=1 loops=1)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.03 rows=1 width=64) (actual time=56.255..96.389 rows=1 loops=1)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32) (actual time=56.211..74.941 rows=1 loops=1)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16) (actual time=0.011..0.020 rows=3 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 tgt_2  (cost=0.42..8.44 rows=1 width=16) (actual time=24.968..24.968 rows=0 loops=3)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.63 rows=4 width=26) (actual time=0.033..21.437 rows=1 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26) (actual time=0.028..21.431 rows=1 loops=1)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.03 rows=1 width=64) (actual time=65.428..66.187 rows=1 loops=1)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32) (actual time=65.375..65.377 rows=1 loops=1)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16) (actual time=0.021..0.028 rows=3 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 tgt_3  (cost=0.42..8.44 rows=1 width=16) (actual time=21.778..21.778 rows=0 loops=3)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.63 rows=4 width=26) (actual time=0.038..0.794 rows=1 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26) (actual time=0.031..0.785 rows=1 loops=1)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.57..31.83 rows=1 width=90) (actual time=0.039..0.039 rows=0 loops=1)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.15..17.55 rows=1 width=58) (actual time=0.039..0.039 rows=0 loops=1)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16) (actual time=0.018..0.019 rows=3 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 tgt_4  (cost=0.15..5.50 rows=1 width=42) (actual time=0.005..0.005 rows=0 loops=3)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..14.22 rows=4 width=26) (never executed)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..4.65 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..4.66 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..4.66 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.22 rows=1 width=78) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div></pre></td></tr></table></figure><br><br></div></details>

<p>　</p>
<p>残念…と思いきや<code>explain analyze</code>の結果を見ると動作が改善されていることがわかりました。<br>以下に冒頭部分を抜き出しました。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"> Update on tr_part tgt  (cost=0.83..115.93 rows=4 width=70) (actual time=235.751..235.751 rows=0 loops=1)</div><div class="line">   Update on tr_part_p_1809_01 tgt_1</div><div class="line">   Update on tr_part_p_1809_02 tgt_2</div><div class="line">   Update on tr_part_p_1809_03 tgt_3</div><div class="line">   Update on tr_part_p_9912_31 tgt_4</div><div class="line">   -&gt;  Nested Loop  (cost=0.83..28.03 rows=1 width=64) (actual time=61.837..72.983 rows=1 loops=1)</div><div class="line">         Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">         -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32) (actual time=61.790..62.534 rows=1 loops=1)</div><div class="line">               -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16) (actual time=17.805..17.811 rows=3 loops=1)</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01★ tgt_1  (cost=0.42..8.44 rows=1 width=16) (actual time=14.901..14.901 rows=0 loops=3)</div><div class="line">                     Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">         -&gt;  Append  (cost=0.42..1.63 rows=4 width=26) (actual time=0.036..10.437 rows=1 loops=1)</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01● b  (cost=0.42..0.48 rows=1 width=26) (actual time=0.027..10.424 rows=1 loops=1■)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02▲ b_1  (cost=0.42..0.48 rows=1 width=26) (never executed▼)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03▲ b_2  (cost=0.42..0.48 rows=1 width=26) (never executed▼)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31▲ b_3  (cost=0.15..0.17 rows=1 width=78) (never executed▼)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key)</div><div class="line">...省略</div></pre></td></tr></table></figure>
<p>10行目のtr_partテーブルの <code>p_1809_01★</code> に対し、13行目の●の同パーティションに、13行目最右の■部分でアクセスがあったことが確認できます。</p>
<p>これに対して、15行目移行の▲で目印をした <code>p_1809_02</code>、 <code>p_1809_03</code>、 <code>p_9912_31</code> のパーティションに対しては、▼部分（never executedと書いていますね）で実際の実行がスキップされていることが確認できます。<br>PostgreSQLがバージョン11になって、パーティション <code>p_1809_02</code>、 <code>p_1809_03</code>、 <code>p_9912_31</code> の結合を試みても仕方のないものとしてスキップを判断できるようになっています。</p>
<p>長年ORACLEを使い倒してきて、ふとPostgreSQLを使うと、こんなこともできないのか、と思うことがあります。<br>しかし、日々成長してきていることも感じられ、愛おしくも思えてくるのがPostgreSQLのいいところですね。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PostgreSQL10までのパーティション機能を利用したプロジェクトにおいて、遅延SQLの調査をするなかで以下のような長い長い実行計画を目にすることがありました。&lt;/p&gt;
&lt;details&gt;&lt;summary&gt;こちらはサンプルテーブルでそれを再現したものです。長いので畳みま
    
    </summary>
    
      <category term="DB" scheme="https://future-architect.github.io/categories/DB/"/>
    
    
      <category term="DB" scheme="https://future-architect.github.io/tags/DB/"/>
    
  </entry>
  
  <entry>
    <title>NLP若手の会 (YANS) 第13回シンポジウム 参加レポート</title>
    <link href="https://future-architect.github.io/articles/20180912/"/>
    <id>https://future-architect.github.io/articles/20180912/</id>
    <published>2018-09-12T08:49:16.000Z</published>
    <updated>2018-09-11T09:23:41.383Z</updated>
    
    <content type="html"><![CDATA[<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>はじめまして！2018年8月半ばからフューチャー株式会社にキャリア採用で入社した田中駿と申します。Strategic AI Groupに所属しています。大学4年次より自然言語処理を専門に研究・開発を行っており、2018年からはNLP若手の会(YANS)の委員も努めています。どうぞよろしくお願いいたします。</p>
<p>さて、弊社は<a href="http://yans.anlp.jp/entry/yans2018" target="_blank" rel="noopener">8/27~29に香川県で開催されたYANS</a>にゴールドスポンサーとして私、貞光、小池の3人で参加してきました！</p>
<p>この記事では、YANSの参加レポートとして以下のトピックを中心に共有します。</p>
<ul>
<li>弊社の発表内容</li>
<li>興味深かったポスター発表</li>
<li>全体的な感想</li>
</ul>
<h1 id="開催場所！！"><a href="#開催場所！！" class="headerlink" title="開催場所！！"></a>開催場所！！</h1><p>2018年のYANSは香川県高松市で開催されました～！花樹海という香川県内ではなかなかなグレード(らしい)の旅館で開催されました。山の上にある(なんとエントランスが8F)落ち着いた佇まいの気品のある旅館でした。</p>
<p>屋外のデッキからは電車が見えて最高でした。</p>
<p><img src="/images/20180912/photo_20180912_01.jpeg"></p>
<h1 id="弊社の発表内容"><a href="#弊社の発表内容" class="headerlink" title="弊社の発表内容"></a>弊社の発表内容</h1><p>弊社の会社についてと、私たちの所属するStrategic AI Groupの紹介を口頭発表・ポスター発表で行いました。<br>スポンサーブースには非常に多くの方が来てくださり、NLPを始めとするAI関連の取り組みに興味を持ってくださりました。弊社はBtoB事業が多く、一般の方には認知されづらい部分が多いので、スポンサー活動などを通してより多くの人に事業内容の面白さ、仕事のやりがいを伝えていきたいなと思いました。</p>
<p><img src="/images/20180912/photo_20180912_02.jpeg"><br>▲ブースターセッションで話してるわたし。オフィスの綺麗さについてアピールしているところ。</p>
<p><img src="/images/20180912/photo_20180912_03.jpeg"><br>▲イベントの紹介をする貞光。年に何回か社外の方を招待してイベントを行っています。</p>
<p><img src="/images/20180912/photo_20180912_04.jpeg"><br>▲ハイテンションでポスター発表する小池。多くの人に足を運んでもらえました。</p>
<p>今回スポンサーブースで展示していたポスターを公開します。<br>NLPに限らず、ワクチン開発や牛の発情タイミング予測など……かなり幅広い案件に取り組んでいます！<br><img src="/images/20180912/photo_20180912_05.jpeg"></p>
<h1 id="面白かった発表"><a href="#面白かった発表" class="headerlink" title="面白かった発表"></a>面白かった発表</h1><p>参加メンバが興味深いと思った発表を紹介します。</p>
<h2 id="テキスト平易化における難易度の制御"><a href="#テキスト平易化における難易度の制御" class="headerlink" title="テキスト平易化における難易度の制御"></a>テキスト平易化における難易度の制御</h2><p>西原大貴, 梶原智之, 荒瀬由紀 (阪大)</p>
<p>文章の平易化手法が多くある中で、本手法は難易度を指定して生成することを可能とする手法です。<br>学習コーパスには、元文と、それに対する言い換え文の難易度が記されているnewselaを用い、その難易度をNMT(Neural Machine Translation)の入力信号として付加する、というシンプルなアプローチでです。<br>従来手法に比べ、各難易度のリファレンス文に対するBLEU（機械翻訳で主に用いられる指標）やSARI（テキスト平易化で主に用いられる手法）で改善を示しています。newselaは１つの元文に対し、すべての難易度での言い換えが行われているわけではない、というのがタスクとしての奥行を感じました。今回は、難易度を単なるラベル情報として入力していますが、難易度の本来意味する連続値として扱うことで、データスパースネスにも頑健に働き得る可能性もあり、今後の発展が楽しみな研究です。</p>
<h2 id="人手による感情ラベル付けにおける応答時間に着目した感情推定難易度の評価"><a href="#人手による感情ラベル付けにおける応答時間に着目した感情推定難易度の評価" class="headerlink" title="人手による感情ラベル付けにおける応答時間に着目した感情推定難易度の評価"></a>人手による感情ラベル付けにおける応答時間に着目した感情推定難易度の評価</h2><p>山下紗苗, 上泰 (明石高専), 加藤恵梨, 酒井健, 奥村紀之 (大手前大学)</p>
<p>感情推定の難易度と、そのアノテーションの時間との間で相関をとろうと試みた意欲的な研究です。<br>今回アノテーションの対象としていたのは著者のツイートで、著者の知人がアノテートするほど、時間は短くなると仮説を持っていたそうですが、実験結果では真逆、つまり著者を全く知らないアノテータのアノテーション時間の方が短い、という傾向が得られたようです。書き手を知っているが故に、いろいろと深く考えてしまうのでしょうか？当初の仮説と逆の結果を率直に発表する、というのも若手の会の面白いところで、良いところだと思います。今回の結果を受けて、今後どのように本研究が展開していくのかとても楽しみな研究でした。</p>
<h2 id="五感に基づく言語表現における個人のバイアスとその補正"><a href="#五感に基づく言語表現における個人のバイアスとその補正" class="headerlink" title="五感に基づく言語表現における個人のバイアスとその補正"></a>五感に基づく言語表現における個人のバイアスとその補正</h2><p>大葉大輔 (東大), 吉永直樹 (東大/生産研), 赤崎智 (東大), 豊田正史 (東大/生産研)</p>
<p>人によって同じ「辛い」という単語でも、個人のバイアスが存在するために人によって捉え方が異なります。この研究では、個人のバイアスの分析をすることによって、「辛い」といった表現に対する捉え方のバイアスを補正することを目的にしています。<br>手法としては、大規模コーパスから分散表現を用いて単語ベクトルを獲得し、個人の文書を用いて追加学習することによりバイアスの補正に使用している。発想が非常に面白く、表現の捉え方の違いによるコミュニケーション齟齬の減少に貢献するのではないかと思いました。</p>
<h2 id="RUSE-文の分散表現を用いた回帰モデルによる機械翻訳の自動評価"><a href="#RUSE-文の分散表現を用いた回帰モデルによる機械翻訳の自動評価" class="headerlink" title="RUSE: 文の分散表現を用いた回帰モデルによる機械翻訳の自動評価"></a>RUSE: 文の分散表現を用いた回帰モデルによる機械翻訳の自動評価</h2><p>嶋中宏希 (首都大), 梶原智之 (阪大), 小町守 (首都大) </p>
<p>機械翻訳における自動評価を提案している研究です。<br>機械翻訳では、BLUE値等の評価指標を使われることが多いですが、これらの評価はN-gramに基づく素性を利用しており、意味的な情報を扱えていません。この研究では、分散表現および回帰モデルを用いて機械翻訳の自動評価を行っており、既存の提案指標よりも人手評価との相関が高いことが報告されています。評価尺度を新しく提案するというアプローチではなく、評価を機械にさせてしまおうという発想は非常に面白いと思いました。</p>
<h1 id="さいごに"><a href="#さいごに" class="headerlink" title="さいごに"></a>さいごに</h1><p>今回のYANSは委員の仕事などで朝が早かったため、なかなかハードでした…。<br>一方で今までお話ししたことのなかった多くの研究者、企業の方々と意見交換ができ、有意義な3日間となりました。栗林公園を散策したり、ボードゲームやインディアンポーカー(独自ルールのやつ)をしたり。。とても盛り上がり、楽しかったです。笑<br>そして最終日には念願の讃岐うどんをいただきました、食べたすぎてたまらなかったので写真とるのを忘れてしまいました。</p>
<p><img src="/images/20180912/photo_20180912_06.jpeg"><br>▲栗林公園の橋。まったりできました</p>
<p>今後も言語処理界隈のイベントには積極的に参加していきます。<br>お会いした時はどうぞよろしくお願いいたします。</p>
<p>フューチャー株式会社では自然言語処理の新卒・キャリア採用、インターンを積極募集中です！</p>
<p>社員の関係がとてもフラット、メンバ間での連携・共有がしっかりと行われており、雰囲気も良く、切磋琢磨できる良い環境だと思っています。そしてなんといってもオフィスがきれいです！！笑</p>
<p>見学や面談など、いつでも受け付けているのでぜひ！:)<br>一緒にNLPしましょうー！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h1&gt;&lt;p&gt;はじめまして！2018年8月半ばからフューチャー株式会社にキャリア採用で入社した田中駿と申します。Strategic AI 
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
      <category term="NLP" scheme="https://future-architect.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>5TB/日 のデータをAWS Glueでさばくためにやったこと（概要編</title>
    <link href="https://future-architect.github.io/articles/20180828/"/>
    <id>https://future-architect.github.io/articles/20180828/</id>
    <published>2018-08-28T04:27:14.000Z</published>
    <updated>2018-08-28T09:23:37.816Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/20180828/photo_20180828_01.png"></p>
<p>みなさん、初めまして、お久しぶりです、こんにちは。<br>フューチャーアーキテクト2018年新卒入社、1年目エンジニアのTIG（Technology Innovation Group）所属の澤田周吾です。大学では機械航空工学を専攻しており、学生時代のインターンなどがキッカケで入社を決意しました。</p>
<p>実は、本記事でフューチャーテックブログの2記事目となります。インターン時代も <a href="https://future-architect.github.io/articles/20170421/">ジャガイモARの記事</a>  を書かせて頂きました。入社してからもこうして業務で学んだIT技術を記事に書くという機会を貰え、なんだか懐かしいやら感慨深いやらの思いで一杯です。</p>
<p>さて、3ヶ月の新人研修後にすぐに配属されたプロジェクトで、AWSを使ったビックデータ分析のための基盤構築をお手伝いしています。わたしは分析のための前処理であるETL（Extract、Transform、Load）処理部分をちょっと変わった性格の先輩方と一緒に開発しており、今回はそれに用いているサービスであるAWS Glueについて紹介いたします。</p>
<p>※記事は2回にわけて発信していきたいと考えています。<br>第一弾として、題名の大規模データを処理するために行った様々な工夫を説明する前に、Glueの概要や開発Tips、制約について書かせていただきます</p>
<h1 id="AWS-Glueとは"><a href="#AWS-Glueとは" class="headerlink" title="AWS Glueとは"></a>AWS Glueとは</h1><p>ご存知の方も多いかと思いますが、簡単にGlueについての説明です。</p>
<blockquote>
<p>AWS Glue は抽出、変換、ロード (ETL) を行う完全マネージド型のサービスで、お客様の分析用データの準備とロードを簡単にします。AWS マネジメントコンソールで数回クリックするだけで、ETL ジョブを作成および実行できます。<br><a href="https://aws.amazon.com/jp/glue/" target="_blank" rel="noopener">引用:AWS公式サイト</a></p>
</blockquote>
<p>簡単に言うと、「データ処理を行うサービス」です。<br>公式サイトにも書かれていますが、Glueの特徴として、5点挙げられます。</p>
<ol>
<li>AWSの一つであること</li>
<li>ETL処理を行うサービスであること</li>
<li>完全マネージド型であること</li>
<li>Scala、Python、Apache Spark を使用できること</li>
<li>並立分散処理ができること</li>
</ol>
<h1 id="今回実現したいこと"><a href="#今回実現したいこと" class="headerlink" title="今回実現したいこと"></a>今回実現したいこと</h1><p>S3やDynamoDBに配備された入力データを、少々複雑な加工ロジックが入ったETL処理を何度か繰り返し、蓄積用のDynamoDBと、分析用のS3に出力することです。<br>入力のマスタデータは日次程度の洗い替えでOK、入力データは10分毎にzip圧縮後で35GB程度がDataLakeに供給され、それらを逐次バッチ的に処理します。ETLの処理ウインドウ時間は10分以内となり、1日では合計5TBに及びます。</p>
<p><img src="/images/20180828/photo_20180828_02.png"></p>
<p>データ量が多いため、Glueが利用できる前だとSpark on EMRで処理することを検討していたと思います。EMRも良いサービスだと認識していますが、10分毎に処理する要件だと、EMRクラスタを常時立ち上げざる得ないため、EMRの保守運用を考えるとマネージドでSparkを扱えるGlueを採用したほうが良いのでは？という判断がなされました。他にもAWS Athenaなども候補に上がりましたが、読み込み時のパーティションは可能なものの、2018.08時点ではクエリ結果をS3に書き込む際に、DynamicPartitionができないという点がネックで採用には至りませんでした。</p>
<p>ETL処理の中身を簡単に言うと…</p>
<ol>
<li>処理対象のフィルター（例: 異常値の排除など）</li>
<li>コンテンツへのエンリッチメント（例: マスタデータと結合し非正規化処理など）</li>
<li>多様な分析軸でのグルーピング（例: ユーザID軸、ユーザの属性軸など）</li>
<li>逆ジオコーディングのために外部サービスにアクセスが必要</li>
<li>k-meansライクなクラスタリング処理が必要</li>
</ol>
<p>などがあります。<br>特に4,5は少し毛色が変わっており、処理特性の違いからETLのパイプラインを当初より少し多めに分割する必要性がでてきました。今回は4つのステップに分割しました。そのため、各ステップは10分よりずっと短い時間で処理を完了させる必要があります。</p>
<p>また、今回の要件だとワークフローは複雑な分岐や待ち合わせが存在せず、前のジョブAが終わったら素直に後ろのジョブBを起動すれば良いだけだったため、StepFunctionなどを導入せずGlueで完結して構築しています。</p>
<h1 id="EMR-と-Glue-比較"><a href="#EMR-と-Glue-比較" class="headerlink" title="EMR と Glue 比較"></a>EMR と Glue 比較</h1><p>EMRとGlueですが、アプリケーションの実装としてはどちらもSparkを利用する以上は差が出ないため、インフラレベルで比較した表となります。どうやら、Glueは内部的にEMRを起動させているようなので、GlueはSparkクラスタを構築せずジョブが実行できるといった仕組みと捉えたほうが理解しやすいかと思います。</p>
<p>EMRの方が細かくチューニングが可能ですが、先に述べたように保守運用性の観点からGlueを採用しました。</p>
<table>
<thead>
<tr>
<th>#</th>
<th>EMR</th>
<th>Glue</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pros</td>
<td>ブートストラップアクション/ステップ処理を通じてOSレベルからの設定変更が可能</td>
<td>ランタイム環境がフルマネージドで提供されるため、クラスタの管理が不要</td>
</tr>
<tr>
<td></td>
<td>コアノード/タスクノード数を調整することでシステムリソース量の調整が可能</td>
<td>サービスとしての単一障害点がない</td>
</tr>
<tr>
<td></td>
<td>Spark以外のツールを利用可能</td>
<td>データカタログを他AWSサービスと共有可能</td>
</tr>
<tr>
<td>Cons</td>
<td>マスタノードが単一障害点（特に常駐起動させておく場合にネック）</td>
<td>ランタイム環境がフルマネージドで提供されるため、OSレベル・クラスタレベルでの設定変更が不可能(システムリソースは調整可能)</td>
</tr>
<tr>
<td></td>
<td>EMRクラスタの設定や起動処理の設計・実装が必要</td>
<td>ー</td>
</tr>
</tbody>
</table>
<h1 id="Glueで行えること"><a href="#Glueで行えること" class="headerlink" title="Glueで行えること"></a>Glueで行えること</h1><p>具体的なイメージが湧きにくいと思いますので、早速ですが、コードベースでどういうことができるかいくつか例示していきたいと思います。<br>今回、コードはPythonで説明していきます。</p>
<h2 id="S3上のファイルの読み書き"><a href="#S3上のファイルの読み書き" class="headerlink" title="S3上のファイルの読み書き"></a>S3上のファイルの読み書き</h2><p>以下のコードでS3に対して読み込み・書き込みが行えます。<br>ファイル形式を変更することで、CSV、JSON、Parquetなどの形式に対応できます。</p>
<p>Glueで定義されたデータ構造のDynamicFrameを使っていきます。<br>使い方はSparkのDataFrameのように扱うことができます。</p>
<figure class="highlight python"><figcaption><span>S3からCSVファイルの読み込み処理</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">df = glueContext.create_dynamic_frame.from_options(</div><div class="line">    connection_type=<span class="string">"s3"</span>,</div><div class="line">    connection_options=&#123;</div><div class="line">        <span class="string">"paths"</span>: [<span class="string">"s3://&#123;0&#125;"</span>.format(<span class="string">"バケット名"</span>)]&#125;,</div><div class="line">    format=<span class="string">"csv"</span>,  <span class="comment"># ファイル形式指定 json, parquet等に変更可</span></div><div class="line">    format_options=&#123;<span class="string">"withHeader"</span>: <span class="keyword">True</span>&#125;  <span class="comment"># 1行目をスキーマ名として認識True</span></div><div class="line">    )</div></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>DynamicFrame(df)をS3にcsv形式で出力処理</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">datasink = glueContext.write_dynamic_frame.from_options(</div><div class="line">        frame=df,  <span class="comment"># 出力するDynamicFrameを指定</span></div><div class="line">        connection_type=<span class="string">"s3"</span>,</div><div class="line">        connection_options=&#123;</div><div class="line">            <span class="string">"path"</span>: [<span class="string">"s3://&#123;0&#125;"</span>.format(<span class="string">"バケット名"</span>)],</div><div class="line">            <span class="string">"partitionKeys"</span>: <span class="string">"パーティションを切るキー名"</span>&#125;,</div><div class="line">        format=<span class="string">"csv"</span>  <span class="comment"># ファイル形式指定</span></div><div class="line">        )</div></pre></td></tr></table></figure>
<p>他のファイル形式については<a href="https://docs.aws.amazon.com/ja_jp/glue/latest/dg/aws-glue-programming-etl-format.html" target="_blank" rel="noopener">AWS Glue の ETL 出力用の形式オプション
</a> を参考ください。CSVであれば区切り文字やヘッダ行出力の有無もオプションで指定できます。</p>
<h2 id="DaynamoDBからの読み込み、書き込み"><a href="#DaynamoDBからの読み込み、書き込み" class="headerlink" title="DaynamoDBからの読み込み、書き込み"></a>DaynamoDBからの読み込み、書き込み</h2><p>DynamoDBへのアクセスはAWS SDK for Pythonなboto3を利用します。<br>2018.08時点では標準のコネクタは存在しないようです。</p>
<figure class="highlight python"><figcaption><span>DynamoDB初期化</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">dynamo_region=<span class="string">"AWSリージョン名"</span></div><div class="line">dynamodb = boto3.resource(</div><div class="line">        <span class="string">'dynamodb'</span>,　</div><div class="line">        region_name=dynamo_region,</div><div class="line">        endpoint_url=<span class="string">'http://dynamodb.'</span> + dynamo_region + <span class="string">'.amazonaws.com'</span></div><div class="line">        )</div></pre></td></tr></table></figure>
<p>あとは素直にget, put すれば読み書きできます。</p>
<figure class="highlight python"><figcaption><span>DynamoDBからデータベースの読み込み</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">table = dynamodb.Table(<span class="string">"テーブル名"</span>)</div><div class="line">response = table.get_item(</div><div class="line">        Key=&#123; <span class="string">'xxx'</span>: xxx, <span class="string">'yyy'</span>: yyy &#125;</div><div class="line">    )</div><div class="line">print(response)</div></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>DynamoDBへデータ出力</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">table = dynamodb.Table(<span class="string">"テーブル名"</span>)</div><div class="line">response = table.put_item(</div><div class="line">    Item=&#123;<span class="string">'xxx'</span>: xxx, <span class="string">'yyy'</span>: yyy&#125;</div><div class="line">)</div><div class="line">print(response)</div></pre></td></tr></table></figure>
<p>もし、DynamoDBへのR/Wを行う場合は、Read/Writeのキャパシティーユニットを確認するとともに、レスポンスでスループット超過時のエラーハンドリングもお忘れないように注意ください。</p>
<h2 id="加工処理"><a href="#加工処理" class="headerlink" title="加工処理"></a>加工処理</h2><p>入力で取得したデータは例えば以下のようにSQLクエリを用いて加工することができます。</p>
<figure class="highlight python"><figcaption><span>SQLを利用した加工処理</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># DynamicFrameをDataFrameに変換</span></div><div class="line">dataFrame = df.toDF()</div><div class="line"></div><div class="line"><span class="comment"># DataFrameにテーブル名を割り当て</span></div><div class="line">dataFrame.registerTempTable(<span class="string">'table_name'</span>)</div><div class="line"></div><div class="line"><span class="comment"># SparkSQL文にてデータ加工</span></div><div class="line">select_table = spark.sql(<span class="string">'SELECT * FROM table_name'</span>)</div></pre></td></tr></table></figure>
<p>今回の開発では情報量が多いことと開発チームメンバーのスキルセットからDataFrameに変換後にSQLで加工する手法を使いました。</p>
<h2 id="ジョブのワークフロー"><a href="#ジョブのワークフロー" class="headerlink" title="ジョブのワークフロー"></a>ジョブのワークフロー</h2><p>GlueのTriggerを利用することで、Glue内でジョブのワークフローを作ることができます。<br>また、起動を制御するためのTriggerは3種類用意されています。</p>
<ol>
<li>Triggerの開始をタイマーで行う ＝ <strong>スケジュール</strong></li>
<li>ジョブイベントが監視対象リストに一致した場合に行う ＝ <strong>ジョブイベント</strong></li>
<li>手動で開始させる ＝ <strong>オンデマンド</strong></li>
</ol>
<p>1のスケジュールトリガー、3のオンデマンドトリガーについてはイメージがつくと思います。<br>2のジョブイベントトリガーについて補足していきます。</p>
<p>ジョブイベントトリガーは、ジョブXが終わったら次のジョブYを起動する、といった依存関係を設定することができます。<br>具体的にはジョブイベントトリガー作成時には以下の項目を選択することができます。</p>
<ul>
<li>監視対象ジョブ（複数可）</li>
<li>トリガーするジョブ（複数可）</li>
<li>「成功、失敗、停止、タイムアウト」の４つのジョブステータス</li>
<li>監視対象ジョブとステータスが全一致でトリガーするか、部分一致でトリガーするか</li>
<li>起動時に渡すパラメータ（セキュリティ設定、ブックマーク、タイムアウト、キー/値）</li>
</ul>
<p>例：ジョブイベントトリガーを利用して以下の様なフローを３つのジョブイベントトリガーを設定することで実現することができます</p>
<p><img src="/images/20180828/photo_20180828_03.png"></p>
<ul>
<li>トリガー1はジョブAが成功したらジョブB, Cを起動</li>
<li>トリガー2はジョブBが成功したらジョブDを起動</li>
<li>トリガー3はジョブC, Dが成功したらジョブEを起動</li>
</ul>
<h1 id="Glue開発Tips"><a href="#Glue開発Tips" class="headerlink" title="Glue開発Tips"></a>Glue開発Tips</h1><p>この1ヶ月で学んだGlueで開発を行う上でのコツをお伝えします</p>
<h2 id="Tips1-Glueデバックについて"><a href="#Tips1-Glueデバックについて" class="headerlink" title="Tips1. Glueデバックについて"></a>Tips1. Glueデバックについて</h2><p>Glueの開発・デバッグには、開発エンドポイントを利用すると便利です。</p>
<p>ローカルのコンソール上から開発エンドポイントへSSH接続することで、Glueに対しpythonやscalaのREPLを使用できます。</p>
<p>エンドポイント作成はGlueのDPU数を指定し、SSH用の鍵を設定するのみです。わずか数クリック、1~2分で完了できます。ただし、エンドポイント作成後は裏でGlue(Spark)のクラスタ構築が行われるため、実際に使用可能となるのはその完了後（約15分ほどかかりました）です。</p>
<p>作成したエンドポイントへApache Zeppelin ノートブックを接続し、ノートブックでの開発も可能なようですが、今回は必要でなかったため使用しませんでした。ノートブックはローカルでもEC2上でも利用可能なようです。詳細は以下をご覧ください。<br><a href="https://docs.aws.amazon.com/ja_jp/glue/latest/dg/dev-endpoint.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/ja_jp/glue/latest/dg/dev-endpoint.html</a></p>
<p>また開発エンドポイントも裏ではGlueのクラスタが上がっているため、通常通りのGlue料金が請求されます。開発エンドポイントのDPU数はデフォルトで5、料金は1DPUあたり$0.44/時かかります。(公式ドキュメントには$0.44/秒と誤記されてますが、そんな高くないです)。</p>
<p>仮に終業時に開発エンドポイントを落とし忘れた場合、翌出社までに以下の金額がかかります。<br>　　5 <em> $0.44 </em> 14時間(20時退社、10時出社) = $30.8<br>1回の飲み代程度です。一晩にしてはなかなかです。自動で落とす方法もちょっと調べてみましたが見つけられず…。<br>忘れずに落とすようにしましょう！</p>
<h3 id="※開発エンドポイントを使わない場合※"><a href="#※開発エンドポイントを使わない場合※" class="headerlink" title="※開発エンドポイントを使わない場合※"></a>※開発エンドポイントを使わない場合※</h3><p>開発開始直後は開発エンドポイントの存在を知らず、Glueコンソール上でソースを編集し実行することで開発をしていました。<br>その当時の開発スタイルの状況も悪い例として記載します。<strong>興味が無い場合はスキップください。</strong></p>
<p><strong>（※開発エンドポインを利用すれば解決できる内容です。開発エンドポイントをぜひ利用しましょう！）</strong></p>
<ul>
<li><p>ジョブ実行に時間かかる</p>
<ul>
<li>Glueジョブは実行の都度リソースを確保しクラスタの構成を行います。そのため純粋なコンピューティング以外で毎回10分ほど待たされます</li>
<li>ソースを編集 → 実行(10分以上かかる) → 結果を確認、ソースを修正 → 実行(10分以上かかる) → …</li>
</ul>
</li>
<li><p>不要なログが多く目的のログに辿り着けない</p>
<ul>
<li>Glueの実行ログはCloudWatch上から確認可能ですが、実行スクリプトのログとSparkのログが同一のログストリーム上に出力されます<ul>
<li>そのため大量のSparkのログに目的のスクリプトログが埋もれます</li>
<li>CloudWatchから目的のログに辿り着けるよう、ログにプレフィクスなどをつけるなど工夫が必要でした</li>
</ul>
</li>
<li>ソースを編集 → 実行(10分以上かかる) → CloudWatchのログストリーム表示 → プレフィクスで目的のログ検索 → 結果を確認 → ソースを修正 → 実行(10分以上かかる) → …</li>
</ul>
</li>
<li><p>ソース修正、実行のための画面遷移が多い</p>
<ul>
<li>ジョブの一覧画面からスクリプトを参照することはできますが、編集するには専用の編集画面で行う必要があります。そのため編集の都度画面遷移が必要です</li>
<li>また、編集画面にはジョブの実行ボタンが存在しますが、このボタンが反応せずほとんど実行開始してくれません(原因は不明)。そのため実行の都度またジョブ一覧画面へと遷移する必要があります</li>
<li>ソースを編集 → 編集画面から実行画面へ → 実行(10分以上かかる) → CloudWatchのログストリーム表示 → プレフィクスで目的のログ検索 → 結果を確認 → 実行画面から編集画面へ → ソースを修正 → 編集画面から実行画面へ → 実行(10分以上かかる) → …</li>
</ul>
</li>
</ul>
<p>どうでしょう？壮絶な開発状況を想像していただけたでしょうか？<br>エンジニアたるもの、あるものは利用して賢く効率よく働きたいと、改めて思いました…</p>
<p>これらの苦労は <strong>開発エンドポイントを利用すれば</strong> その多くが回避できます。ぜひ利用しましょう！</p>
<h2 id="Tips2-AWS-Athenaで簡易的にデータ確認"><a href="#Tips2-AWS-Athenaで簡易的にデータ確認" class="headerlink" title="Tips2. AWS Athenaで簡易的にデータ確認"></a>Tips2. AWS Athenaで簡易的にデータ確認</h2><p>Glueのテーブルを使用する場合は、Athenaのクエリで中身を確認できるため開発が捗りました。</p>
<p>Athenaからテーブルに対して、 <code>SELECT * FROM TABLE</code> などのクエリを投げても良いですし、Glueコンソールのテーブル一覧画面から「アクション」→「データの確認」を選択しても良いです。後者の場合は自動でAthenaから <code>SELECT * FROM TABLE limit 10</code> というクエリを投げてくれます。どちらの場合もAthenaの料金が発生するため、読み取りデータ量には注意が必要です。</p>
<p>また、開発にSparkSQLを用いる場合はAthenaも同じSQLであるため、AthenaでSQLを開発してからSparkへ移植という使い方が可能です。ただし、AthenaはPrestoベース、SparkSQLはHiveSQLのスタイルをベースに開発されており、利用できる構文に微妙な差異がるため注意が必要です。<br>Athenaで開発したSQLをそのまま移植しようとした時に少しハマりもしました。</p>
<p>例えば文字列結合の場合、以下のようなSQLはAthenaでは利用できてSparkSQLでは利用できません。</p>
<figure class="highlight sql"><figcaption><span>AthenaSQLとSparkSQL</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">-- AthenaではOK、SparkSQLではNG</span></div><div class="line"><span class="keyword">SELECT</span> <span class="string">'str1'</span> || <span class="string">'str2'</span> || <span class="string">'str3'</span></div><div class="line"></div><div class="line"><span class="comment">-- SparkSQLでOK</span></div><div class="line"><span class="keyword">SELECT</span> <span class="keyword">CONCAT</span>(<span class="string">'str1'</span>, <span class="string">'str2'</span>, <span class="string">'str3'</span>)</div></pre></td></tr></table></figure>
<p>SparkSQLの詳細については、GlueのVersionが2018.08月時点では2.1.1ですので、下記のガイドを参考ください。<br><a href="https://spark.apache.org/docs/2.1.1/sql-programming-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/2.1.1/sql-programming-guide.html</a></p>
<h2 id="Tips3-Glueのカタログデータについて"><a href="#Tips3-Glueのカタログデータについて" class="headerlink" title="Tips3. Glueのカタログデータについて"></a>Tips3. Glueのカタログデータについて</h2><p>Glueといえばカタログデータ、という印象がありましたが実はカタログが未登録でもETLジョブは実行可能です。</p>
<p>GlueのデータカタログはApache Hive メタストアとの互換性がありますが、EMR以外にもAthenaやRedshift Spectrum でも利用できます。</p>
<p>開発Tips2であるように一部のS3バケットに対してはAthenaのクエリを発行したかったためデータカタログを設定しましたが、ETL処理に閉じて見た場合に恩恵が無いように感じられたため最終的にはデータカタログを利用しませんでした。</p>
<h2 id="Tips4-DataFrameとDynamicFrameについて"><a href="#Tips4-DataFrameとDynamicFrameについて" class="headerlink" title="Tips4. DataFrameとDynamicFrameについて"></a>Tips4. DataFrameとDynamicFrameについて</h2><p>Glueでは2種類のDataFrameを利用することができます。<br>SparkのDataFrameと、Glueで独自に定義されたDynamicFrameです。</p>
<p>両者ともテーブルの構造でデータを持ち、データ操作を行えるという点は共通していますが、DynamicFrameはchoice型を扱うという点で差異があります。choice型とは、同一列に複数のデータ型を持つことができる型です。</p>
<p>例えば、同一列にstringとdoubleを含むデータをDynamicFrameに読み込んだ場合、以下のようなイメージとなります。</p>
<p>取り込み元データ</p>
<table>
<thead>
<tr>
<th style="text-align:left">col1</th>
<th style="text-align:left">col2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">str</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">20.3</td>
</tr>
</tbody>
</table>
<p>取り込み後DynamicFrame</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">root</div><div class="line">┣- col1: int</div><div class="line">┣- col2: choice</div><div class="line">┃┣- string</div><div class="line">┃┣- double</div></pre></td></tr></table></figure>
<p>col2は2種類の型のデータが存在するためchoice型となり、stringとdouble両方を保持します。<br>様々なデータ取り込みに対応可能とするため、このような構造になっているのでしょう。</p>
<p>ただchoice型はこのままでは扱いにくいため、resolveChoice関数によって扱いやすい形へ変換してあげるとよいです。<br>resolveChoice関数では以下のことができます。</p>
<ul>
<li>choice列を任意の型にcastする。(choice列を、例えばstring列へ変換する)</li>
<li>choice列に含まれる型別に、新しく列を生成する。(stringとdoubleの2列を生成する)</li>
<li>choice列に含まれる型を保持できる構造体の列を生成する。(stringとdoubleを保持できる構造体列を1列生成する)</li>
<li>choice列を任意の型にcastした列を生成する。(例えばstring列を1列生成する)</li>
</ul>
<p>今回は、調査時の情報量が圧倒的にSparkのDataFrameの方が多く使いやすいため、データの取り込み後choice列は早々にstringへ変換し、かつSparkのDataFrameへ変換して使用しています。</p>
<p>データの取り込みはDynamicFrameで行い、本格的なデータの加工はSparkのDataFrameで行う、という使い分けが良いかと思います。</p>
<p>DynamicFrameやresolveChoiceの詳細は以下を参考ください。<br><a href="https://docs.aws.amazon.com/ja_jp/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/ja_jp/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html</a> (edited)</p>
<h1 id="Glueの注意点"><a href="#Glueの注意点" class="headerlink" title="Glueの注意点"></a>Glueの注意点</h1><p>Glueを利用して感じた注意点をまとめます。</p>
<h2 id="注意1-既存データに対するUpdate"><a href="#注意1-既存データに対するUpdate" class="headerlink" title="注意1. 既存データに対するUpdate"></a>注意1. 既存データに対するUpdate</h2><p>Glueではカラムの加工、テーブルの新規作成（SQLでいうCreate As Select）、テーブルのJoinなどETL処理ができます。またその特性上、中間データはS3上に配備されることが多いと思います。</p>
<p>しかしS3上にある既存カラムの <strong>Updateはできません</strong> ので注意が必要です。<br>これは裏側で動くSparkがあくまでS3へ追記しか行っていないからでしょう。<br>どうしてもUpdateしたいときは以下で代用できないかなど追加の検討が必要です。</p>
<ol>
<li>Delete &amp; Insert でテーブルやパーティション自体を再作成する</li>
<li>既存データの更新せずデータを追記し、抽出時にDistinctするロジックを追加する</li>
</ol>
<p>どちらにしても、書き込みや読み込みに余分な処理が発生するため処理コストが多くかかってしまいます。<br>最終的な利用元である分析に対して、どれくらいのデータ鮮度が求められるか、費用対効果で考える必要があると思います。</p>
<p>※ちなみに、DynamoDBに対しては書き換えたい情報だけに絞り込んでInsertすることで、実質的にUpdate処理が可能となりますが、DynamoDBのRCU/WCUの費用を考えると利用したいユースケースは少なそうです。</p>
<h2 id="注意2-C言語に依存するパッケージ（Pandas等）が利用不可"><a href="#注意2-C言語に依存するパッケージ（Pandas等）が利用不可" class="headerlink" title="注意2. C言語に依存するパッケージ（Pandas等）が利用不可"></a>注意2. C言語に依存するパッケージ（Pandas等）が利用不可</h2><p><a href="https://docs.aws.amazon.com/ja_jp/glue/latest/dg/aws-glue-programming-python-libraries.html" target="_blank" rel="noopener">リファレンスにも記載</a>がありますが、Glueの仕様でC言語依存パッケージを使うことができません。<br>Scikit_learnを使いたかったのですが、内部パッケージにpandasが使われているため起動できず、Scikit_learnの処理は違うアーキテクチャ（ECS）に切り分けました。すでに使いたいものが決まっている場合は注意してください。</p>
<p>もともとは、PySparkのUDFで処理可能と想定していましたが、Glueのこの制約を見逃していて、半日潰してしまいました。</p>
<p>C言語依存パッケージがGlueで使えない理由としては、コンパイルが絡んでいるのではないかと考えています。今後のGlueの機能拡張で使えるようになってくれると便利さが増しますね。</p>
<h2 id="注意3-ジョブブックマークが対応していない入力"><a href="#注意3-ジョブブックマークが対応していない入力" class="headerlink" title="注意3. ジョブブックマークが対応していない入力"></a>注意3. ジョブブックマークが対応していない入力</h2><p>Glueの機能にジョブブックマークというすでに処理されたデータかどうかを判定し、処理済みであれば次のジョブでは入力データに含めないという機能があります。<br>2018.08時点ではJSON、CSVなどには対応しているものの、zipファイルやParquestには対応していませんでした。<br><a href="https://docs.aws.amazon.com/ja_jp/glue/latest/dg/monitor-continuations.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/ja_jp/glue/latest/dg/monitor-continuations.html</a></p>
<p>そのため、もしジョブブックマーク非対応の入力に対しては、手動でオフセット管理する必要があります。<br>今回、わたしたしは入力ディレクトリに処理済みかどうかのフラグファイルを配備し、Glueジョブ上でその有無を確認することで処理対象とするか判定するロジックを追加しました。</p>
<h2 id="注意4-S3上のソースファイルの実行"><a href="#注意4-S3上のソースファイルの実行" class="headerlink" title="注意4. S3上のソースファイルの実行"></a>注意4. S3上のソースファイルの実行</h2><p>S3にソースファイルを配置する際に複数ファイルの場合は、zip圧縮する必要があります。<br>地味ですが忘れると動かないのでご注意を。</p>
<h2 id="注意5-並立分散処理"><a href="#注意5-並立分散処理" class="headerlink" title="注意5. 並立分散処理"></a>注意5. 並立分散処理</h2><p>Sparkの設定にちょっとした工夫が必要です。<br>第2回目の記事で詳しく説明したいと思います。</p>
<h2 id="注意6-料金について"><a href="#注意6-料金について" class="headerlink" title="注意6. 料金について"></a>注意6. 料金について</h2><p>Glueの料金計算はやや特殊でDPU (Data Processing Unit) という数に基づいて時間（1秒）ごとに課金が発生します。2018.08時点では1DPUでは4vCPU・16GBメモリが提供されます。<br>2018.08時点では <strong>10分の最小期間が設定</strong> されているため、処理時間が10分以下のミニバッチを連続的に起動させたい場合にはコスト的には不利になってしまいます。</p>
<p><a href="https://aws.amazon.com/jp/glue/pricing/" target="_blank" rel="noopener">https://aws.amazon.com/jp/glue/pricing/</a></p>
<p>これを避けるために分析部門にとってはデータの鮮度は下がるものの、20-30分単になどに処理頻度を変更する余地が無いか、費用対効果から見た全体最適の視点で検討中です。</p>
<h2 id="注意7-リソースの確保について"><a href="#注意7-リソースの確保について" class="headerlink" title="注意7. リソースの確保について"></a>注意7. リソースの確保について</h2><p>（2018.08時点、東京リージョンで発生した事象です）Glueのリソースは先に述べたDPUという単位でコンピューティングされます。これを性能検証のために、数十DPUといった比較的大きめに確保しようとするとリソースが確保できず起動できなかったことが何度かありました。<br>2018.08時点ではGlueリソースをリザーブド化することもできず、設定レベルでの回避が難しい状態です。</p>
<p>東京リージョンでGlueが利用可能になったのは<a href="https://aws.amazon.com/jp/about-aws/whats-new/2017/12/aws-glue-is-now-available-in-the-asia-pacific-tokyo-aws-region/" target="_blank" rel="noopener">2017.12</a> と比較的新しく、今後も継続的にリソースの増強などが期待されるため、改善に向かうと予想しています。現時点では実行タイミングによっては確保が難しい場合があるようです。人気のサービスであるという証拠なのかもしれませんね。</p>
<p>他の時間帯・日本国外リージョンなどを試すことや、処理粒度をある程度細かくし急激に大きなDPUを確保しないようにするなどの工夫が必要になってきます。また、どうしても確実に実行できないと困る！という場合は、リザーブドインスタンスでEMRを利用するしか無いようです。</p>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>Glue（直訳：のり）とはよく名を付けたものだと感じています。<br>粗削りで膨大なデータを、使いやすい形に成形してあげる。つまり、データと後続のシステムをうまくつなぎ合わせることができるものがGlueです。</p>
<p>以下にあてはまる方はGlueの導入を考えてみたらどうでしょうか。</p>
<ul>
<li>ビッグデータの処理が必要</li>
<li>起動時間は常時ではなく短い</li>
<li>サーバーを立てる余裕がない</li>
<li>運用、保守する余裕がない</li>
<li>パイオニア精神がある</li>
</ul>
<p>実はGlueの記事はネット上にはまだ多くない状態です。<br>そのため、Glue開発を導いていきたいというパイオニア精神ある方におすすめの領域だと思います。</p>
<h1 id="次回のGlueの記事について"><a href="#次回のGlueの記事について" class="headerlink" title="次回のGlueの記事について"></a>次回のGlueの記事について</h1><p>次回の内容はGlueを用いた性能改善を予定しています。<br>皆さんの参考になれば光栄です。</p>
<p>Glueを検討の方はご気軽にご連絡ください。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/20180828/photo_20180828_01.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;みなさん、初めまして、お久しぶりです、こんにちは。&lt;br&gt;フューチャーアーキテクト2018年新卒入社、1年目エンジニアのTIG（Technology Inno
    
    </summary>
    
      <category term="BigData" scheme="https://future-architect.github.io/categories/BigData/"/>
    
    
      <category term="AWS" scheme="https://future-architect.github.io/tags/AWS/"/>
    
  </entry>
  
  <entry>
    <title>データベースマイグレーション ～OracleからPostgreSQLへ～　－第２回ー</title>
    <link href="https://future-architect.github.io/articles/20180809/"/>
    <id>https://future-architect.github.io/articles/20180809/</id>
    <published>2018-08-09T06:00:36.000Z</published>
    <updated>2018-08-09T05:31:32.626Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-初めに"><a href="#1-初めに" class="headerlink" title="1. 初めに"></a>1. 初めに</h2><p>こんにちは。Technology Innovation Groupの岸田です。</p>
<p>前回に引き続き、データベースのマイグレーションがテーマです。<br>第1回はデータベースマイグレーションについて流れやポイントとなる点について記載してきました。<br>第2回は以下3点について紹介します。</p>
<ul>
<li>移行支援ツールの紹介</li>
<li>実際にシステムで稼働しているデータベース環境とアプリケーション（SQL）を使った評価</li>
<li>データベースマイグレーションの検討の方針</li>
</ul>
<h2 id="2-移行支援ツール"><a href="#2-移行支援ツール" class="headerlink" title="2. 移行支援ツール"></a>2. 移行支援ツール</h2><p>データベースのマイグレーションに際しては、移行ツールがいくつか公開されているため、それを利用するのが良いでしょう。このようなツールを利用してマイグレーションの作業量を削減していくことになります。</p>
<table>
<thead>
<tr>
<th>#</th>
<th>ツール名称</th>
<th>説明</th>
<th>参照先</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>ora2pg</td>
<td>GPLにもとづくオープンソースフリーソフトウェア</td>
<td><a href="http://ora2pg.darold.net/" target="_blank" rel="noopener">ora2pgのHP</a></td>
</tr>
<tr>
<td>2</td>
<td>AWS Schema Conversion Tool</td>
<td>AWSにて公開のツール</td>
<td><a href="https://docs.aws.amazon.com/ja_jp/SchemaConversionTool/latest/userguide/CHAP_Welcome.html" target="_blank" rel="noopener">AWS Schema Conversion Tool</a></td>
</tr>
</tbody>
</table>
<p>簡単ではありますが、2つの移行ツール(ora2pg/AWS Schema Conversion Tool)について紹介します。<br>移行ツールは現利用環境のデータベース定義を別環境に再現（ライセンスに注意！）し、再現環境に接続してツールを実行します。<br>もちろん、現利用環境に直接接続できる場合は別環境を用意する必要はございません。</p>
<h3 id="2-1-ora2pg"><a href="#2-1-ora2pg" class="headerlink" title="2-1 ora2pg"></a>2-1 ora2pg</h3><p>ora2pgはOracle/MySQLからPostgreSQLへの移行を支援するツールです。動作環境はLinuxとなります。</p>
<h4 id="できること"><a href="#できること" class="headerlink" title="できること"></a>できること</h4><p>ora2pgでは以下を行うことが可能です。</p>
<ul>
<li>スキーマ定義のコンバージョン用DDLの作成<ul>
<li>接続した環境からOracleデータベースにあるオブジェクト定義からPostgreSQLで実行可能なDDLの出力</li>
</ul>
</li>
<li>スキーマコンバージョンのレポート出力<ul>
<li>PostgreSQLに変換する際の評価レポート</li>
</ul>
</li>
<li>データ移行SQLの作成<ul>
<li>PostgreSQLで実行可能なINSERT文またはCOPY文の形式によるDMLの出力</li>
</ul>
</li>
<li>SQLファイルのコンバージョン</li>
</ul>
<p>ora2pgの具体的な利用方法は<a href="https://ora2pg.darold.net/documentation.html" target="_blank" rel="noopener">こちら</a>を参照してください。<br>参考までに、本ブログではora2pgはバージョン18.2を利用して確認しております。</p>
<p>コンバージョンの実行は、confファイル内で<code>TYPE</code>句にて指定し、個々に確認していきます。インストール時に用意されているconfファイル内にTYPEで指定できるキーワードが記載されております。（SYNONYMとDBLINKは記載がありませんが、指定が可能です。）<br>TYPEで指定できるキーワードとしては大きく3つの種類に分けられます。</p>
<table>
<thead>
<tr>
<th style="text-align:left">種別</th>
<th style="text-align:left">キーワード</th>
<th style="text-align:left">説明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">TABLE</td>
<td style="text-align:left">テーブル、インデックス、制約等のテーブルに関連するオブジェクト</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">PACKAGE</td>
<td style="text-align:left">パッケージ ⇒ シノニム＋ファンクションに変換</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">VIEW</td>
<td style="text-align:left">ビュー</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">GRANT</td>
<td style="text-align:left">オブジェクトの権限</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">SEQUENCE</td>
<td style="text-align:left">順序</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">TRIGGER</td>
<td style="text-align:left">トリガー</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">FUNCTION</td>
<td style="text-align:left">ファンクション</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">PROCEDURE</td>
<td style="text-align:left">プロシージャ ⇒ ファンクションに変換</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">TABLESPACE</td>
<td style="text-align:left">表領域</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">TYPE</td>
<td style="text-align:left">タイプ</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">PARTITION</td>
<td style="text-align:left">パーティション表 ⇒ 親子型のパーティション表に変換</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">FDW</td>
<td style="text-align:left">外部表</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">MVIEW</td>
<td style="text-align:left">マテリアライズド・ビュー</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">KETTLE</td>
<td style="text-align:left">XMLテンプレート</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">SYNONYM</td>
<td style="text-align:left">シノニム</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">DBLINK</td>
<td style="text-align:left">データベース・リンク</td>
</tr>
<tr>
<td style="text-align:left">データ</td>
<td style="text-align:left">INSERT</td>
<td style="text-align:left">データ登録DML（INSERT形式）</td>
</tr>
<tr>
<td style="text-align:left">データ</td>
<td style="text-align:left">COPY</td>
<td style="text-align:left">データ登録DML（COPY形式）</td>
</tr>
<tr>
<td style="text-align:left">SQL</td>
<td style="text-align:left">QUERY</td>
<td style="text-align:left">SQL文の変換</td>
</tr>
</tbody>
</table>
<p>スキーマコンバージョンのレポートは以下のようなコマンドとなります。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/usr/local/bin/ora2pg -c /etc/ora2pg/ora2pg.conf -t SHOW_REPORT --estimate_cost --dump_as_html &gt; report.html</div></pre></td></tr></table></figure></p>
<p>レポートの出力結果例は以下です。</p>
<p><img src="/images/20180809/ora2pg_report.png" alt=""></p>
<p>ora2pgのSQLの変換については、ファイル単位にコンバートしていくため、各SQLファイルに対してora2pgを実行する必要があります。多少手間ではありますが、シェルを組んでディレクトリ内のファイル（例えば拡張子がsqlのものとか）に対してora2pgを実行することになるでしょう。また、コンバートできないものについてはエラーが出力されることなく、そのままの記載内容でアウトプットファイルが出力されますので、個々にコンバージョン結果の確認が必要となります。</p>
<h3 id="2-2-AWS-Schema-Conversion-Tool-AWS-SCT"><a href="#2-2-AWS-Schema-Conversion-Tool-AWS-SCT" class="headerlink" title="2-2 AWS Schema Conversion Tool(AWS-SCT)"></a>2-2 AWS Schema Conversion Tool(AWS-SCT)</h3><p>AWS-SCTは、AWSが提供しているツールです。オンプレのデータベースシステムをAWSクラウドへシフトする支援用のツールとなっていまして、変換可能なデータベースが多数存在します。<br>変換可能なデータベースの対応表は以下の通りです。</p>
<table>
<thead>
<tr>
<th style="text-align:left">変換元データベース</th>
<th style="text-align:left">変換先データベース</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">MS SQL Server</td>
<td style="text-align:left">MySQL/PostgreSQL/Redshift</td>
</tr>
<tr>
<td style="text-align:left">MySQL</td>
<td style="text-align:left">PostgreSQL</td>
</tr>
<tr>
<td style="text-align:left">Oracle</td>
<td style="text-align:left">MySQL/PostgreSQL/Redshift</td>
</tr>
<tr>
<td style="text-align:left">PostgreSQL</td>
<td style="text-align:left">MySQL</td>
</tr>
<tr>
<td style="text-align:left">Greenplum</td>
<td style="text-align:left">Redshift</td>
</tr>
<tr>
<td style="text-align:left">Netezza</td>
<td style="text-align:left">Redshift</td>
</tr>
<tr>
<td style="text-align:left">Vertica</td>
<td style="text-align:left">Redshift</td>
</tr>
</tbody>
</table>
<p>AWS-SCTを起動して、プロジェクト（作業領域）をオープンすると、変換元と変換先を選択することになります。<br>例えば、接続元がOracle(OLTP)を選択すると接続先で選べるのは、以下となっております。</p>
<ul>
<li>Amazon RDS for MySQL</li>
<li>Amazon Aurora (MySQL compatible)</li>
<li>Amazon RDS for PostgreSQL</li>
<li>Amazon Aurora (PostgreSQL compatible)</li>
<li>Amazon RDS for Oracle</li>
</ul>
<h4 id="できること-1"><a href="#できること-1" class="headerlink" title="できること"></a>できること</h4><p>AWS-SCTでは以下を行うことが可能です。</p>
<ul>
<li>スキーマ定義のコンバージョン用DDLの作成<ul>
<li>接続した環境からOracleデータベースのオブジェクト定義からPostgreSQLで実行可能なDDLの出力</li>
</ul>
</li>
<li>スキーマコンバージョンのレポート出力<ul>
<li>PostgreSQLに変換する際の評価レポート</li>
</ul>
</li>
<li>変換先データベースに対するスキーマの作成<ul>
<li>上記DDLを実行して変換先のデータベースにスキーマ及びオブジェクトを作成</li>
</ul>
</li>
<li>SQLファイルのコンバージョン</li>
</ul>
<p>AWS-SCTはGUIでの操作なのでユーザフレンドリーですね。また、レポートについても画面で表示されるため、イメージしやすいです。<br>スキーマのコンバージョンレポートイメージは以下。</p>
<p><img src="/images/20180809/AWS-SCT_user_report.png" alt=""></p>
<p>以下はオブジェクトの詳細イメージ。</p>
<p><img src="/images/20180809/AWS-SCT_object_report.png" alt=""></p>
<p>緑は自動変換可能、グレーっぽいのは単純変換で対応可能、黄色は多少手を加える必要がある、赤は要見直しという感じに考えていただければよいです。</p>
<p>AWS-SCTの中でとても便利な機能はアプリケーションのコンバージョンです。<br>Windows上の指定したフォルダ配下に格納されているファイル（もちろんサブフォルダも対象）の中からSQLを抽出してコンバージョンを評価します。ファイルの形式はJAVA/C++/C#といったものが選択できます。もちろんSQLのみが記載されているファイルでもOKです。スキーマのコンバージョン同様に抽出したSQL単位に対応内容をグルーピングします。レポート結果イメージは以下です。</p>
<p><img src="/images/20180809/AWS-SCT_sql_report.png" alt=""></p>
<p>また、各SQL毎にGUI上でツールによって変換された内容を見ることができます。</p>
<p>下図の例では、4つのSQL文が記述されているファイルに対するコンバージョン結果を表示しております。上部の左上の欄にはソースファイルの内容が表示されます。その下の枠には、ソースファイルから抽出されたSQL文が表示されます。1つのファイルに複数のSQL文が記載されていた場合は、SQLを選択することで個別に表示されます。抽出されたSQLの右の枠にPostgreSQL用に変換されたSQLが表示されます。その枠の上部のApplyボタンで元ファイルへの反映ができ、右上の枠内に反映結果が表示されます。その枠の上部のSaveボタンで反映結果を元のファイルへ保存することも可能です。現時点では評価したファイルすべてに対して一括反映⇒一括保存ができるような機能は無いと思われますので、コンバージョン結果の反映は面倒ですね。（そのような機能があるのかもしれませんが見つけられてません。）</p>
<p><img src="/images/20180809/AWS-SCT_sql_conv.png" alt=""></p>
<p>自動変換ができないものについては、各SQL単位にアドバイスが出力されます。そのアドバイスから、手動でメンテナンスを実施していくことになります。</p>
<h2 id="3-移行支援ツール検証"><a href="#3-移行支援ツール検証" class="headerlink" title="3. 移行支援ツール検証"></a>3. 移行支援ツール検証</h2><p>実際にとあるシステムのDB環境及びアプリケーションに対して移行ツールを使ってみました。</p>
<h3 id="3-1-テーブルのコンバージョン"><a href="#3-1-テーブルのコンバージョン" class="headerlink" title="3-1 テーブルのコンバージョン"></a>3-1 テーブルのコンバージョン</h3><p>実際のテーブルのデータ型についてはツールにより自動変換されます。ora2pgではconfファイル内で、変換ルールを定義することも可能です。<br>あくまでもルールに基づいた変換となっておりますので、システム内での標準化基準があるようでしたら、そちらに従うようにしてください。<br>それぞれのツールでのデフォルトの自動変換ルールは以下の表のようになっております。</p>
<table>
<thead>
<tr>
<th style="text-align:left">属性</th>
<th style="text-align:left">Oracleのデータ型</th>
<th style="text-align:left">ora2pgでの変換後のデータ型</th>
<th style="text-align:left">AWS-SCTでの変換後のデータ型</th>
<th>備考</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">文字列</td>
<td style="text-align:left">CHAR(n)</td>
<td style="text-align:left">char(n)</td>
<td style="text-align:left">character(n)</td>
<td>PostgreSQLのnは文字数</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NCHAR(n)</td>
<td style="text-align:left">char(n)</td>
<td style="text-align:left">character(n)</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">VARCHAR2(n)</td>
<td style="text-align:left">varchar(n)</td>
<td style="text-align:left">character varying(n)</td>
<td>PostgreSQLのnは文字数</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NVARCHAR2(n)</td>
<td style="text-align:left">varchar(n)</td>
<td style="text-align:left">character varying(n)</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">CLOB</td>
<td style="text-align:left">text</td>
<td style="text-align:left">text</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">LONG</td>
<td style="text-align:left">text</td>
<td style="text-align:left">text</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">数値</td>
<td style="text-align:left">NUMBER</td>
<td style="text-align:left">bigint</td>
<td style="text-align:left">double</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n)</td>
<td style="text-align:left">smallint</td>
<td style="text-align:left">numeric(n,0)</td>
<td>n=1～4</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n)</td>
<td style="text-align:left">integer</td>
<td style="text-align:left">numeric(n,0)</td>
<td>n=5～9</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n)</td>
<td style="text-align:left">bigint</td>
<td style="text-align:left">numeric(n,0)</td>
<td>n=10～19</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n)</td>
<td style="text-align:left">decimal</td>
<td style="text-align:left">numeric(n,0)</td>
<td>n=20～38</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n,m)</td>
<td style="text-align:left">real</td>
<td style="text-align:left">numeric(n,m)</td>
<td>n=2～6</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n,m)</td>
<td style="text-align:left">double precision</td>
<td style="text-align:left">numeric(n,m)</td>
<td>n=7～15</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n,m)</td>
<td style="text-align:left">decimal</td>
<td style="text-align:left">numeric(n,m)</td>
<td>n=16～38</td>
</tr>
<tr>
<td style="text-align:left">日付</td>
<td style="text-align:left">DATE</td>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">timestamp</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">TIMESTAMP</td>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">timestamp</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">バイナリ</td>
<td style="text-align:left">BLOB</td>
<td style="text-align:left">bytea</td>
<td style="text-align:left">bytea</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">RAW</td>
<td style="text-align:left">bytea</td>
<td style="text-align:left">bytea</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">その他</td>
<td style="text-align:left">ROWID</td>
<td style="text-align:left">oid</td>
<td style="text-align:left">character(255)</td>
<td></td>
</tr>
</tbody>
</table>
<p>特殊型については個別に御確認ください。</p>
<h3 id="3-2-アプリケーションソースのコンバージョン"><a href="#3-2-アプリケーションソースのコンバージョン" class="headerlink" title="3-2 アプリケーションソースのコンバージョン"></a>3-2 アプリケーションソースのコンバージョン</h3><p>スキーマ定義のコンバージョンは一定ルールで変更できるのが確認できましたが、マイグレーション検討を行うにあたって一番の壁がアプリケーションソースのコンバージョンです。SQLは各製品で互換性がない部分があるためソース(SQL)を一つ一つ確認していく必要があります。そのためソースの全量を把握することがまず最初の第一歩となります。</p>
<p>今回利用したシステムのアプリケーション(java)はDBに対するクエリはjavaソース内に記載されているのではなく、SQLごとにファイルが作成される実装方式のためSQLファイルを1か所に纏めてコンバージョンの検証を実行することができました。SQLの本数は1,150本です。</p>
<p>ora2pgはアプリケーションコードのファイル単位に実行が必要であり、また実行結果も目視確認しなければならないことから、今回はAWS-SCTの実行事例をご紹介致します。</p>
<p>SQLのコンバージョン結果レポートを出力させると<strong>SQLConversion Actionsタブ</strong>にコンバージョン評価に対するアドバイスの一覧が表示されます。各ISSUEが発生しているSQL数が<strong>Number of occurrences</strong>として記載されており、▶をクリックすることでそのISSUEが発生しているファイルの一覧を表示することができますので、どのSQLでどのISSUEが発生しているか一目でわかるようになっています。また、各ISSUEの部分を見てわかる通り、ISSUEに関連した記載のあるマニュアルのURLも表示されていますので、実際にどのように直すかの参考になります。</p>
<p><img src="/images/20180809/AWS-SCT_sql_conv_detail.png" alt=""></p>
<p>今回コンバージョン検証をでてきた中で見直さなくてはいけないものとしてはどのようなものがあったのでしょう。少し見ていきます。</p>
<h3 id="SQL自動変換"><a href="#SQL自動変換" class="headerlink" title="SQL自動変換"></a>SQL自動変換</h3><p>前回のブログにて記載した内容（外部結合・DECODE関数・HINT句）については自動的に変換されます。<br>加えて、Oracleでは省略が可能な表や列の別名の定義の際に記載する<code>AS</code>の補完等も実施されます。<br>注意が必要なのは、AWS-SCTでの変換は変換後のデータベースをRDSを想定しているため、RDSで用意されているOracle互換用のスキーマ（aws_oracle_ext）で代替が可能な記載については自動的に変換されます。例えば、<code>SYSDATE</code>、<code>ADD_MONTHS</code>、<code>TO_DATE</code>、<code>TO_CHAR</code>等があります。</p>
<h4 id="UPDATE文"><a href="#UPDATE文" class="headerlink" title="UPDATE文"></a>UPDATE文</h4><p>UPDATE文については2種類のISSUEとして挙がってきています。<code>5065</code>と<code>5608</code>です。改修ポイントとして以下に纏めます。それぞれ、最新のPostgreSQLでは修正点は少なくて済みます。</p>
<table>
<thead>
<tr>
<th style="text-align:left">ISSUE番号</th>
<th style="text-align:left">メッセージ</th>
<th style="text-align:left">詳細内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">5065</td>
<td style="text-align:left">PostgreSQL doesn’t support the UPDATE statement for a subquery</td>
<td style="text-align:left">UPDATEの対象テーブルにサブクエリ(VIEW)を指定することができない　⇒　9.1以降であればCTE等を使って改修してください。</td>
</tr>
<tr>
<td style="text-align:left">5608</td>
<td style="text-align:left">Unable to convert the UPDATE statement with multiple-column subquery in SET clause</td>
<td style="text-align:left">SET句に複数行を指定し、更新後の値をサブクエリで指定する方法はサポートされていない  → 9.5以降で記載できるようになったため、そのままで実行可能となります。</td>
</tr>
</tbody>
</table>
<h4 id="結果相違"><a href="#結果相違" class="headerlink" title="結果相違"></a>結果相違</h4><p>OracleとPostgreSQLでの実行で検索結果が異なる可能性があります。<br>ここで挙がったSQLについては改修後のテストで重点的に新旧結果比較をすることをお奨め致します。</p>
<p>今回の検証で挙がった事象としては3種類ありました。</p>
<p>まずは、関数の<code>GREATEST</code>と<code>LEAST</code>です。検証結果としては<code>5271</code>と<code>5272</code>となります。<br>PostgreSQLだと関数で指定した列内にNULLが含まれるとNULLが返ります。そのため、COALESCE関数でNULLを変換する必要がでてきます。</p>
<p>次に、正規表現になります。検証結果としては<code>5617</code>となります。Oracleでは、<code>REGEXP_LIKE</code>関数となりますが、PostgreSQLにはそのような関数が無く、AWS-SCTでは<code>~</code>に自動変換されます。正規表現の判定がOracleとPostgreSQLで異なる可能性があり警告として出力されているものと考えております。</p>
<h4 id="変換不可"><a href="#変換不可" class="headerlink" title="変換不可"></a>変換不可</h4><p>移行支援ツールで自動変換が困難なものが、検証結果の<code>5340</code>、<code>5621</code>と<code>9996</code>にあたります。</p>
<p>MERGE文、PostgreSQLでは実装されていない事前定義関数で代替が困難なもの（今回はSUBSTRB関数）や複雑なSQL文のようなものとなります。コンバージョン結果としては何も変換されずにISSUEで挙がっている文言が記載されます。<br>参考までに、MERGE文については前回のブログを参照していただければ修正ポイントが分かります。</p>
<h4 id="変換結果まとめ"><a href="#変換結果まとめ" class="headerlink" title="変換結果まとめ"></a>変換結果まとめ</h4><p>AWS-SCTのSQLのコンバージョン検証結果を纏めると以下のようになります。</p>
<p><img src="/images/20180809/AWS-SCT_sql_conv_summary2.png" alt=""></p>
<p>AWS-SCTによる分類のうちSUCCESSとLOWがそこまで労力をかけない修正（変換）でPostgreSQLで実行可能となります。また、上記で記載したようにSET句でのサブクエリの指定はPostgreSQL 9.5以降では可能となっているため、そちらを加えると<strong>88.2%</strong>となります。HIGHと分類されている、行ロックについては第1回にて説明しておりますので、そちらを参考にしてセッションパラメータの設定にて対処可能ですので、大きな修正となるのはMERGE文、サブクエリの更新とSUBSTRB関数の実装のみとなり、全体の6%程度に過ぎません。</p>
<p>このように、ソース全体のうち修正が必要なSQL量をざっと確認することができ、かつ修正難易度の目安についても確認できることが分かりました。コンバージョン計画立案時のコスト、スケジュール算出にとしても有用なものとなりそうです。</p>
<h5 id="参考までに"><a href="#参考までに" class="headerlink" title="参考までに"></a>参考までに</h5><p>ora2pgについても、外部結合やDECODE等の変換ができていることは確認しております。驚いたのは、ora2pgの外部結合の変換はすべてLEFT OUTER JOINに統一されているところです。（AWS-SCTは(+)の記載位置によりRIGHT OUTER JOINの変換もあります。）すべてを目視確認する場合はora2pgのSQL変換でも良いかもしれません。</p>
<h2 id="3-データベースマイグレーション検討の方針"><a href="#3-データベースマイグレーション検討の方針" class="headerlink" title="3. データベースマイグレーション検討の方針"></a>3. データベースマイグレーション検討の方針</h2><p>データベースマイグレーションを検討する際には、作業項目毎マイグレーションの難易度をに評価して実現性を検討していくことになります。どのようなシステムがマイグレーションがし易いのか考えてみます。</p>
<h3 id="データベースオブジェクト"><a href="#データベースオブジェクト" class="headerlink" title="データベースオブジェクト"></a>データベースオブジェクト</h3><p>データベースオブジェクトについての変換は第1回でも触れましたが、圧倒的にストアド・サブプログラムが難易度が高いです。その他についてはPostgreSQLで実装されていないOracle固有のオブジェクトを利用している場合は検討が必要となってきます。</p>
<h3 id="アプリケーションソース"><a href="#アプリケーションソース" class="headerlink" title="アプリケーションソース"></a>アプリケーションソース</h3><p>アプリケーションについては、ソースがファイルとして保持しているようであれば、変換ツールを施行することが可能となります。そのため、ソースファイルのような形でツールが利用できない場合は難易度が高くなります。動的にSQLを組んで実行する形やアプリケーション内にSQLが分散している場合などがそれにあたります。</p>
<p>また、ストアド・プログラムが存在する場合は、コンバージョン及びテストの難易度が上がります。<br>PostgreSQLのPL/pgSQLでファンクションとして作成することは可能ですが、コンバージョンのタイミングでアプリケーション側にロジックを寄せる検討をすることも視野に入れてください。<br>PL/SQLをPL/pgSQLにコンバージョンする際に気を付ける点としては、以下となります。</p>
<ul>
<li>PL/pgSQL関数内部ではCOMMITが使用できない</li>
<li>エラーが発生した場合は内部処理はすべてロールバックされる</li>
<li>パッケージが存在しないため、関数を跨った変数を定義できない</li>
<li>変数はすべて宣言する必要がある</li>
</ul>
<p>データベースへの接続方法としては、OracleもPostgreSQLも変わらないため特に意識する必要はありません。</p>
<table>
<thead>
<tr>
<th style="text-align:left">難易度</th>
<th style="text-align:left">アプリケーション実装方法</th>
<th style="text-align:left">システム例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>高</strong></td>
<td style="text-align:left">PL/SQLの実行／動的SQL</td>
<td style="text-align:left">バックグラウンドでの集計処理メインのシステム、分析用の検索システム</td>
</tr>
<tr>
<td style="text-align:left"><strong>低</strong></td>
<td style="text-align:left">実行されるSQLが外出しされている</td>
<td style="text-align:left">定型フォームのオンライン検索システム</td>
</tr>
</tbody>
</table>
<h3 id="非機能要件"><a href="#非機能要件" class="headerlink" title="非機能要件"></a>非機能要件</h3><p>これまでは、アプリケーションについて触れてきましたが、PostgreSQLはシステム内ではデータベースサーバとして稼働していくことになりますので、非機能要件についても要件を満たすかどうかを検討していくことになります。</p>
<p>現行システムで可用性要件や性能要件のためにOracle RAC構成を取っている場合があると思います。<br>Oracle RACはShared Everythingのデータベース構成となっていて、複数台のサーバがアクティブ状態で1つのデータベースを共有して稼動しますので、単体のサーバ障害ではシステムが停止しないということが大きな特徴です。また、複数台がアクティブに稼働しておりますので、分散処理も可能となります。  この要件が崩せない場合は、やはりOracleからのマイグレーションは難しくなります。</p>
<p>PostgreSQLでは、可用性を上げるために3rdベンダのクラスタウェアを利用したActive-Standby構成が考えられます。また、分散処理の一例としてレプリケーション機能を利用したMaster-Slave構成として参照処理をSlave側で実行させるといったことも考えられます。</p>
<p>その他、運用（バックアップ／監視／セキュリティ）等、非機能要件を確認したうえで、マイグレーションの評価をしていってください。OracleのEnterprise Editionの場合、PostgreSQLにはない機能が豊富にふくまれているため細かく確認が必要です。</p>
<p>クラウドサービスの場合、可用性（レプリケーション）、運用（バックアップ、監視、セキュリティ）がDB機能として提供されています（例えば、AWS RDS）のでこの部分のハードルはぐっと低くなると考えています。</p>
<h3 id="データ移行"><a href="#データ移行" class="headerlink" title="データ移行"></a>データ移行</h3><p>データベースのマイグレーションではなくても、システムの刷新時には検討されるのが、データの移行です。すいません、データ移行は非常に様々な検討項目があるため、書くとなると1回分くらいになってしまうかと思いますので、第2回のブログでも割愛させていただきます。<br>ただ、マイグレーションの難易度の評価は割愛せずに必ず実施してください。</p>
<h2 id="4-最後に"><a href="#4-最後に" class="headerlink" title="4. 最後に"></a>4. 最後に</h2><p>ここまでデータベースのマイグレーション（PostgreSQL）について確認してきましたが、ツールを利用したマイグレーションが有用であることが確認できました。また、実際のコンバージョンだけでなくプロジェクト計画を立てる際の一次評価としても有用と言えます。</p>
<p>第1回・第2回とはデータベースのマイグレーションに特化した内容で記載してきましたが、PostgreSQLへのマイグレーションの判断が下された後には、もちろんPostgreSQLとしてのデータベース設計は必要です。パラメータや配置などの物理設計や統計情報取得、VACUUM処理、バックアップ等の運用設計もしっかりとやっていきましょう。</p>
<p>フューチャーでは仮想的なDBチームが形成されており、各プロジェクトへの横断的に支援しております。OracleやPostgreSQLを中心としたノウハウの蓄積・共有なども活発に行っていますので、ご興味ある方は是非一緒に働いていきましょう！！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-初めに&quot;&gt;&lt;a href=&quot;#1-初めに&quot; class=&quot;headerlink&quot; title=&quot;1. 初めに&quot;&gt;&lt;/a&gt;1. 初めに&lt;/h2&gt;&lt;p&gt;こんにちは。Technology Innovation Groupの岸田です。&lt;/p&gt;
&lt;p&gt;前回に引き続き、デ
    
    </summary>
    
      <category term="DB" scheme="https://future-architect.github.io/categories/DB/"/>
    
    
      <category term="Migration" scheme="https://future-architect.github.io/tags/Migration/"/>
    
      <category term="PostgresSQL" scheme="https://future-architect.github.io/tags/PostgresSQL/"/>
    
      <category term="Oracle" scheme="https://future-architect.github.io/tags/Oracle/"/>
    
  </entry>
  
  <entry>
    <title>人工知能学会（JSAI2018）参加報告</title>
    <link href="https://future-architect.github.io/articles/20180723/"/>
    <id>https://future-architect.github.io/articles/20180723/</id>
    <published>2018-07-23T04:56:36.000Z</published>
    <updated>2018-07-23T05:11:28.147Z</updated>
    
    <content type="html"><![CDATA[<h2 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h2><p>みなさんこんにちは。SAIG(Strategic AI Group)の小池です。<br>前回の<a href="https://future-architect.github.io/articles/20180222/">NIPS2017 LT報告</a>以来の登場となります。<br>今回は、2018年6月5日(火)〜6月8日(金)に行われました人工知能学会（JSAI2018）にSAIGで参加して来ましたので、ご報告を致します。フューチャーでは、プラチナスポンサーとしてのブース出展・口頭発表・セッション聴講を行ってまいりました。</p>
<p><img src="/images/20180723/photo_20180723_01.png" class="img-middle-size"></p>
<p>場所は鹿児島県城山ホテル鹿児島にて開催されました。鹿児島空港から鹿児島中央駅までバスで1時間くらい、鹿児島中央駅から城山ホテル鹿児島まで30分程かかりました。会場は山の上にあり移動が大変でしたが、景観が良く、高級感溢れるホテルでした。</p>
<h2 id="口頭発表"><a href="#口頭発表" class="headerlink" title="口頭発表"></a>口頭発表</h2><p>SAIGの貞光九月がAI応用-産業応用(3)にて、「<a href="https://confit.atlas.jp/guide/event/jsai2018/subject/2M2-04/detail?lang=ja" target="_blank" rel="noopener">半教師有りグラフニューラルネットワークを用いたCRUD関係に基づくシステム移行単位の最適化</a>」と題して発表してまいりました。</p>
<h2 id="口頭発表の内容"><a href="#口頭発表の内容" class="headerlink" title="口頭発表の内容"></a>口頭発表の内容</h2><p>大規模なシステムが、社会的あるいは工学的観点で旧式化した際、システムの最新化が必要となります。例えば銀行のシステムが銀行の合併により改修が必要となるといった事例が代表的です。旧システムを無計画に改修していては、効率も悪く、むしろ前よりパフォーマンスを悪化することさえあるでしょう。そこで、どのようなサブセットでシステムを移行すべきか、という「移行単位」を人手で設計します。この際用いるのがCRUD表です。CRUD表とは、図1のように、システム内の機能とテーブルの参照関係を表で表したものです。CRUD表を見ることで、例えば一つのテーブルをcreateする機能と、削除する機能は、同じ移行単位とすべき、といった方針が見いだせます。</p>
<p><img src="/images/20180723/photo_20180723_02.png"><br>図1　CRUD表の例</p>
<p>本研究ではこのCRUD表から自動的に移行単位を推定することを試みています。CRUD表がグラフに対する隣接行列と解釈できることに着目し、半教師あり学習に基づくグラフノードの分類法を用います。具体的には<a href="https://papers.nips.cc/paper/6212-diffusion-convolutional-neural-networks" target="_blank" rel="noopener">Diffusion Convolutional Neural Network</a>を用い、機能とテーブルのファイル配置情報、いわゆるファイルパスを、CRUDグラフと統合して用います。さらにファイルパスの木構造の特性を活かすことができる<a href="https://arxiv.org/pdf/1705.08039.pdf" target="_blank" rel="noopener">Poincarē Embeddings</a>を併用することで、従来法に比べ、人手の移行単位に最も近い結果を得ています(図2)。</p>
<p><img src="/images/20180723/photo_20180723_03.png"></p>
<p>図2 CRUD情報とファイルパスの統合イメージおよびその手法の実験結果</p>
<h2 id="企業ブース"><a href="#企業ブース" class="headerlink" title="企業ブース"></a>企業ブース</h2><p>企業ブースでは、FutureにおけるAI案件の実績を展示いたしました。多くの学生さん、企業の方に来ていただき、大変盛り上がりました。Futureオリジナル清涼タブレットを配っていたのですが、3日を通して払底するなど多くの方に来ていただきました。</p>
<p><img src="/images/20180723/photo_20180723_04.jpeg" class="img-middle-size"></p>
<h2 id="セッション聴講"><a href="#セッション聴講" class="headerlink" title="セッション聴講"></a>セッション聴講</h2><p>口頭発表やインタラクティブセッションに参加してきました。その中でも、面白かったと思うものを紹介したいと思います。(以下の要約には、SAIGメンバーである貞光、藤田、勝村の3名にお手伝いいただきました）</p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/2C1-05/detail?lang=ja" target="_blank" rel="noopener">Word2Vecにおける出力側の重みに注目した文書分類手法の検討　(内田ら)</a></li>
</ul>
<p>近年言語処理において、word2vecのような分散表現が注目されています。分散表現は文書のクラスタリングや検索タスクにおいて、強力に働きます。一般的なword2vecにおける獲得手法は、3層のニューラルネットを用いて第1層の重み（W in)を学習させて利用します。一方で本研究では、第2層の重み(w out)も共起情報量があることからw in, w out両方を用いるべきだと主張しています。W in W outの両ベクトルの平均をとって単語ベクトルとすることで、分類タスクにおいて従来のw2vよりも精度が上がることを示しています。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/2J2-02/detail?lang=ja" target="_blank" rel="noopener">位置情報データによる競合店舗の利用状況の多様性を用いた購買予測手法の提案　（新美ら）</a></li>
</ul>
<p>実店舗での購買を把握する手法として、アンケートを用いた市場調査や、家計簿アプリのレシートスキャン等の利用が試みられてきましたが、これらは回答の偏りや調査対象者の負担という課題があります。<br>本研究では、①大手スーパーマーケットチェーンT社のID-POSデータ、②T社各店舗、および競合と想定される近隣の複数店舗に設定したジオフェンス（※）内への消費者の出入情報をマッチングさせる方法を提案しています。<br>※地域を特定の距離で区切った仮想の区画のこと。消費者が特定の複数のアプリケーションを利用した際にバックグラウンドで取得される位置情報が一般的であるが、GPSに基づく正確な位置情報ではなく、ジオフェンス内への侵入・退出を記録する方法を採用しています。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/1E2-03/detail?lang=ja" target="_blank" rel="noopener">深層学習を用いたユーザ離脱予測 (宮崎ら)</a></li>
</ul>
<p>ユーザ離脱予測に関する既存研究ではユーザ属性情報や行動データなどの特徴量を機械学習のモデルに読み込ませ、将来の一定期間における離脱を予想するものであり、Random ForestやSVMなど様々な手法が使われています。一方で深層学習を用いたユーザ離脱予測に関する研究は多くありません。本研究で、はユーザの離脱予測モデルに時系列相関を取り込むためにLSTMを採用し、既存手法であるRandom Forestと時系列性を考慮しない中間層が4層のMLPとの比較し、既存手法より提案手法が精度が高いことを示しました。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/3A1-03/detail?lang=ja" target="_blank" rel="noopener">低解像度の料理画像を超解像するためのSRGANの応用 (永野ら)</a></li>
</ul>
<p>Deep Learningによって目覚ましい発展を遂げた研究内容の一つに超解像が挙げられます。<br>超解像に関して様々なモデルが提案される中、人間にとっての見た目を評価するMOStestingにおいて他の超解像手法を凌ぐ性能を発揮しているものが、SRGAN[Leding 16]です。しかしSRGANは元画像のjpgノイズや撮影の際に生じたブレを修復できない問題や撮影対象「らしい」テクスチャが生成されない問題があります。本研究では料理画像を対象にこれらの問題に対して、解きたい問題に応じた適切なデータセットを構築するといった基本的なアイデアに基づき定性的・定量的な実験を実施した。SRGANに対しノイズを加えた低解像度画像の作成やドメイン毎にデータを分割してモデルを作成・比較することで、性能が向上することを確認しています。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/3E1-04/detail?lang=ja" target="_blank" rel="noopener">化学構造式のためのハイパーグラフ文法 (梶野)</a></li>
</ul>
<p>任意の特徴を持つ化学分子を設計する際、近年SMILESと呼ばれる文字列表現を元に、VAEを用いる手法が提案されています。(Gomez-Bombarelli 2018) しかしSMILESとVAEを組み合わせた時の欠点として、出力された文字列が分子グラフの要件を満たさないという課題がありました。著者らは独自に、常に原子価を保持するという条件を担保する分子ハイパーグラフ構造(HRG)を定義し、今後VAEを組み合わせることで分子設計が容易になると主張しています。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/3E1-01/detail?lang=ja" target="_blank" rel="noopener">BPE Sequence to Sequence を利用した単変異によるタンパク質耐熱性変化予測　（河野ら）</a></li>
</ul>
<p>工業的に有用なタンパク質の耐熱性を向上させることを目的に、タンパク質を構成する数百のアミノ酸の一つを別のアミノ酸（全20種類）に置換する方法がありますが、実際にどのアミノ酸を変化させると効果があるのか予測が難しいといった課題があります。<br>本研究では、アミノ酸配列全体に関する情報を十分に含む特徴量として、Byte Pair Encoding(BPE) Seq2Seqモデルに、既知の特徴量（実験条件やアミノ酸の性質など）を組み合わせたものを提案し、既存法より高い精度で耐熱性変化を予測できることを示しています。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/1P3-05/detail?lang=ja" target="_blank" rel="noopener">CADソフトの操作ログ分析による操作スキルの抽出 (新實ら)</a></li>
</ul>
<p>CADの手順操作ログから、決定木を使ってベテランのスキルを検知するという内容。<br>JSAIではこのように、ある程度枯れた技術を産業応用していく、という発表が多くありましたが、本研究は研究課題が明確かつ、有益な効果が得られた、ということがとても分かりやすく示されていました。<br><br></p>
<h2 id="おわりに"><a href="#おわりに" class="headerlink" title="おわりに"></a>おわりに</h2><p>JSAI2018では、多くの収穫があり非常に有益だったと思います。<br>知識もさることながら、優秀な研究者・エンジニアの方との出会いや企業さんとの出会いもありました。<br>今後も学会やイベントに参加していきますので、見かけた際には気軽にお声をかけてくださればと思います。<br>以上小池でした。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h2&gt;&lt;p&gt;みなさんこんにちは。SAIG(Strategic AI Group)の小池です。&lt;br&gt;前回の&lt;a href=&quot;https:
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="MachineLearning8" scheme="https://future-architect.github.io/tags/MachineLearning8/"/>
    
  </entry>
  
  <entry>
    <title>データベースマイグレーション ～OracleからPostgreSQLへ～　ー第１回ー</title>
    <link href="https://future-architect.github.io/articles/20180529/"/>
    <id>https://future-architect.github.io/articles/20180529/</id>
    <published>2018-05-29T02:06:05.000Z</published>
    <updated>2018-05-29T02:44:12.077Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-初めに"><a href="#1-初めに" class="headerlink" title="1. 初めに"></a>1. 初めに</h2><p>こんにちは。Technology Innovation Groupの岸田です。</p>
<p>データベースシステムに対しては、高い信頼性・可用性・安定性が求められることから、データベースとしては、Oracle Database（以降はOracleと記載）やMicrosoft SQL Serverなどの商用製品が採用されてきました。</p>
<p>十数年前より、商用データベースの高いコストに不満を持つ企業において、OSSデータベースに注目が集まってきており、近年では基幹システムにおいてもOSSデータベースが採用されるケースも多くなっています。<br>このような背景から技術者としてもOSSデータベースのスキルは非常に求められている状況かと思います。</p>
<p>元々高額なライセンス料に加え、仮想基盤における課金体系の問題や2016年のOracle Database Standard Edition Oneの廃止などにより、基盤更改などを契機にOSS検討事例が増えてきました。<br>また、クラウド基盤を選択肢に入れた場合にも、柔軟にスケールアップが可能なOSSを採用したいという要望もあります。</p>
<p>OSSデータベースで広く採用されているのはPostgreSQLとMySQLがあります。今回は、エンタープライズ領域においてマイグレーション例が多いOracleからPostgreSQLについて、考慮すべき事項について2回にわたり紹介していきます。</p>
<h2 id="2-マイグレーションの流れ"><a href="#2-マイグレーションの流れ" class="headerlink" title="2. マイグレーションの流れ"></a>2. マイグレーションの流れ</h2><h3 id="データベースの選定（アセスメント）"><a href="#データベースの選定（アセスメント）" class="headerlink" title="データベースの選定（アセスメント）"></a>データベースの選定（アセスメント）</h3><p>単にコスト面に問題を抱えているからといって、安易にOSSデータベースへのマイグレーションを決断することは危険です。そのシステムにおいて提供しているサービスに関して、マイグレーションによって機能要件及び非機能要件を満たせなくなっては元も子もありません。</p>
<p>マイグレーションの検討については、OracleとPostgreSQLの技術的な検討だけでなく、コスト算定（コストメリット）、移行期間やテストなどで多方面での検討が必要となります。この検討については、以下のようにQCDの視点で検討を行う必要があります。</p>
<h4 id="Quality（品質）"><a href="#Quality（品質）" class="headerlink" title="Quality（品質）"></a>Quality（品質）</h4><p>「アプリケーション機能面」「システム非機能面」の両面について実現性を検討し、ノックアウト項目が存在しないかを評価します。システムによっては現行システムとのデータ連携や災害対策目的で別サイトへのレプリケーションを実現する必要が出てきます。この時点で抜け漏れが発生してしまわないように、現行の運用面を含めた細かい評価が必要となります。</p>
<h4 id="Cost（コスト）"><a href="#Cost（コスト）" class="headerlink" title="Cost（コスト）"></a>Cost（コスト）</h4><p>マイグレーションに関連するコスト評価をおこないます。以下の2点が考えられます。</p>
<ol>
<li>マイグレーションに関連する費用：「アプリケーションソース移行」「データベースミドルウェア自体の移行」「データ移行」  </li>
<li>新基盤での費用：「ソフトウェアライセンス、基盤費用」</li>
</ol>
<h4 id="Delivery（納期）"><a href="#Delivery（納期）" class="headerlink" title="Delivery（納期）"></a>Delivery（納期）</h4><p>マイグレーション実現にむけたスケジュールの評価を行います。基盤更改期限がある場合についはその期間内に実現可能かという評価が必要になります。</p>
<p>これらの検討結果を基にデータベースマイグレーションの判断を行います。</p>
<h3 id="マイグレーション決定後の作業イメージ"><a href="#マイグレーション決定後の作業イメージ" class="headerlink" title="マイグレーション決定後の作業イメージ"></a>マイグレーション決定後の作業イメージ</h3><p>データベースのマイグレーションが決定した後は、一例として以下のように進めていきます。</p>
<p><img src="/images/20180529/migration_phase.png"></p>
<p>赤字となっている作業については、実装されているアプリケーションにより作業量が大きく変動するものとなります。<br>データベースのマイグレーションの成功は、現状アプリケーションの正確な把握に直結すると考えておりますので、現状把握については専門技術者を交えてしっかりと実施することをお奨め致します。</p>
<h2 id="3-マイグレーションのポイント"><a href="#3-マイグレーションのポイント" class="headerlink" title="3. マイグレーションのポイント"></a>3. マイグレーションのポイント</h2><p>前章にてマイグレーションの流れを説明しましたが、この章では実際にマイグレーションを実施するにあたりポイントとなる点を記載していきます。ポイントとしては以下の3点となります。</p>
<ol>
<li><strong>スキーマ移行</strong><ul>
<li>データベースの内部環境（オブジェクト等）の移行</li>
<li>2章のDB設計・データベース移行（定義移行）にあたる</li>
</ul>
</li>
<li><strong>アプリケーション移行</strong><ul>
<li>アプリケーションプログラムの移行。主にSQL改修となる</li>
</ul>
</li>
<li><strong>データ移行</strong><ul>
<li>データベースに格納されているデータの移行</li>
</ul>
</li>
</ol>
<p>上記それぞれについてみていきます。（今回はデータ移行は紹介まで）</p>
<h3 id="3-1-スキーマ移行（データベースオブジェクト）"><a href="#3-1-スキーマ移行（データベースオブジェクト）" class="headerlink" title="3-1. スキーマ移行（データベースオブジェクト）"></a>3-1. スキーマ移行（データベースオブジェクト）</h3><p>2章で記載しましたDB設計フェーズにて方針を設計し、データベース移行フェーズにて実装致します。</p>
<p>Oracleに存在する主なオブジェクトについてPostgreSQLの存在有無は以下の通りです。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Oracle</th>
<th style="text-align:left">PostgreSQL</th>
<th style="text-align:left">備考</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">テーブル</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">インデックス</td>
<td style="text-align:left">△</td>
<td style="text-align:left">B-Treeインデックスとパーティションインデックスが存在。逆キーインデックス、ビットマップインデックス等は存在しない。</td>
</tr>
<tr>
<td style="text-align:left">ビュー</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">マテリアライズド・ビュー</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">シノニム</td>
<td style="text-align:left">×</td>
<td style="text-align:left">一部ビューで代替可能。</td>
</tr>
<tr>
<td style="text-align:left">シーケンス</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">トリガー</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">データベースリンク</td>
<td style="text-align:left">×</td>
<td style="text-align:left">dblink関数または、FDW(Foreign Data Wrapper) で代替可能。</td>
</tr>
<tr>
<td style="text-align:left">パッケージ</td>
<td style="text-align:left">×</td>
<td style="text-align:left">3-2で記載。</td>
</tr>
<tr>
<td style="text-align:left">プロシージャ</td>
<td style="text-align:left">×</td>
<td style="text-align:left">3-2で記載。</td>
</tr>
<tr>
<td style="text-align:left">ファンクション</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<p>〇：存在　△：一部存在　×：無し</p>
<p>インデックスについては一部の種類のインデックスは存在しませんが、インデックス自体は性能要件を満たすために作成されていますので、インデックスの存在自体がマイグレーションに影響を及ぼすことはありません。ただし、マイグレーション後の性能要件を満たすために別の方法を検討する可能性があります。</p>
<p>その他につきましては、代替機能を含めてPostgreSQLにて実装ができると考えております。</p>
<p>ストアド・パッケージ／プロシージャ／ファンクションについては3-2.アプリケーション移行で記載致します。</p>
<h4 id="データ型の変換"><a href="#データ型の変換" class="headerlink" title="データ型の変換"></a>データ型の変換</h4><p>テーブルのコンバージョンの際には、データ型を意識する必要があります。OracleとPostgreSQLのデータ型の対応表は以下の通りです。数値型にはいくつかのデータ型が存在しますので、システムとしてルールを定めることをお奨め致します。</p>
<table>
<thead>
<tr>
<th style="text-align:left">属性</th>
<th style="text-align:left">Oracleのデータ型</th>
<th style="text-align:left">PostgreSQLのデータ型</th>
<th style="text-align:left">備考</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">文字列</td>
<td style="text-align:left">CHAR/NCHAR/VARCHAR2/NVARCHAR2</td>
<td style="text-align:left">char/varchar</td>
<td style="text-align:left">PostgreSQLの文字型の精度は文字数を意味する</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">CLOB/LONG</td>
<td style="text-align:left">text</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">数値</td>
<td style="text-align:left">NUMBER</td>
<td style="text-align:left">smallint/bigint/integer/decimal/real/double precision</td>
<td style="text-align:left">精度によってデータ型を選択</td>
</tr>
<tr>
<td style="text-align:left">日付</td>
<td style="text-align:left">DATE/TIMESTAMP</td>
<td style="text-align:left">timestamp</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">バイナリ</td>
<td style="text-align:left">BLOB/RAW</td>
<td style="text-align:left">bytea</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">その他</td>
<td style="text-align:left">ROWID</td>
<td style="text-align:left">oid</td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h3 id="3-2-アプリケーション（SQL）移行"><a href="#3-2-アプリケーション（SQL）移行" class="headerlink" title="3-2.アプリケーション（SQL）移行"></a>3-2.アプリケーション（SQL）移行</h3><p>アプリケーションの移行については、同じ製品のバージョンアップをする場合であっても一通りアプリケーションの動作確認（結果の現新一致確認）は実施することが多いと思います。</p>
<p>データベースのマイグレーションの場合は、このアプリケーションの動作確認において結果の不一致による原因究明やアプリケーションの見直しが発生する可能性が高くなることが考えられるため、当初よりアプリケーションの移行作業の作業量を多めに見積もることが大事だと思います。3章で説明したツールなどを利用して自動的なコンバージョンでアプリケーションの改修にかかる時間は削減できますが、アプリケーションの動作確認は必ず実施してください。</p>
<p>アプリケーションの動作確認と同様に重要なのは、性能テストとなります。データベースのマイグレーションでは、データベースの機能の差異がありますので、SQLの処理性能は事前の机上予測が難しいといえます。この性能テストについてもデータベースのバージョンアップ時のテスト作業量よりも多めに見積もっておくことをお奨め致します。</p>
<p>Oracleにて実装されているSQLについて、Oracle独自の記法であったとしてもPostgreSQLにて実装は可能であると考えておりますので、アプリケーション改修によりマイグレーションがNGとなることは無いと思います。</p>
<p>ただし、アプリケーションに依存して、改修作業や改修後の確認テストの作業量が変動していきますので注意が必要です。</p>
<p>アプリケーション移行についての考え方及び対策の一部を以下に紹介していきます。</p>
<h4 id="ストアド・サブプログラム変換"><a href="#ストアド・サブプログラム変換" class="headerlink" title="ストアド・サブプログラム変換"></a>ストアド・サブプログラム変換</h4><p>ストアド・サブプログラムは、Oracleでのストアド・パッケージ、ストアド・プロシージャ、ストアド・ファンクションの総称です。<br>PostgreSQLにはパッケージ及びプロシージャが存在しません。<br>Oracleのストアド・パッケージ及びストアド・プロシージャはPostgreSQLのファンクションで実装することになります。<br>プロシージャ、ファンクションの集合体であるパッケージはスキーマで代用します。<br>まとめると以下のようになります。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Oracle</th>
<th style="text-align:left">PostgreSQL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">パッケージ</td>
<td style="text-align:left">スキーマ</td>
</tr>
<tr>
<td style="text-align:left">プロシージャ</td>
<td style="text-align:left">ファンクション</td>
</tr>
<tr>
<td style="text-align:left">ファンクション</td>
<td style="text-align:left">ファンクション</td>
</tr>
</tbody>
</table>
<p>PostgreSQLのファンクションでOracleと大きく異なる部分は、トランザクションの制御となります。PostgreSQLのファンクションは、呼び出し元のトランザクションに依存するため、ファンクション内でCOMMITの発行はできません。つまり、ファンクション内でエラーが発生した場合は、ファンクション内の処理はすべてロールバックします。</p>
<p>参考までに、Oracleでは、<code>PRAGMA AUTONOMOUS_TRANSACTION</code>を利用して呼び出し元とトランザクションを分離することができます。Oracleのストアド・サブプログラム内でトランザクションを分離している場合は、PostgreSQLではロジックを見直す必要があります。</p>
<h4 id="外部結合"><a href="#外部結合" class="headerlink" title="外部結合"></a>外部結合</h4><p>Oracle独自の記法としては外部結合が挙げられます。外部結合はSQLにおいて結合条件で対応するレコードが存在しない場合でも優先となるテーブルについてはレコードが除外されない結合方法です。</p>
<p>Oracle 9i以降はSQL標準である<code>[LEFT | RIGHT] OUTER JOIN</code>の記述がサポートされるようになり、オラクル社としても同バージョンからOUTER JOINによる記法を<a href="https://docs.oracle.com/cd/E82638_01/SQLRF/Joins.htm#GUID-29A4584C-0741-4E6A-A89B-DCFAA222994A" target="_blank" rel="noopener">マニュアル上でも推奨しています</a>。</p>
<p>とはいえ、Oracleで動かすSQLの外部結合は(+)表記をよく目にします。<br>主な理由としては以下じゃないかなと思ってます。</p>
<ul>
<li>Oracle 8i 以前に作成したSQLが今でも使われている。（バージョンアップを繰り返していて改修していない。）</li>
<li>以前プロジェクトとして作成していたシステムのSQLコーディング基準書では(+)表記で記載することが基準となっており、現状でも(+)表記自体は利用可能であるため、基準書自体を修正することができていない。また、成功したプロジェクトの基準書を横展開している。</li>
<li>プログラムの改修や新機能導入においても、現行で動作しているSQLを踏襲して作成する。</li>
</ul>
<p>PostgreSQLにおいて外部結合は当然SQL標準であるOUTER JOINの記載となりますので、データベースマイグレーションの際にはSQLのコンバージョンが必要です。</p>
<p>(+)表記での外部結合をコンバージョンするときに気を付けなければならないのは、リテラル値に対する外部結合の条件がある場合です。</p>
<p>言葉だけでは分かりづらいと思いますので、Oracleで用意されているサンプルスキーマ（SCOTTユーザ）で見ていきます。テーブルはEMP表とDEPT表を使います。デフォルトの状態から少しだけ値を変えているところはあります。</p>
<p><img src="/images/20180529/demo.png"></p>
<p>2種類の外部結合表記のSQL文を用意しました。</p>
<p>①リテラル条件にも「(+)」を付与したパターン</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> d.deptno, d.dname, e.empno, e.ename, e.comm</div><div class="line"><span class="keyword">FROM</span> emp e, dept d</div><div class="line"><span class="keyword">WHERE</span> e.deptno(+) = d.deptno</div><div class="line"><span class="keyword">AND</span> e.comm(+) = <span class="number">300</span>;</div></pre></td></tr></table></figure>
<p>②リテラル条件には「(+)」を付与しないパターン<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> d.deptno, d.dname, e.empno, e.ename, e.comm</div><div class="line"><span class="keyword">FROM</span> emp e, dept d</div><div class="line"><span class="keyword">WHERE</span> e.deptno(+) = d.deptno</div><div class="line"><span class="keyword">AND</span> e.comm = <span class="number">300</span>;</div></pre></td></tr></table></figure></p>
<p>検索結果は以下の通りです。</p>
<p><img src="/images/20180529/outerjoin_result_SQL.png"></p>
<p>①の場合は結合前にリテラル条件で絞っている、②の場合は結合後にリテラル条件で絞っている、ということです。</p>
<p>これをSQL標準であるOUTER JOINで記載すると以下のようになります。OUTER JOINの条件となるか全体の条件となるかの違いがSQL標準の方が分かりやすいですね。</p>
<p>①<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> d.deptno, d.dname, e.empno, e.ename, e.comm</div><div class="line"><span class="keyword">FROM</span> scott.emp e</div><div class="line">    <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> scott.dept d</div><div class="line">        <span class="keyword">ON</span> (e.deptno = d.deptno <span class="keyword">AND</span> e.comm = <span class="number">300</span>);</div></pre></td></tr></table></figure></p>
<p>②<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> d.deptno, d.dname, e.empno, e.ename, e.comm</div><div class="line"><span class="keyword">FROM</span> scott.emp e</div><div class="line">    <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> scott.dept d</div><div class="line">        <span class="keyword">ON</span> (e.deptno = d.deptno)</div><div class="line"><span class="keyword">WHERE</span> e.comm = <span class="number">300</span>;</div></pre></td></tr></table></figure></p>
<p>念のため結果も。</p>
<p><img src="/images/20180529/outerjoin_result_SQL.png"></p>
<p>PostgreSQLでも。</p>
<p><img src="/images/20180529/outerjoin_result_postgres.png"></p>
<p>同じですね。</p>
<p>将来のことを考えて、外部結合はSQL標準のOUTER JOINで記述していくことにしましょう。</p>
<h4 id="組み込み関数"><a href="#組み込み関数" class="headerlink" title="組み込み関数"></a>組み込み関数</h4><p>組み込み関数はデータベース毎に事前に用意されている関数です。データベース毎に仕様が異なることもありますので、移行の際には注意が必要です。OracleとPostgreSQLの組み込み関数の対比表については<a href="https://www.pgecons.org/wp-content/uploads/PGECons/2012/WG2/10_Built-inFunctionMigrationResearch/10_Appendix_01_Built-inFunctionComparativeTable(Oracle-PostgreSQL).pdf" target="_blank" rel="noopener">こちら</a>に詳しく載っておりますので、参考にしてください。</p>
<p>OracleとPostgreSQLの両方で用意されていますが、機能仕様が全く異なる関数としては<code>DECODE</code>があります。<br>OracleではDECODE関数は条件分岐として使われています。構文としては以下です。</p>
<p><img src="/images/20180529/decode_oracle.png" class="img-middle-size"></p>
<p>PostgreSQLでは、DECODE関数はテキスト表現からバイナリデータを復号する関数となっております。構文としては以下です。（formatオプションはbase64/hex/escapeから選択。）</p>
<p><img src="/images/20180529/decode_postgres.png" class="img-small-size"></p>
<p>OracleでのDECODE関数はPostgreSQLではcase文に変換します。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> empno, ename, deptno,</div><div class="line"><span class="keyword">DECODE</span>( deptno, <span class="number">10</span>, <span class="string">'ACCOUNTING'</span>,</div><div class="line">                <span class="number">20</span>, <span class="string">'RESEARCH'</span>,</div><div class="line">                <span class="number">30</span>, <span class="string">'SALES'</span>,</div><div class="line">                <span class="number">40</span>, <span class="string">'OPERATIONS'</span> ) <span class="keyword">as</span> dname</div><div class="line"><span class="keyword">FROM</span> emp;</div></pre></td></tr></table></figure>
<p>変換後は以下となります。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> empno, ename, deptno,</div><div class="line"><span class="keyword">CASE</span> deptno <span class="keyword">WHEN</span> <span class="number">10</span> <span class="keyword">THEN</span> <span class="string">'ACCOUNTING'</span></div><div class="line">            <span class="keyword">WHEN</span> <span class="number">20</span> <span class="keyword">THEN</span> <span class="string">'RESEARCH'</span></div><div class="line">            <span class="keyword">WHEN</span> <span class="number">30</span> <span class="keyword">THEN</span> <span class="string">'SALES'</span></div><div class="line">            <span class="keyword">WHEN</span> <span class="number">40</span> <span class="keyword">THEN</span> <span class="string">'OPERATIONS'</span> <span class="keyword">END</span> <span class="keyword">as</span> dname</div><div class="line"><span class="keyword">FROM</span> emp;</div></pre></td></tr></table></figure>
<p>DECODEについては以上です。</p>
<p>一部のOracle独自の組み込み関数は、orafceモジュールをインストールすることで、いくつかOracleと同じ関数が実装されます。orafceで実装される関数については<a href="http://orafce.github.io/orafce/index-ja.html" target="_blank" rel="noopener">こちら</a>を参考にしてください。また、AWSのRDSでも事前にOracleからの移行用にモジュール（スキーマ：aws_oracle_ext）が準備されています。</p>
<h4 id="行レベルロック"><a href="#行レベルロック" class="headerlink" title="行レベルロック"></a>行レベルロック</h4><p>一般的に業務ロジック上では競合による不整合を回避するため、<code>SELECT ～ FOR UPDATE</code>により行レベルでロックを取得し、トランザクション中のレコードが他から更新・削除されることを防ぎます。</p>
<p>Oracleにおいては、<code>FOR UPDATE</code>句として<code>[WAIT n|NOWAIT]</code>の記述ができます。どちらも指定しない場合は、行が使用可能になるまで待機した後でSELECT文の結果が戻されます。（そんな記載はできませんが、<code>WAIT ∞</code>の指定のような挙動です。）</p>
<p>PostgreSQLでは、<code>WAIT n</code>が存在しないため、WAITと記載するとOracleにおける<code>WAIT</code>句を未指定とした場合と同様の挙動となります。つまり、Oracleにおいて<code>WAIT n</code>が指定されていた場合は、PostgreSQLではSQLにて実装することができません。</p>
<p>代替となるかは実行形式によりますが、<code>lock_timeout</code>のパラメータを指定することでOracleと同じ挙動となる可能性はあります。このパラメータを設定して<code>SELECT ～ FOR UPDATE</code>を発行しすると、該当レコードがロック状態であった場合は設定したパラメータの時間経過するとエラーとなります。<code>lock_timeout</code>はセッションレベルでの変更も可能ですので、トランザクション開始時にパラメータを設定をしてSQLを実行、トランザクション終了時にパラメータをリセットするという処理仕様とすることもできます。</p>
<h4 id="ヒント句"><a href="#ヒント句" class="headerlink" title="ヒント句"></a>ヒント句</h4><p>OracleにおいてSQLの性能問題は実行計画が原因であることが多いのではないでしょうか。確かに、Oracleにおける性能劣化対策としての「ヒント句を記載して実行計画を固定化する」は、統計情報に依存せずにSQLの性能を安定させる最適な解決策なのかもしれません。（バージョンアップの時にヒントが無くなったり、オプティマイザの機能向上により性能の良い実行計画が選ばれないというデメリット(?)もあります。）また、システムのSQLコーディング基準でヒント句を記載するといったルールがある場合もあるでしょう。</p>
<p>PostgreSQLではバージョン9.1以降<code>pg_hint_plan</code>モジュールをインストールすることでヒント句の記載はできますが、Oracleほど数多くの種類のヒントがあるわけではありません。（参考までにヒント句の種類としては、pg_hint_plan 1.1で23個、Oracle 12cR1で332個あります。）</p>
<p>データベースによりオプティマイザが全く異なりますので、データベースをマイグレーションすると実行計画が変化するのは致し方無いと考えております。そこで、ヒント句の記載のあるSQLについては、コンバージョン時には一旦ヒント句を削除して性能を見ることになります。処理性能が思わしくない場合は個別にチューニングの対応を施すことになります。</p>
<h4 id="NULLと空文字"><a href="#NULLと空文字" class="headerlink" title="NULLと空文字"></a>NULLと空文字</h4><p>OracleではNULLと空文字は同義で、空文字はNULLとして扱われます。PostgreSQLではNULLと空文字は別物です。従いまして、Oracle上で動作するSQL内で条件句として<code>WHERE COL is NULL</code>といった記載がある場合は、マイグレーションにより結果が異なってくる可能性があるので注意が必要です。</p>
<p>PostgreSQLでは、NULLの使用を禁止するといった基準を作った方が良いと思います。その場合は、データの移行時にNULLはすべて空文字に変換することは忘れずに！</p>
<p>NULLの四則演算やNULLと文字列の連結については、すべてNULLとなってしまうため、NULLが格納される可能性がある列を取り扱う場合は、必ず<code>COALESCE</code>関数を使って処理してください。（OracleでいうところのNVL関数ですね。）</p>
<h4 id="MERGE文"><a href="#MERGE文" class="headerlink" title="MERGE文"></a>MERGE文</h4><p>OracleではMERGE文が利用できますが、PosrgreSQLではMERGE文は存在しません。<br>PosrgreSQL 9.5からUPSERT文（INSERT ON CONFLICT）が使用可能となります。</p>
<p>こちらもサンプルスキーマ環境で見てみましょう。<br>empと同じ定義のemp_up表を作成してます。</p>
<p><img src="/images/20180529/merge_empup.png"></p>
<p>emp_up表のレコードを見てemp表にレコードが存在した場合はUPDATEをして、emp表にレコードが無ければINSERTをするというMERGE文を作ってみました。（今回はSAL列とCOMM列だけをUPDATEしてます。）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">MERGE</span> <span class="keyword">INTO</span> emp e</div><div class="line"><span class="keyword">USING</span> emp_up u <span class="keyword">ON</span> (e.empno = u.empno)</div><div class="line">  <span class="keyword">WHEN</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span></div><div class="line">    <span class="keyword">UPDATE</span> <span class="keyword">SET</span> e.sal = u.sal, e.comm = u.comm</div><div class="line">  <span class="keyword">WHEN</span> <span class="keyword">NOT</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span></div><div class="line">    <span class="keyword">INSERT</span> <span class="keyword">VALUES</span> ( u.empno, u.ename, u.job, u.mgr, u.hiredate, u.sal, u.comm, u.deptno );</div></pre></td></tr></table></figure>
<p>実行結果です。</p>
<p><img src="/images/20180529/merge_result.png"></p>
<p>empno:7369のsal列が変更され、empno:8000のレコードが作成されていますね。</p>
<p>これをPostgreSQLで実装すると以下のようになります。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> scott.emp</div><div class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> scott.emp_up</div><div class="line"><span class="keyword">ON</span> CONFLICT (empno)</div><div class="line"><span class="keyword">DO</span> <span class="keyword">UPDATE</span> <span class="keyword">SET</span> sal = excluded.sal, comm = excluded.comm ;</div></pre></td></tr></table></figure>
<p>実行結果です。</p>
<p><img src="/images/20180529/upsert_result.png"></p>
<p>同じ結果となりました。</p>
<p>このUPSERT文ですが、PostgreSQL 9におけるパーティションテーブルに対しては機能しません。理由としては、PostgreSQLのパーティションテーブルは親となるテーブルとパーティション単位の個別テーブルを作成して、親テーブルへのDML発行時にはトリガーにより該当のパーティションテーブルを更新しており、親表に対して<code>INSERT ON CONFLICT</code>を発行したとしてもトリガーとしては該当パーティションテーブルに対して<code>INSERT ON CONFLICT</code>を発行しないからです。</p>
<p>それであれば、トリガー内でのパーティションテーブルに対する構文を<code>INSERT ON CONFLICT</code>となるように作り直せば良いかというとそうもいきません。それは通常のINSERT文が発行された際もON CONFLICT付きのINSERTとなってしまうからです。</p>
<p>では、パーティションテーブルに対するMERGE文の変換はどうすればよいかというと、PostgreSQL 9.1で導入されたCommon Table Expression(CTE)を使って代替ができます。SQL文としては以下のようになります。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">WITH insrt AS (<span class="keyword">SELECT</span> * <span class="keyword">FROM</span> scott.emp_up),</div><div class="line">     updt  <span class="keyword">AS</span> (<span class="keyword">UPDATE</span> scott.emp <span class="keyword">set</span> sal = insrt.sal <span class="keyword">FROM</span> insrt</div><div class="line">                             <span class="keyword">WHERE</span> emp.empno = insrt.empno <span class="keyword">RETURNING</span> emp.empno)</div><div class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> scott.emp</div><div class="line">(<span class="keyword">SELECT</span> * <span class="keyword">FROM</span> insrt <span class="keyword">WHERE</span> empno <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> empno <span class="keyword">FROM</span> updt));</div></pre></td></tr></table></figure>
<p>実行結果です。</p>
<p><img src="/images/20180529/insert_CTE_result.png"></p>
<p>通常テーブルであれば、もちろんUPSERTでもCTEでも結果は一緒ですね。</p>
<h3 id="3-3-データ移行について"><a href="#3-3-データ移行について" class="headerlink" title="3-3. データ移行について"></a>3-3. データ移行について</h3><p>データ移行については今回のブログでは詳細な記載は割愛しますが、データ移行もマイグレーションにおいては非常に重要な作業となります。移行時間、キャラクタセットの違いやデータ抽出方法など、移行要件を満たすために様々な検討・設計が必要となります。</p>
<p>データベースの切替時にはデータベースを利用する業務の全面停止が必要にはなりますが、その停止時間を最小限とする要件を持つシステムも多いと思います。その場合は、データの事前移行＋切替直前まで常時レプリケーション⇒切替といった方式で業務停止時間を最短とする案もあります。事前移行方法やレプリケーション方式については機会があれば詳しく書こうとは思いますが、検討する事項はたくさんあります。<br>レプリケーション方式の一例としては、SaaSとして提供されているAWS Database Migration Serviceがあります。こちらのサービスは移行先のデータベースがAWSのPaaSを利用している場合となります。<br>AWS DMSの詳細は<a href="https://aws.amazon.com/jp/dms/" target="_blank" rel="noopener">こちら</a>を参照してください。</p>
<h2 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h2><p>第1回では、データベースマイグレーションの背景や流れと一般的なマイグレーションのポイントとなる点について記載していきました。データベースオブジェクト（スキーマ）、アプリケーション（SQL）について、多くの場合は一定ルールに基づき変更可能であることが分かります。</p>
<p>次回は、一般的に利用されているマイグレーションツールと、実際のアプリケーションにてマイグレーションの評価をおこなった例ついて記載していきたいと思います。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-初めに&quot;&gt;&lt;a href=&quot;#1-初めに&quot; class=&quot;headerlink&quot; title=&quot;1. 初めに&quot;&gt;&lt;/a&gt;1. 初めに&lt;/h2&gt;&lt;p&gt;こんにちは。Technology Innovation Groupの岸田です。&lt;/p&gt;
&lt;p&gt;データベースシステ
    
    </summary>
    
      <category term="DB" scheme="https://future-architect.github.io/categories/DB/"/>
    
    
      <category term="Migration" scheme="https://future-architect.github.io/tags/Migration/"/>
    
      <category term="PostgresSQL" scheme="https://future-architect.github.io/tags/PostgresSQL/"/>
    
      <category term="Oracle" scheme="https://future-architect.github.io/tags/Oracle/"/>
    
  </entry>
  
  <entry>
    <title>IoT/M2M展（音声認識サービス）の展示</title>
    <link href="https://future-architect.github.io/articles/20180522/"/>
    <id>https://future-architect.github.io/articles/20180522/</id>
    <published>2018-05-22T05:20:58.000Z</published>
    <updated>2018-05-22T05:58:15.910Z</updated>
    
    <content type="html"><![CDATA[<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>こんにちは、Strategic AI Group（通称 : SAIG）の岩田です。<br>前回の<a href="https://future-architect.github.io/articles/20180413/">ICLR2018記事</a>に引き続いての投稿になります。</p>
<p>今回は少し話題を変えまして、先日出展したIoT/M2M展のご報告と、そこで披露した音声認識デモについてのご紹介となります。</p>
<h1 id="IoT-M2M-展のご紹介"><a href="#IoT-M2M-展のご紹介" class="headerlink" title="IoT/M2M 展のご紹介"></a>IoT/M2M 展のご紹介</h1><ul>
<li>Japan IT Weekという非常に大きな展示会が年に3回あり、その一つとしてIoT/M2M展があります</li>
<li>IoT/M2M以外にもクラウドやAI・自動化、情報セキュリティ等、企業として関心が高いものブースが沢山あります</li>
<li>国内最大級の展示会イベントなので、とにかく人が多いです！（通路を歩くのもやっと・・）</li>
</ul>
<h1 id="展示のきっかけ"><a href="#展示のきっかけ" class="headerlink" title="展示のきっかけ"></a>展示のきっかけ</h1><ul>
<li>IoT/M2M展には弊社のグループ会社である <a href="https://www.trexedge.co.jp/" target="_blank" rel="noopener"><strong>TrexEdge</strong></a> が出展を決めました</li>
<li>共同出展という形で、ブース内の一部を借りてFutureのAIサービスも紹介を行なう運びとなりました</li>
</ul>
<p><img src="/images/20180523/photo_20180523_01.jpg" class="img-middle-size"></p>
<h2 id="出展物"><a href="#出展物" class="headerlink" title="出展物"></a>出展物</h2><p>2017年より開発していた音声認識サービスを展示してきました。<br>今回は展示（音声認識サービス）としてのポイントをいくつかご紹介します。</p>
<p><strong>※そもそも音声認識とは？</strong> という方は是非こちらを参照ください。<br><a href="https://www.slideshare.net/c5tom/ss-56184353" target="_blank" rel="noopener">自称・世界一わかりやすい音声認識入門</a></p>
<p><img src="/images/20180523/photo_20180523_02.jpeg" class="img-middle-size"></p>
<h3 id="サービス化への道のり"><a href="#サービス化への道のり" class="headerlink" title="サービス化への道のり"></a>サービス化への道のり</h3><p>音声認識サービスとしては、Google Cloud Platform の <a href="https://cloud.google.com/speech/?hl=ja" target="_blank" rel="noopener"><strong>Speech API</strong></a>を用いています。<br>自分たちでスクラッチでゼロから開発するのではなく、クラウド機械学習API を利用するという選択をしました。<br>理由は以下の2点があります。</p>
<ol>
<li>Google/Amazon/Microsoft/Apple等々のITガリバーと認識エンジンレベルで競い合うことはハードルが高く、差別化も難しい</li>
<li>クイックにデモサービスを作成し、実用段階を確かめたい（音声データの収集、認識エンジン作成の手間を削減する）</li>
</ol>
<p>Google APIの採用理由は、自社内で比較した中で最も精度が高かったためです。</p>
<p>早速、これを用いて「プロタイプ」を作り、触ってもらって実用レベルなのかを判断してみました（わくわく）。</p>
<p>…しかし、社内での判断としては「<strong>クラウドAPIを単純に呼び出すレベルでは実用としてはまだ遠い</strong>」というものでした。</p>
<p>そのため、クラウドAPIを活用し、その上に肉付けすることによって、「<strong>プロトタイプ</strong>」ラインから「<strong>現場で使える</strong>」ラインに昇華するという戦略を考えました。</p>
<p><img src="/images/20180523/photo_20180523_03.png"></p>
<p>「<strong>現場で使える</strong>」ラインにするために出した答えとしては、音声認識結果に対する<strong>後処理校正機能</strong>でした。<br>業界に関連するコーパスと作成した言語モデルを用いて、誤りと思われる部分を可視化してあげます。</p>
<p>また、Google側では現状として結果に句読点が入らないなど、実用で考えたときに足りない部分がいくつかあります。その部分を補い、そして顧客に合った形でサービス(UI/UX・AI/システムの機能運用)をカスタマイズする。</p>
<p>もちろん理想としては、人の手を煩わせない（どんな音声でも正解率100%）ですが録音ユースケースによっては、満点を求められることはないケースもあります。</p>
<p>したがって、ミニマムとしてそのケースの品質は担保し徐々に適用できるケースを広げていくというのがサービスの始まりになっています。</p>
<h3 id="サービス名"><a href="#サービス名" class="headerlink" title="サービス名"></a>サービス名</h3><p>本サービスとしての名称は <strong>Future Transcribe AI Platform</strong> として展開を実施していきます。</p>
<p><img src="/images/20180523/photo_20180523_04.png" class="img-middle-size"></p>
<p>こうしてサービスの名前も決まり、そして縁がありIoT/M2M展示会に出展できることとなりました。<br>当日は多くの質問や引き合いがあり、嬉しい限りでした。</p>
<h3 id="※注意"><a href="#※注意" class="headerlink" title="※注意"></a><strong>※注意</strong></h3><p>上記の書き方だと、特に考えもなしにプロトタイプを作った感があるので、少し補足しておきます。<br>音声認識には下記2点で優遇すべきポイントがあると認識しています。</p>
<ul>
<li>音声認識（文字おこし）をバリバリ使ってますという実例報告は多くない</li>
<li>顧客によってはインプットとなる音声の録音ユースケースが多い</li>
</ul>
<p>この2点を踏まえて、作ってみる価値があるという結論になってからの Go としています。</p>
<h3 id="業務観点として必要なマイクの選定"><a href="#業務観点として必要なマイクの選定" class="headerlink" title="業務観点として必要なマイクの選定"></a>業務観点として必要なマイクの選定</h3><p>サービスとして、認識エンジンの精度を向上させるのはもちろんですが、インプットとなる音声自体の質も重要です。<br>単一指向性や全指向性、付属するマイクの数、そして価格帯等を観点として複数マイクをならべた検証も現場で利用する上では必要です。</p>
<p>この録音ユースケースに対してマイクの自体の比較と、認識結果の比較の展示も行いました。</p>
<p>例えば、こちらはインタビューに対しての低価格帯レコーダー vs 高価格帯レコーダーの認識結果です。</p>
<p><img src="/images/20180523/photo_20180523_05.png"></p>
<p>一部変換ミスもありますが、言葉尻などの細かい部分で高価格帯は正しく認識することができています。</p>
<p>では、この結果から例えば</p>
<ul>
<li>低価格帯のマイクでもどのように録音できれば精度が向上するのか</li>
<li>マイクの組み合わせによって、より精度を向上させることができるのか</li>
</ul>
<p>といった疑問やアイデアがでてくるかと思います。<br>この点は社内で研究している最中ですので、何か有益な結果がでましたらまた公開できればと思います。</p>
<h2 id="おわりに"><a href="#おわりに" class="headerlink" title="おわりに"></a>おわりに</h2><p>今回は展示（音声認識サービス）にスポットを当てた話となりました。</p>
<p>サービスとしてのご紹介となってしまいましたが、弊社のSAIGグループでは、「AI導入の検討」から「カスタマイズされたAIシステム構築・運用」まで一貫してお手伝いさせていただきます。<br>お問い合わせは<a href="https://www.future.co.jp/contact_us/" target="_blank" rel="noopener"><strong>こちら</strong></a>からお願いします。</p>
<p>また、10月にございます<a href="http://expo.nikkeibp.co.jp/xtech/ex/ai/index.html" target="_blank" rel="noopener"><strong>人工知能／ビジネスAI 2018</strong></a>にもブースを出展する予定ですので是非お立ち寄りいただければと思います。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h1&gt;&lt;p&gt;こんにちは、Strategic AI Group（通称 : SAIG）の岩田です。&lt;br&gt;前回の&lt;a href=&quot;https
    
    </summary>
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/categories/MachineLearning/"/>
    
    
      <category term="IoT" scheme="https://future-architect.github.io/tags/IoT/"/>
    
  </entry>
  
  <entry>
    <title>ICLR2018 LT大会</title>
    <link href="https://future-architect.github.io/articles/20180413/"/>
    <id>https://future-architect.github.io/articles/20180413/</id>
    <published>2018-04-12T16:17:13.000Z</published>
    <updated>2018-04-12T16:52:46.755Z</updated>
    
    <content type="html"><![CDATA[<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>みなさん、こんにちは。<br>Strategic AI Group（通称 : SAIG）の岩田です。</p>
<p>今回は以前に社内開催された<a href="https://future-architect.github.io/articles/20180222/">NIPS2017のLT大会</a>につづき<br>ICLR2018の社内LT大会の模様を紹介いたします！</p>
<h1 id="ICLRとは"><a href="#ICLRとは" class="headerlink" title="ICLRとは"></a>ICLRとは</h1><ul>
<li>International Conference on Learning Representations の略</li>
<li>2013年度に初開催されたカンファレンスで、今年で6回目</li>
<li>2018年本大会ページは<a href="https://iclr.cc/doku.php?id=iclr2018:main" target="_blank" rel="noopener">こちら</a></li>
<li>表現学習という意味では採択される内容も広めです。ざっと投稿された表題に目を通しましたが、やはり研究内容としてHOTであるadversarial（敵対的な）に関連する研究は多かったように見受けられました。</li>
</ul>
<h2 id="社内勉強会の様子"><a href="#社内勉強会の様子" class="headerlink" title="社内勉強会の様子"></a>社内勉強会の様子</h2><p>SAIGグループに閉じた開催ではなく、社内で興味のあるメンバーを募って実施しています。<br>開催時間帯としては、一番メンバーの集まりやすいお昼時に開催です！</p>
<p><img src="/images/20180403/photo_20180403_01.jpeg"><br><img src="/images/20180403/photo_20180403_02.jpeg"></p>
<p>実演も交えた形で初学習者にもわかりやすい内容になっていました。<br>（プレゼンへのこだわりは、たとえLTスタイルだとしてもFutureらしさがあります）<br><img src="/images/20180403/photo_20180403_03.jpeg"></p>
<h1 id="論文の選択"><a href="#論文の選択" class="headerlink" title="論文の選択"></a>論文の選択</h1><p><a href="https://openreview.net/group?id=ICLR.cc/2018/Conference" target="_blank" rel="noopener">Conference Track</a> の「Oral Papers」「Poster Papers」から各々が興味あるものを選択し、披露しました。<br>私個人としては分散表現に興味があるので、関連するテーマを選択しています。</p>
<h1 id="LTの内容"><a href="#LTの内容" class="headerlink" title="LTの内容"></a>LTの内容</h1><p>簡単にではありますが、それぞれの発表内容を記載します！</p>
<h2 id="1-Diffusion-Convolutional-Recurrent-Neural-Network-Data-Driven-Traffic-Forecasting"><a href="#1-Diffusion-Convolutional-Recurrent-Neural-Network-Data-Driven-Traffic-Forecasting" class="headerlink" title="1. Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting"></a>1. <a href="https://openreview.net/pdf?id=SJiHXGWAZ" target="_blank" rel="noopener">Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</a></h2><ul>
<li>発表者：貞光</li>
</ul>
<h3 id="手法-HOW"><a href="#手法-HOW" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>従来の時系列予測問題に対しては、ARIMAモデルやカルマンフィルタ等が用いられていますが、交通量のネットワーク自体がグラフの性質を持つため、グラフをより適切に扱える方法が要求されます。</p>
<p>加えて問題となるのがエッジ重みの非対称性です。ホリデーシーズン等を思い浮かべるとわかりやすいと思いますが、上り線と下り線でまったく交通量が異なりますよね。そのため、同じエッジであっても、エッジの方向によって重みが異なる非対称性を扱う必要が生じます。（図１参照）</p>
<p>近年では、グラフをニューラルネットワークで扱うための手法が多く提案されており、その手法は大きく2種類に大別できます。</p>
<p>1つ目はグラフスペクトル情報を用いた系で、Graph Convolutional Network (GCN) [Bruna+2014] が代表的です。[Yu+2017]は、GCNに対し系列データを扱えるように拡張しましたが、無向グラフしか扱えないという問題があり、交通量のようなエッジ重みの非対称性には対応できませんでした。</p>
<p>2つ目は明示的にグラフスペクトル情報を用いない手法で、 Diffusion CNN (DCNN) が代表的です。<br>著者らはDCNNを時系列変化に適応させたDCRNNを提案し、交通量予測を高精度に実現しました。</p>
<p>実験の結果、ARIMAやFull Connected LSTMに比べ大幅な予測精度改善を実現しています。（図2参照）<br>また、コードは以下のgithubで公開されていますので興味のある方は是非試してみてください。<br><a href="https://github.com/liyaguang/DCRNN" target="_blank" rel="noopener">https://github.com/liyaguang/DCRNN</a></p>
<h6 id="図１"><a href="#図１" class="headerlink" title="図１"></a>図１</h6><p><img src="/images/20180403/photo_20180403_04.png" class="img-small-size"></p>
<h6 id="図2"><a href="#図2" class="headerlink" title="図2"></a>図2</h6><p><img src="/images/20180403/photo_20180403_05.png" class="img-small-size"></p>
<h6 id="Bruno2014-Joan-Bruna-Wojciech-Zaremba-Arthur-Szlam-and-Yann-LeCun-Spectral-networks-and-locally-connected-networks-on-graphs-In-ICLR-2014"><a href="#Bruno2014-Joan-Bruna-Wojciech-Zaremba-Arthur-Szlam-and-Yann-LeCun-Spectral-networks-and-locally-connected-networks-on-graphs-In-ICLR-2014" class="headerlink" title="[Bruno2014] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014"></a>[Bruno2014] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014</h6><h6 id="Yu2017-Bing-Yu-Haoteng-Yin-and-Zhanxing-Zhu-Spatio-temporal-graph-convolutional-neural-network-A-deep-learning-framework-for-traffic-forecasting-arXiv-preprint-arXiv-1709-04875-2017a"><a href="#Yu2017-Bing-Yu-Haoteng-Yin-and-Zhanxing-Zhu-Spatio-temporal-graph-convolutional-neural-network-A-deep-learning-framework-for-traffic-forecasting-arXiv-preprint-arXiv-1709-04875-2017a" class="headerlink" title="[Yu2017] Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting. arXiv preprint arXiv:1709.04875, 2017a"></a>[Yu2017] Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting. arXiv preprint arXiv:1709.04875, 2017a</h6><h2 id="2-A-New-Method-of-Region-Embedding-for-Text-Classification"><a href="#2-A-New-Method-of-Region-Embedding-for-Text-Classification" class="headerlink" title="2. A New Method of Region Embedding for Text Classification"></a>2. <a href="https://openreview.net/pdf?id=BkSDMA36Z" target="_blank" rel="noopener">A New Method of Region Embedding for Text Classification</a></h2><ul>
<li>発表者：岩田</li>
</ul>
<h3 id="手法-HOW-1"><a href="#手法-HOW-1" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>小単位での語順表現を取得する際にCNN等の手法を使わずに、Local Context Unitを用いてembeddingを表現する手法です。文章に対して適用されるregion（範囲）を決めて、取得できた文字列を通常のembedding結果 (e) に対してアダマール積算をします。<br>（例：The food is not very good in this hotel.という文章があった場合、中心語:not、region:2とすると<br>food is not very good　が取得されます、これが今回でいうところの Local Context Unit (K) になります）</p>
<p>その積算結果 P を各コンテキスト毎に抽出し、Max pooling層を通して、最大値となるものを抽出します<br>（アーキテクチャとしては下図参照）その結果を用いて、課題の分類問題に当てはめていくというのが大枠になります。</p>
<p>精度としては、8つのデータセットに対して6つが最高精度を収めたという記載があり、Local Context Unitを用いた結果、より語彙の重要性を表現できた、係り受けを明確にできたため正しいsentimentを取得することができたという結果になっています。</p>
<p><img src="/images/20180403/22aed6d5-3534-d5c6-4b57-d282fd89e609.png" class="img-middle-size"></p>
<h2 id="3-Don’t-Decay-the-Learning-Rate-Increase-the-Batch-Size"><a href="#3-Don’t-Decay-the-Learning-Rate-Increase-the-Batch-Size" class="headerlink" title="3. Don’t Decay the Learning Rate, Increase the Batch Size "></a>3. <a href="https://openreview.net/pdf?id=B1Yy1BxCZ" target="_blank" rel="noopener">Don’t Decay the Learning Rate, Increase the Batch Size </a></h2><ul>
<li>発表者：小池</li>
</ul>
<h3 id="手法-HOW-2"><a href="#手法-HOW-2" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>分散学習の際に、精度を落とさずに学習を早く行う手法を示しています。以前からImageNetのような大量のデータを学習するには時間がかかることが言われていました。理論的にはバッチサイズを大きくすれば学習が早く終わりますが、バッチサイズが大きいと精度の面でなかなか学習がうまくいかないことが以前から問題となっていました。</p>
<p>一般的なDeep Learningの学習では、学習が進むにつれLearning rateを小さくしていきますよね。ただしこれの手法は時間が非常にかかります。</p>
<p>本論文では、学習段階で学習率を小さくする代わりにバッチサイズを途中で増していくと学習がうまくいくことを主張しています。これの何が良いかというと、バッチサイズを大きくできる＝早く学習ができるというメリットがあります。論文では、数値実験で学習率を減衰させた場合とバッチサイズを増やした場合を比較しており、両者ともに精度面では有意な差がないことを示しています。なお、数値実験ではWide ResNetでCIFAR10の学習を行っています。</p>
<p><img src="/images/20180403/photo_20180403_06.png"></p>
<h2 id="4-Learning-Deep-Mean-Field-Games-for-Modeling-Large-Population-Behavior"><a href="#4-Learning-Deep-Mean-Field-Games-for-Modeling-Large-Population-Behavior" class="headerlink" title="4. Learning Deep Mean Field Games for Modeling Large Population Behavior"></a>4. <a href="https://openreview.net/pdf?id=HktK4BeCZ" target="_blank" rel="noopener">Learning Deep Mean Field Games for Modeling Large Population Behavior</a></h2><ul>
<li>発表者：西森</li>
</ul>
<h3 id="手法-HOW-3"><a href="#手法-HOW-3" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>Collective Behavior（集団行動）中でも人口分布をモデル化し、その時間的発展を予測する手法です。</p>
<p>具体的な方法としては、Mean Field Game(以下MFG)のコスト関数（＝報酬関数）を逆強化学習によってデータから学習する手法を提案したものになります。</p>
<p>MFGはコスト関数をヒューリスティックに定める必要があります。しかし、現実の集合行動に対してコスト関数を設計するのは困難なため、これらの実験はトイプロブレム（現実には役に立たない範囲）に留まっていました。<br>しかし、MFGをマルコフ決定過程（以下、MDP）として表すことでベルマン最適方程式を用いたシングルエージェントの強化学習で解くことができたという論文になります。</p>
<p>実験として、Twitterの人気トピック予測を行い、既存手法であるVAR,RNNよりも高い精度を示すことを確認しています。（下図参照）。また、図(a)より、提案手法では「人気のトピックが、朝から夜にかけて人気になっていく」という特徴をとらえることができています。</p>
<p><img src="/images/20180403/91612b55-3f56-a567-5341-9c5502188575.png" class="img-middle-size"></p>
<h2 id="5-Towards-Neural-Phrase-based-Machine-Translation"><a href="#5-Towards-Neural-Phrase-based-Machine-Translation" class="headerlink" title="5. Towards Neural Phrase-based Machine Translation"></a>5. <a href="https://openreview.net/pdf?id=HktJec1RZ" target="_blank" rel="noopener">Towards Neural Phrase-based Machine Translation</a></h2><ul>
<li>発表者：平賀</li>
</ul>
<h3 id="手法-HOW-4"><a href="#手法-HOW-4" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>attentionを使わずに、ニューラルネットワークを用いた機械翻訳手法です。<br>Sleep-WAke Networks (以下SWAN) にsoft reorderingを適応させた Neural Phrase-based Machine Translation (以下NPMT) の提案になります。</p>
<p>NPMTの構造としては、下図のようになります。input (ドイツ語) をoutput (英語) に翻訳する際に、最初のポイントとしてはsoft reorderingの実施になります。</p>
<p>SWANはinputとoutputの繋がりが単調である必要があるため、単語ごとにマッピングを行いinとoutで一致度が高い順に、語を並び替える作業を実施します。その結果をBi-directional RNNに適用し、得られた結果の集合を次ステップであるSWANに適用し、最終的な翻訳結果の出力となります。</p>
<p><img src="/images/20180403/008b8c0c-6330-edfc-2806-7a94c3c50564.png" class="img-middle-size"></p>
<p>実験結果としては、既存手法としてattentionを使ったsequence-to-sequenceモデルと比較したときに<br>BLEU評価で精度向上が見られました。</p>
<h2 id="6-Learning-how-to-explain-neural-networks-PatternNet-and-PatternAttribution"><a href="#6-Learning-how-to-explain-neural-networks-PatternNet-and-PatternAttribution" class="headerlink" title="6. Learning how to explain neural networks: PatternNet and PatternAttribution"></a>6. <a href="https://openreview.net/pdf?id=Hkn7CBaTW" target="_blank" rel="noopener">Learning how to explain neural networks: PatternNet and PatternAttribution</a></h2><ul>
<li>発表者：明官</li>
</ul>
<h3 id="手法-HOW-5"><a href="#手法-HOW-5" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>ニューラルネットワークを用いた出力結果は「どうやってその結果を導いた」のかという根拠が不明確です。<br>（つまりブラックボックスである）本論文はその根拠を推定する新しい手法となります。</p>
<p>従来手法としては、重みとして表現される W を分析するということが考えられます。つまり、ネットワークとしてどういうところに特徴があるのかをポイントで探ったり、出力層から追いかけて各層の出力と重みから貢献度はどの程度なのかを識別したりと W に関する様々な手法があります。</p>
<p>しかし、重みを分析しても結局は出力に寄与する値を導けないという内容を本論文では主張しています。というのも、本質的に W はノイズを消すように更新されるものであるということからです。</p>
<p>本手法では、まず始めにノイズ削減に着目します。ノイズを最小にする操作に注目することでどのような操作がクリティカルなのかを見極めます。またその操作が線形的な場合に限らず活性化関数である ReLU のような場合でも、パターン分け（ReLUの場合 x &gt; 0 の場合とそれ以外の2パターン）を考慮することで対応が可能であることを示しています。</p>
<p>本提案手である Pattern Net and Pattern Attribution を用いた結果を下図になります。<br>従来手法に対して、人が理解できる表現抽出が可能になっていることが理解できます。</p>
<p><img src="/images/20180403/9da73f3b-069c-f600-95ea-c38aec4b67d4.png" class="img-middle-size"></p>
<h2 id="7-A-Neural-Representation-of-Sketch-Drawings"><a href="#7-A-Neural-Representation-of-Sketch-Drawings" class="headerlink" title="7. A Neural Representation of Sketch Drawings "></a>7. <a href="https://openreview.net/pdf?id=Hy6GHpkCW" target="_blank" rel="noopener">A Neural Representation of Sketch Drawings </a></h2><ul>
<li>発表者：石橋</li>
</ul>
<h3 id="手法-HOW-6"><a href="#手法-HOW-6" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>本研究は人が作成したスケッチをインプットとして、特定のクラスの手書き風スケッチをアウトプットする取り組みです。データセットとして、オンラインで提供されている、スケッチ認識サービスQuickDrawから得られた75クラス各7万枚のスケッチを利用しています。</p>
<p>研究の特徴としては、スケッチのデータが画像ではなく、(Δx,Δy,p1,p2,p3)で表される点のリストSで構成されている点が挙げられます。Δx,Δyは前の点からの移動、 p1,p2,p3はペンの状態(接触しているか、終わりか)を表すパラメータとなっており、画像というよりも書き順に近いデータを保持していることがわかります。</p>
<p>このような点のリストSを入力として、Sequence-to-Sequence Variational Autoencoderモデルを用いた学習を行い、Sketch-RNNを作成しています。（猫や豚のクラスに近づけて手書き風スケッチが出力されている）</p>
<p><img src="/images/20180403/8788edcb-0b74-334c-8b15-1e290c1e0b6c.png"></p>
<p>また、特定のクラスの手書き風スケッチをアウトプットするだけではなく、２クラスの補間を行うスケッチや、スケッチの加減算も可能となります。</p>
<p><img src="/images/20180403/8810c4d3-59f9-446d-1bde-310f01357ae6.png"></p>
<h2 id="おわりに"><a href="#おわりに" class="headerlink" title="おわりに"></a>おわりに</h2><p>今回も前回のNIPSと同様に、非常に学びのある勉強会となりました。</p>
<p>少し宣伝となりますが、フューチャーのSAIGチームでは、プロジェクトに所属しながらも自分の興味のある領域を常にインプットし、ビジネスにつなげる思いを持つメンバーが揃っています。</p>
<p>Futureでの働き方に興味がある方は是非<a href="https://www.future.co.jp/recruit/" target="_blank" rel="noopener">こちら</a>を参照してみてください。<br>エントリーお待ちしてます！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h1&gt;&lt;p&gt;みなさん、こんにちは。&lt;br&gt;Strategic AI Group（通称 : SAIG）の岩田です。&lt;/p&gt;
&lt;p&gt;今回は以
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>Future AI EXPO 開催！</title>
    <link href="https://future-architect.github.io/articles/20180301/"/>
    <id>https://future-architect.github.io/articles/20180301/</id>
    <published>2018-03-01T03:01:22.000Z</published>
    <updated>2019-01-07T03:00:37.166Z</updated>
    
    <content type="html"><![CDATA[<p>こんにちは、「元」フューチャーアーキテクトR&amp;Dチームの貞光です。</p>
<p>実は、我々R&amp;Dチームの所属が、<b>2018年3月1日</b> に少しだけ変わりましたので、今回はそのお知らせを。</p>
<p>フューチャーアーキテクト株式会社から、フューチャー持株会社の下へと移動し、<br>今後は”<b>Strategic AI Group</b>“ という名前で活動します。</p>
<p>さて、そんな枕詞を挟みつつ、今回の記事では2月末に開催した “<b>Future AI EXPO</b>“ を紹介します。<br>Future AI EXPO は、今回が初開催となるAI/IoT技術の社内向け展示イベントです。</p>
<p>弊社の１４階に広々としたリビングルームがあるのですが、この日はイベント開催のために人の波で埋め尽くされました。参加者数は社内だけにも関わらず300人以上！</p>
<p>本イベントをきっかけに、新たなコラボレーションが生まれたり、技術相談会が始まったり、<br>今後も継続開催希望！というポジティブな声をたくさんいただきました。</p>
<p>少しだけ展示の内容を紹介しますと、</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">・ノイズのある伝票画像における文字認識</div><div class="line">・数万点の小売アイテムに対する需要予測</div><div class="line">・システム開発効率化のための深層学習適用</div><div class="line">・日本の打ち上げた衛星「みちびき」と通信するモジュールの展示</div><div class="line">・自治体と協力したLoRaWanを用いたデータ収集　(株式会社TrexEdge)</div><div class="line">・画像解析を用いたプロ野球選手のトラッキング　(ライブリッツ株式会社)</div></pre></td></tr></table></figure>
<p>などなど盛り沢山。<br>今後はこれらの技術を、本blogや講演、学会活動等を通じ社外へも発信していく予定ですので、どうぞお楽しみに！</p>
<p>このように、特徴的なデータと人工知能関連技術とを組み合わせ、SAIGは新しい領域に日々チャレンジしていきます。</p>
<p>フューチャーは、データと人材のメルティング・ポット。渾沌、活気、未知、未来が集まっています。<br>フューチャー/SAIGにご興味を持たれた方はぜひお気軽にご連絡ください！</p>
<p>なお、３月に言語処理学会全国大会、６月に人工知能学会全国大会にスポンサーとしても参加する予定ですので、<br>もし参加される方がいらっしゃいましたら是非お話しましょう！</p>
<p><img src="/images/20180301/photo_20180301_01.jpeg"></p>
<p><img src="/images/20180301/photo_20180301_02.jpeg"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;こんにちは、「元」フューチャーアーキテクトR&amp;amp;Dチームの貞光です。&lt;/p&gt;
&lt;p&gt;実は、我々R&amp;amp;Dチームの所属が、&lt;b&gt;2018年3月1日&lt;/b&gt; に少しだけ変わりましたので、今回はそのお知らせを。&lt;/p&gt;
&lt;p&gt;フューチャーアーキテクト株式会社から、フュー
    
    </summary>
    
      <category term="Culture" scheme="https://future-architect.github.io/categories/Culture/"/>
    
    
      <category term="Conference" scheme="https://future-architect.github.io/tags/Conference/"/>
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>NIPS2017 LT報告</title>
    <link href="https://future-architect.github.io/articles/20180222/"/>
    <id>https://future-architect.github.io/articles/20180222/</id>
    <published>2018-02-22T01:18:28.000Z</published>
    <updated>2018-02-22T01:41:18.569Z</updated>
    
    <content type="html"><![CDATA[<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>みなさんこんにちは。<br>データサイエンスチームの小池です。<a href="https://future-architect.github.io/articles/20170526/">前回の記事</a>を書いてから半年ぶりの登場となります。<br>データサイエンスチームでは定期的に勉強会を開催しているのですが、特別イベントとして有志でNIPS 2017 LT大会を開催しました。今回はLT大会の様子と発表された論文を紹介いたします。</p>
<h1 id="NIPSとは"><a href="#NIPSとは" class="headerlink" title="NIPSとは"></a>NIPSとは</h1><p>ところでみなさんNIPSをご存知でしょうか。<br><a href="https://nips.cc/" target="_blank" rel="noopener">NIPS（Neural Information Processing Systems）</a>は機械学習（AI）系のトップカンファレンスで、<br>今最も熱い学会です。最近のAIブームもあり、提出論文数は年々増え続け2017年には3,240本もの論文が提出されました。（論文は<a href="https://papers.nips.cc/book/advances-in-neural-information-processing-systems-30-2017" target="_blank" rel="noopener">こちら</a>から読むことができます。）</p>
<p>そんな中、データサイエンスチームは今年の新人を誘ってNIPS2017LT大会を実施しました。<br>（ちなみにNIPS2017のaccepted paperの紹介がメインですが、NIPS以外に読みたい論文があれば、その紹介もOKとしました）</p>
<h1 id="NIPS2017-LT大会の様子"><a href="#NIPS2017-LT大会の様子" class="headerlink" title="NIPS2017 LT大会の様子"></a>NIPS2017 LT大会の様子</h1><p>LT大会はメンバーが多いため3回に分けて行われました。<br>数式ベースから紹介するものやデモを用いたもの、ITコンサルタントとしてそれを業務にどう活かしていくかという話題もあり、私個人にとっても非常に有益でした。</p>
<p><img src="/images/20180222/photo_20180222_01.jpeg"></p>
<p>大会中は熱い議論交わされも行われ 大いに盛り上がりました。</p>
<p><img src="/images/20180222/photo_20180222_02.jpeg"></p>
<h1 id="NIPS2017LT大会で紹介された論文の概要"><a href="#NIPS2017LT大会で紹介された論文の概要" class="headerlink" title="NIPS2017LT大会で紹介された論文の概要"></a>NIPS2017LT大会で紹介された論文の概要</h1><p>ここからは今大会で発表された論文をいくつか紹介いたします。<br>なお論文のまとめには新人の田中さんに手伝っていただきました。</p>
<h2 id="NIPS-2017論文"><a href="#NIPS-2017論文" class="headerlink" title="NIPS 2017論文"></a>NIPS 2017論文</h2><ul>
<li>Learned in Translation: Contextualized Word Vectors(Bryan McCann et. al. NIPS2017)  </li>
</ul>
<p>NLPのタスクではword vectorが用いられますが、contextの表現能力が十分ではありません。そこでcontextualized word vector(CoVe)を提案しています。CoVeは機械翻訳で学習されたseq2seq LSTM encoderから得られます。実験の結果、様々なNLPタスクでCoVeを用いることで良い結果が得られました。なお、余談になりますが先日ELMoという新しいembedding法が提案されました。CoVeとの比較もされており、より精度が高いとされています。</p>
<p><img src="/images/20180222/photo_20180222_03.png"></p>
<p><br><br><br></p>
<ul>
<li>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?(Alex Kendall et. al. NIPS 2017)</li>
</ul>
<p>画像処理における不確実性には偶発的不確実性(Aleatoric uncertainty)と認知的不確実性(Epistemic uncertainty)の2種類があります。偶発的不確実性とは、訓練データが不足するために生じる不確かさで、認知的不確実性とは訓練データがどれだけ多くあっても十分に説明できない情報に対する不確かさです。従来はどちらか一方しか考慮できませんでしたが、本論文では偶発的不確実性と認知的不確実性の両方を考慮可能な Bayesian deep learning frameworkの提案をしています。Bayesian deep learningでは、事後分布を推定することができるため不確実性の度合いを認識できます。 偶発的不確実性と認知的不確実性を従来の手法にBayesian neural networksを加えることによって考慮できるようにしています。画像判別タスクにこのNNを用いた結果、従来の手法より精度が向上しました。</p>
<p><img src="/images/20180222/photo_20180222_04.png"><br><br></p>
<ul>
<li>Best of Both Worlds: Transferring Knowledge from　Discriminative Learning to a Generative Visual Dialog Model(Jiasen Lu et al. NIPS2017)  </li>
</ul>
<p>画像に対しての質問をすると、対話の中で質問に受け答えするようなモデルの提案を行っています。 従来手法は対話生成にRNNを用いている一方、本手法ではGANを使用していることが特徴となります。ただし、ここで用いているGANは一般的なGANとは異なっています。Discriminator は、会話のリストを受け取り、受け答えとして自然なものは高いスコアをつけるように学習します。一方Generatorは、自分の生成した会話をDiscriminator に高いスコアを付けてもらえるような会話を生成しようと試みます。本論文では従来手法よりも高い精度で対話の応答ができたという結果が示されています。</p>
<p><img src="/images/20180222/photo_20180222_05.png"></p>
<p><br></p>
<ul>
<li>Stabilizing Training of Generative Adversarial Networks through Regularization (Kevin Roth et al. NIPS2017)</li>
</ul>
<p>GANの学習は設定パラメーターに対してセンシティブに過ぎることが多く、質が良いアウトプットを出すことは困難です。この課題を解決するためにDiscriminatorにノイズを入れる方法が知られています。本論文ではノイズ追加などを数式的に分析し、偽データに対するDiscriminatorの勾配を正規化するよう手法を提案しています。これは余談になるのですが、Githubに<a href="https://github.com/rothk/Stabilizing_GANs" target="_blank" rel="noopener">コード</a>も公開されていたため、モデルに実装してみたところ、確かにDiscriminatorとGeneratorの学習がうまい具合に進みました。GANに興味がある方はぜひトライしてみてください。</p>
<p><br></p>
<ul>
<li>Poincaré Embeddings for Learning Hierarchical Representations(Maximilian Nickel et al. NIPS2017)</li>
</ul>
<p>一般的な機械学習における表現獲得にはユークリッド空間が利用されますが、本論文では表現獲得にPoincaré空間の利用を試みています。Poincaré空間は双曲空間の一種であり、距離の定義が式1で表される空間のことです。Poincaré空間を用いる利点として、ユークリッド空間と比較して、超球の外にいけばいくほど距離が密になるので効率良く空間を利用できることがあげられます。Poincaré空間にembeddingすることで、ユークリッド空間で200次元が必要だったタスクがたったの5次元で同精度を得ることができました。<br><img src="/images/20180222/photo_20180222_06.png" class="img-small-size"></p>
<p><img src="/images/20180222/photo_20180222_07.png"></p>
<p><br></p>
<h2 id="その他論文"><a href="#その他論文" class="headerlink" title="その他論文"></a>その他論文</h2><ul>
<li>A Comprehensive Survey on Cross-modal Retrieval(Kaiye Wang et al. IEEE2016)</li>
</ul>
<p>画像から文字を検索したり、文字から動画を検索したりするcross modal検索の既存手法を整理した論文です。<br>cross modal検索の手法はいくつか提案されていますが、各手法の得手・不得手に加え、各特徴をまとめています。 例えば、画像および文書をベクトル化する手法には、Binaryにする手法と実数値を用いる手法がありますが、前者は検索が早い一方、後者は精度が良いという傾向が示されています。各手法に対し、オープンタスクでの精度の比較も行っています。<br><br></p>
<ul>
<li>Comicolorization: Semi-Automatic Manga Colorization(Chie Furusawa et al. SIGGRAPH Asia 2017)</li>
</ul>
<p>白黒漫画１タイトル＋参照画像と呼ばれるキャラクターのカラー画像を入力とし、白黒漫画をディープラーニングで自動着色する論文です。<br>白黒写真を自動彩色するCNN（<a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf）から以下の3点を変更することによって、キャラクターを分類した上で、キャラクター毎に鮮やかに自動着色しています。" target="_blank" rel="noopener">http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf）から以下の3点を変更することによって、キャラクターを分類した上で、キャラクター毎に鮮やかに自動着色しています。</a></p>
<ul>
<li>キャラクターを分類しやすくするため 、クラス分類を行うネットワークのラベルをキャラクターのラベルに変更</li>
<li>色彩特徴量の効果を促進するため、CNNでは使用されていなかったDiscriminatorを追加</li>
<li>インタラクティブな彩色を可能にするため、学習時に参照画像内のキャラクターに使われている色の有無を示すベクトルを抽出し追加<br><img src="/images/20180222/photo_20180222_08.png"></li>
</ul>
<h1 id="終わりに"><a href="#終わりに" class="headerlink" title="終わりに"></a>終わりに</h1><p>今回初の試みであるNIPS2017LT大会は、最新の機械学習の動向をメンバー間で共有することができました。<br>機械学習の最先端である手法を学び、それらを業務にどう活かしていくかという議論もでき非常に有益でした。<br>フューチャーアーキテクトのデータサイエンスチームでは、最先端の機械学習を用いて顧客の課題を解決できるエンジニアを募集しています。<br>興味がある方は<a href="https://www.future.co.jp/recruit/" target="_blank" rel="noopener">こちら</a>からエントリーをお願いします。より良い未来を一緒に作っていきましょう！</p>
<h2 id="予告"><a href="#予告" class="headerlink" title="予告"></a>予告</h2><p>次回はICLR2018のLT大会を行います。お楽しみに。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h1&gt;&lt;p&gt;みなさんこんにちは。&lt;br&gt;データサイエンスチームの小池です。&lt;a href=&quot;https://future-archite
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>エンプラ&amp;オンプレでもAnsible導入成功したのでユーザー会で発表してきた</title>
    <link href="https://future-architect.github.io/articles/20180209/"/>
    <id>https://future-architect.github.io/articles/20180209/</id>
    <published>2018-02-09T02:12:14.000Z</published>
    <updated>2018-02-09T02:40:19.532Z</updated>
    
    <content type="html"><![CDATA[<p>エンプラ&amp;オンプレでもAnsible導入成功したのでユーザー会で発表してきた<br>こんにちは。齋場です。<br>少し前ですが、弊社でAnsibleを導入した事例をAnsibleユーザ会で発表してきました。(どちらも5minのLTですが) Ansibleを導入したおかげか、PJは問題なくリリースすることができ、やっと落ち着いたのでブログを書きたいと思います。</p>
<p><strong>キラキラした最新技術</strong>をエンプラ&amp;オンプレPJに<strong>泥臭ーく</strong>導入した話です。</p>
<h1 id="発表内容"><a href="#発表内容" class="headerlink" title="発表内容"></a>発表内容</h1><ul>
<li>2017/8/28(月) 3社共同企画 Ansible 夏祭り<br><iframe src="//www.slideshare.net/slideshow/embed_code/key/1HlAM9KhjgeejI" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/taroshun/3ansible" title="新卒3年目のぼくが、でぶおぷす???なインフラおじさん方にAnsibleを導入してみた" target="_blank">新卒3年目のぼくが、でぶおぷす???なインフラおじさん方にAnsibleを導入してみた</a> </strong> from <strong><a href="https://www.slideshare.net/taroshun" target="_blank">Shuntaro Saiba</a></strong> </div></li>
</ul>
<ul>
<li>2017/12/21(木) Ansible Night in Tokyo<br><iframe src="//www.slideshare.net/slideshow/embed_code/key/BuEj51P21HL3yF" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/taroshun/3ojisanansible" title="新卒3年目のぼくが、でぶおぷす???なエンプラ金融PJにAnsibleを導入してみた" target="_blank">新卒3年目のぼくが、でぶおぷす???なエンプラ金融PJにAnsibleを導入してみた</a> </strong> from <strong><a href="https://www.slideshare.net/taroshun" target="_blank">Shuntaro Saiba</a></strong> </div></li>
</ul>
<p>どちらの内容も多くの皆様に共感いただくことができた気がします。。!!特にエンプラPJでのサーバ管理のツラミについて</p>
<h1 id="そもそもAnsibleとは"><a href="#そもそもAnsibleとは" class="headerlink" title="そもそもAnsibleとは"></a>そもそもAnsibleとは</h1><p>Ansible？な方に対して、軽く説明させていただきます。Ansibleを説明するにはまず、<strong>Infrastracture as Code</strong>を説明する必要があります。</p>
<h2 id="Infrastructure-as-Codeとは"><a href="#Infrastructure-as-Codeとは" class="headerlink" title="Infrastructure as Codeとは"></a>Infrastructure as Codeとは</h2><ul>
<li>Infrastructure as Codeとは「自動実行可能なコードの形でインフラの状態を記述し、インフラ構築を自動化するプロセス」です。</li>
</ul>
<p><img src="/images/20180209/photo_20180209_03.png"></p>
<ul>
<li>インフラ構築を3つに分類すると以下のようになり、それぞれの分類に対応する自動化ツールがあります。</li>
</ul>
<p><img src="/images/20180209/photo_20180209_04.png"></p>
<h2 id="Ansibleとは"><a href="#Ansibleとは" class="headerlink" title="Ansibleとは"></a>Ansibleとは</h2><ul>
<li>Ansibleとはオープンソースのサーバ構成管理ツールです。</li>
<li>上図のように主にOS/ミドルウェア設定に主に用いられます。 (実際はサーバ管理だけではなく多くの機能があります)</li>
</ul>
<p><img src="/images/20180209/photo_20180209_05.png"></p>
<ul>
<li>よくAnsibleと比較されるのが以下の2つです。<ul>
<li>Ansibleは<strong>エージェントレス</strong>ということもあり、比較的に簡単に導入することができます</li>
<li>あとは定義ファイルの形式がyamlかjsonかの違いというのも大きなポイントです</li>
</ul>
</li>
</ul>
<p><img src="/images/20180209/photo_20180209_06.png"></p>
<ul>
<li>Ansibleの使い方は非常にシンプルです。基本の構成要素は以下になります<ul>
<li><strong>Invetroy</strong>ファイル: (管理対象のサーバを定義するファイル)</li>
<li><strong>Playbook</strong>ファイル: (サーバのあるべき状態を定義するファイル)</li>
</ul>
</li>
</ul>
<p><img src="/images/20180209/photo_20180209_07.png"></p>
<p>Ansibleでサーバのあるべき状態をコードとして定義して、Playbookを実行するとサーバは定義した状態に収束します。(冪統性がある)<br>なんとも便利そうです。これなら数百台あるサーバであっても<strong>構築の自動化</strong>と<strong>高品質な構成管理</strong>ができそうですね。 以下のようにInfrastructure as Codeを導入すれば今までの課題が解決することは明らかだと思います。</p>
<p><img src="/images/20180209/photo_20180209_08.png"></p>
<h1 id="エンプラ-amp-オンプレへの導入へのチャレンジ"><a href="#エンプラ-amp-オンプレへの導入へのチャレンジ" class="headerlink" title="エンプラ&amp;オンプレへの導入へのチャレンジ!!"></a>エンプラ&amp;オンプレへの導入へのチャレンジ!!</h1><p>ここからが本題です。</p>
<h2 id="導入したいけど、、"><a href="#導入したいけど、、" class="headerlink" title="導入したいけど、、"></a>導入したいけど、、</h2><p>Infrastructure as Codeでインフラ構築の自動化！高品質な構成管理！ ができるとは言えど、<strong>実際導入するのは色々と障壁がありますよね。</strong><br>以下のような声もPJから聞こえてきそうです。(エンタープライズ&amp;オンプレだと特に)</p>
<ul>
<li><strong>便利だけど、使うのは(ソースコード書くのは)難しんでしょ？？</strong></li>
<li><strong>みんながAnsible使えるようにならないといけないんでしょ？？</strong></li>
<li><strong>今までの運用手順から変わってしまうんでしょ？？</strong></li>
<li><strong>セキュリティ面とか大丈夫？？</strong></li>
</ul>
<p>先述しましたが、Ansibleはエージェントレス型であったため障壁はかなり下がりました。 ただ、それだけでは導入はうまくいきませんでした。</p>
<h2 id="エンプラ-amp-オンプレ領域って特に難しい、、"><a href="#エンプラ-amp-オンプレ領域って特に難しい、、" class="headerlink" title="エンプラ&amp;オンプレ領域って特に難しい、、??"></a>エンプラ&amp;オンプレ領域って特に難しい、、??</h2><p>個人的に、<strong>エンプラ&amp;オンプレの領域って、Infrastructure as Codeの導入が一番困難だと思っています。</strong></p>
<p><img src="/images/20180209/photo_20180209_09.png"></p>
<p>まず、<strong>オンプレ</strong> これが相性が悪い。その大きな要因は<strong>Mutable(可変)</strong>であるためです。<br>クラウドであればサーバを都度破棄して、新規に構築する <strong>“Immutable”</strong>であるので、Infrastructure as Code本来の使い方に準じて利用できると思うのですが、オンプレだと作り直しができない”Mutable”であり、手入れをしながら長く付き合っていくことになります。その点を考慮してInfrastructure as Codeを利用しなければいけません。</p>
<p>(※ オンプレでも仮想マシンで、かつ”Immutable”な使い方をすれば話は別です)<br>(※ 逆に言えばクラウドでもMutableな使い方をしていると相性悪いと思います)</p>
<p><img src="/images/20180209/photo_20180209_10.png"></p>
<p>そして、<strong>エンプラ</strong> これも相性が悪い。<strong>“制約”</strong>とか<strong>“いままでの文化”</strong>が導入の大きな障壁になります。簡単に言うなら大きくて動きずらい。</p>
<p>日本ではあまり見かけないのですが、エンプラ&amp;ウェブ系って以下のように比喩されることが多いそうです。<strong>“ユニコーン”と”馬”</strong>って、、なんか納得感。。</p>
<blockquote>
<p>Facebook、Twitter、EtsyなどのWeb企業がDevOpsについて語るとき、彼らはユニコーンのような魔法の世界に住んでいます。クラウドベースであり、1つのサービスのみを提供することに重点を置いている企業は、共通のプラットフォームを利用して、誰もが同じページで作業できるようにします。<br>フォード、ゼネラルエレクトリック、ゼネラル・ダイナミクスなどの企業には、多くのチーム、数千の製品とサービス、そして豊富な技術を提供するさまざまなテクノロジーがあります。そのため、チーム全体を共通のページに置くことが困難になり、大きなDevOps転換を実行するのはより大きな課題です。</p>
</blockquote>
<p>出典: <a href="https://www.cloudtp.com/doppler/where-is-the-ea-in-devops/" target="_blank" rel="noopener">Where is the “EA” in DevOps?</a></p>
<p><img src="/images/20180209/photo_20180209_11.png"></p>
<h1 id="どのようにして導入したか"><a href="#どのようにして導入したか" class="headerlink" title="どのようにして導入したか"></a>どのようにして導入したか</h1><p>今回、PJ(<strong>エンプラ</strong>&amp;<strong>オンプレ</strong>)にInfrastructure as Code導入成功したわけですが、やはり一筋縄にはいかないわけで。。ここからは導入の過程で工夫した点をご紹介します。</p>
<h2 id="目指したサーバ管理方法"><a href="#目指したサーバ管理方法" class="headerlink" title="目指したサーバ管理方法"></a>目指したサーバ管理方法</h2><p>まず、Infrastructure as Codeを導入することでどのようなサーバ管理方法を実現するかのビジョンを定めました。</p>
<p><strong>1. インフラへの変更はソースコード経由で行い、手作業での変更は原則禁止する</strong><br><strong>2. 実環境とのソースコードの定義は自動突合を実施し、不整合を検知可能にする</strong></p>
<p>オンプレ(Mutable)だからこその考慮点がこれで、変更履歴を付けずに誰かが手でサーバを葬っちゃうともう<strong>Infrastructure as Codeの意味ってなくなっちゃう</strong>んですよね。クラウド(Immutable)だと、常に破棄→新規作成の繰り返しなのでこの点は考慮しなくてもよいですから。<br>私自身も含め<strong>「ちょっとの変更だし、あとで直せばよいから手で設定いじっちゃお。変更履歴は適当なメモで」</strong>って考えでサーバに変更を加える輩は絶対に存在します。だからこそ、そこを拾えるように実環境とソースコードの自動突合の部分にもこだわりました。</p>
<p><img src="/images/20180209/photo_20180209_12.png"></p>
<h2 id="作り上げたフレームワーク"><a href="#作り上げたフレームワーク" class="headerlink" title="作り上げたフレームワーク"></a>作り上げたフレームワーク</h2><ul>
<li>こんな感じのフレームワーク(と言っては大げさかもしれませんが)を作りました。</li>
<li>Ansibleの使い方だけではなく、PJに導入するために以下も考えました。<ul>
<li>ビジョン</li>
<li>手順書</li>
<li>ワークフロー</li>
</ul>
</li>
</ul>
<p><img src="/images/20180209/photo_20180209_13.png"></p>
<ul>
<li>構成は以下のようなイメージです。</li>
<li>JenkinsとGitlabを駆使して、継続的にAnsible実行&amp;フィードバックができるような仕組みを心掛けました。</li>
</ul>
<p><img src="/images/20180209/photo_20180209_14.png"></p>
<p>後述する”特に工夫したこと”で詳細記載します。</p>
<h2 id="特に工夫したこと"><a href="#特に工夫したこと" class="headerlink" title="特に工夫したこと"></a>特に工夫したこと</h2><p>いろいろな面で工夫しました。ほとんどは発表資料に記載しているので、ここでは特に工夫をしたことを紹介します。大きく三つあります。</p>
<ul>
<li><strong>1. ただ新しい考え方を押し付けるのはダメ。今までの考え方に歩み寄る</strong></li>
<li><strong>2. メンバーみんなにキャッチアップを求められない。みんなが利用できる仕組みを作る</strong></li>
<li><strong>3. あきらめない</strong></li>
</ul>
<h3 id="1-考え方を押し付けるのはダメ。今までの考え方に歩み寄る"><a href="#1-考え方を押し付けるのはダメ。今までの考え方に歩み寄る" class="headerlink" title="1. 考え方を押し付けるのはダメ。今までの考え方に歩み寄る"></a><strong>1. 考え方を押し付けるのはダメ。今までの考え方に歩み寄る</strong></h3><h4 id="Ansibleで全部やろうとしない"><a href="#Ansibleで全部やろうとしない" class="headerlink" title="Ansibleで全部やろうとしない"></a>Ansibleで全部やろうとしない</h4><ul>
<li>インフラ構築すべてをAnsibleで実装。は理想ですが、ベンダが構築する部分があったりと、すべてをソースコード化するのは受け入れ側も導入側にもパワーが足りなかったので以下の用途で使用を留めました。しかし、これだけでも効果はかなり大きかったです。<ul>
<li>ファイル配布</li>
<li>ディレクトリ作成</li>
<li>ユーザグループ作成</li>
<li>パッケージインストール</li>
</ul>
</li>
</ul>
<h4 id="Ansibleのベストプラクティスが私たちにベストとは限らない"><a href="#Ansibleのベストプラクティスが私たちにベストとは限らない" class="headerlink" title="Ansibleのベストプラクティスが私たちにベストとは限らない"></a>Ansibleのベストプラクティスが私たちにベストとは限らない</h4><ul>
<li>Ansibleのベストプラクティスに<strong>あえて従わなかった</strong>ことで今までの考え方に歩み寄りできるようにしました。</li>
<li>私たちが使うAnsibleの構成は下図のように、<strong>“role”=”サーバ種”</strong>となっています。(本来ならば”role”=”mysql”とか)</li>
<li>また、Ansibleでのファイル管理に関しては、<strong>“動的ファイル(Jinja Templating)”は一切使っていません。</strong></li>
<li>インフラをソースコード化するというプロセスに<strong>拒否反応を抱かせないよう</strong>、今までの考え方に親和するようにしました。</li>
</ul>
<p>※ このせいで、taskファイルや静的ファイルが二重管理になってしまいますが、それは後述のtaskファイル自動生成で補うことができました。</p>
<p><img src="/images/20180209/photo_20180209_15.png"></p>
<h3 id="2-メンバーみんなにキャッチアップを求められない。みんなが利用できる仕組みを作る"><a href="#2-メンバーみんなにキャッチアップを求められない。みんなが利用できる仕組みを作る" class="headerlink" title="2. メンバーみんなにキャッチアップを求められない。みんなが利用できる仕組みを作る"></a><strong>2. メンバーみんなにキャッチアップを求められない。みんなが利用できる仕組みを作る</strong></h3><h4 id="Ansibleファイルが書けない-それならExcelから自動生成しよう"><a href="#Ansibleファイルが書けない-それならExcelから自動生成しよう" class="headerlink" title="Ansibleファイルが書けない? それならExcelから自動生成しよう"></a>Ansibleファイルが書けない? それならExcelから自動生成しよう</h4><ul>
<li>導入当初はAnsibleソースを書けるのは私だけの状態。</li>
<li>他のインフラ構築メンバにも使ってもらわないと、全然回らないと感じてExcelに書いた定義から<strong>Ansibleファイルを自動生成</strong>するような仕組みを作りました。</li>
<li>AnsibleがわからなくてもExcelという<strong>慣れ親しんだインターフェース</strong>を設けたことで、Ansibleでの構築はかなり軌道に乗りました。<br>※ Excel1行 → YAML形式 と変換するだけだったので簡単なPythonスクリプトで済んだのも助かりました。</li>
</ul>
<h4 id="Gitが使えない-それならJenkinsにコミットしてもらおう"><a href="#Gitが使えない-それならJenkinsにコミットしてもらおう" class="headerlink" title="Gitが使えない? それならJenkinsにコミットしてもらおう"></a>Gitが使えない? それならJenkinsにコミットしてもらおう</h4><ul>
<li>PJは、Gitを使える人は数人いるかどうか、という状態でした。</li>
<li>ただでさえAnsibleという新しいツールを導入するのに、Gitも、、は厳しそうだったのでGit操作はJenkinsに任せることにしました。</li>
<li>歩み寄って、AnsibleファイルもSVNで管理、、、も考えましたがここだけは譲れませんでした。</li>
</ul>
<p><img src="/images/20180209/photo_20180209_16.png"></p>
<h3 id="3-あきらめない"><a href="#3-あきらめない" class="headerlink" title="3. あきらめない"></a><strong>3. あきらめない</strong></h3><h4 id="絶対にサーバを手で葬らせない"><a href="#絶対にサーバを手で葬らせない" class="headerlink" title="絶対にサーバを手で葬らせない"></a>絶対にサーバを手で葬らせない</h4><p>あとは執念です。。</p>
<p><strong>“環境がカオスになったのを直していく苦労 &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Ansibleを導入する苦労”</strong><br>であると私は確信していたので最後まであきらめませんでした。</p>
<p><strong>「このディレクトリ、時間ないので手で作っちゃってもいいですか？」</strong><br><strong>「新しい設定ファイル必要になったので、手で作っちゃっていいですか？」</strong></p>
<p>他のインフラ構築メンバからそんな声がたくさん聞こえてきます。たとえ作業をし終わった後でもAnsibleのソースコード化をしないと、その構築範囲は<strong>構成管理ができなくなります。</strong>それでは意味がありません。なので、Ansibleを使える人だけで人が行った手作業をソースコード化を継続するのが非常に大変でした。。しかし、あきらめてはダメです。(途中からExcelソース自動化を導入したのでだいぶ楽にはなりましたが、軌道にのるまでは辛かった)<br>※Ansibleは冪統性があるので、いったん手で作業したところをソースコード化しても問題ない</p>
<h4 id="分かってもらえるまで説明する"><a href="#分かってもらえるまで説明する" class="headerlink" title="分かってもらえるまで説明する"></a>分かってもらえるまで説明する</h4><p>インフラチームにはすんなり受け入れられたのですが、チーム全体にInfrastructure as Codeという全く新しい考え方をわかってもらうのもなかなか時間がかかりました。<br>しかし、<strong>開発だけではなく運用でも</strong>使っていってもらうためには保守・運用チーム含めチーム全体に考え方を理解してもらう必要があります。これもあきらめてはダメです。そのためにチーム各所に何度も何度もプレゼンを行いました。<br>Ansibleは簡単に使いやすいとは言え、いままでのサーバ管理の考え方とはガラッと変わってしまうため、最初のほうは？？？？？が連発ですが、継続的に普及活動を行ったので腹落ちして理解してもらえたと思います。資料にも記載しましたが、上の人から攻めていくのがおすすめです。あとは、実演して見せることも。</p>
<ul>
<li>導入のために作った説明資料の数々</li>
</ul>
<p><img src="/images/20180209/photo_20180209_17.png"></p>
<ul>
<li>併せてDevOps勉強会も実施しました</li>
</ul>
<p><img src="/images/20180209/photo_20180209_18.png"></p>
<h1 id="導入の成果"><a href="#導入の成果" class="headerlink" title="導入の成果"></a>導入の成果</h1><p>なによりPJが無事にリリースできたのが大きな成果でしたが、以下も功績が非常に大きかったです。</p>
<h2 id="作業自動化"><a href="#作業自動化" class="headerlink" title="作業自動化"></a>作業自動化</h2><p>今までサーバへの変更は以下のような<strong>“ネ伸Excel”</strong>で手順を書いて、レビューしてもらって、<strong>手作業</strong>で実施していたものが、Ansibleによって<strong>自動化</strong>することができました。これにより、工数も大きく削減できましたし、ヒューマンエラーもなくなりました。なにより、手順書作るのも、作業実施するのもとにかく<strong>“楽”</strong>になりましたね。</p>
<ul>
<li>今までの手順書と新しい手順書</li>
</ul>
<p><img src="/images/20180209/photo_20180209_19.png"><br><img src="/images/20180209/photo_20180209_20.png"></p>
<h2 id="構成管理"><a href="#構成管理" class="headerlink" title="構成管理"></a>構成管理</h2><p>今までは<strong>SVNのExcel上</strong>でしかできていなかったMWやOSの設定ファイル、またディレクトリの権限の構成管理は、<strong>ソースコードとして管理</strong>することができるようになりました。(ファイル、ディレクトリのパーミッション含め)<br>アプリの世界では当たり前ですが、コミットメッセージに<strong>変更の経緯</strong>も残せますし、Gitのマージリクエストで<strong>承認依頼</strong>も出せます。当たり前のことがやっとできるようになったって感じです。<br>また、Git上にファイルをすべて管理しているので各サーバに入って設定値確認、なんて効率の悪いことは一切必要ありません。</p>
<ul>
<li>Postfixの設定を変更した際の例</li>
</ul>
<p><img src="/images/20180209/photo_20180209_21.png"></p>
<h1 id="補足：導入までの時間"><a href="#補足：導入までの時間" class="headerlink" title="補足：導入までの時間"></a>補足：導入までの時間</h1><p><img src="/images/20180209/photo_20180209_22.png"></p>
<p>導入までの歩みをまとめてみました。開発(Dev)は一人でガンガン作りこんでいってしまえばよいだけなのでそんなに大変ではありませんでしたが、運用(Ops)でも使えるように、いろいろな関係者を巻き込んでいくのはなかなか骨が折れました。ただ、保守・運用チームが積極的に協力してくれたので、なんとか運用でも使えるようになるまでこぎつけることができました。感謝です。</p>
<h1 id="最後に"><a href="#最後に" class="headerlink" title="最後に"></a>最後に</h1><p>Ansibleのようなツールはどんどん新しいものが出てきて、日に日に便利になっていきます。<br>私たちのPJは”ユニコーン”ではないので<strong>工夫と時間</strong>がかなり必要ですが、それでもその恩恵を受けることはできます。<br>実際に導入までこぎつけて、<strong>“先進的なツール”と”今までのPJのやり方や文化”、これをどう結びつけるかが重要だと感じました。</strong></p>
<p>また、ここまでの仕組みを整えられたのもPJメンバの協力があったからだと感じています。<br>この仕組み作りの重要性を理解し「どんどんやっちゃっていいよー」とGOをくれたリーダに感謝です。<br>また、一緒にワークフローを検討し運用での利用の仕組みを整えてくれた保守・運用チームにも感謝です。<br>勉強会の交流会や、Twitterでは「Ansible広めたいけど、チームの誰も賛同してくれない」と言っている人はたくさんいました。(うちっていい会社ですね！)</p>
<p>と言っても、まだまだこの仕組みは使い始められて間もないのでこれからさらに改良を重ねていきたいと考えています。</p>
<p>(注意)<br>資料に”おじさん”という表現がありますが、プレゼンでのウケを狙ったものです。本当は”おじさん”とか思っていません。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;エンプラ&amp;amp;オンプレでもAnsible導入成功したのでユーザー会で発表してきた&lt;br&gt;こんにちは。齋場です。&lt;br&gt;少し前ですが、弊社でAnsibleを導入した事例をAnsibleユーザ会で発表してきました。(どちらも5minのLTですが) Ansibleを導入したお
    
    </summary>
    
      <category term="Infrastructure" scheme="https://future-architect.github.io/categories/Infrastructure/"/>
    
    
      <category term="Infrastructure" scheme="https://future-architect.github.io/tags/Infrastructure/"/>
    
      <category term="Ansible" scheme="https://future-architect.github.io/tags/Ansible/"/>
    
  </entry>
  
  <entry>
    <title>第1回Future開発合宿</title>
    <link href="https://future-architect.github.io/articles/20171217/"/>
    <id>https://future-architect.github.io/articles/20171217/</id>
    <published>2017-12-17T05:28:42.000Z</published>
    <updated>2017-12-17T05:40:06.696Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://adventar.org/calendars/2449" target="_blank" rel="noopener">フューチャーアーキテクト裏アドベントカレンダー2017</a>の16日目です。</p>
<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>開発合宿とは…<br>新しいナニカを生み出すための儀式である…</p>
<p>みなさまこんにちは。早2年目になりました谷村です。</p>
<p>2017年はWeb業界で開発合宿が何かと話題になる年でしたね。流れに乗ってFutureでも合宿を開催してみたので、今後合宿を開催してみたい方や、合宿行ってみたいけど迷ってる方向けにレポートします。</p>
<p>なお、Futureでいう開発合宿は、各々がプライベートで開発したいものを開発する有志活動です。</p>
<h1 id="開催のきっかけ"><a href="#開催のきっかけ" class="headerlink" title="開催のきっかけ"></a>開催のきっかけ</h1><p>ある日、<a href="https://github.com/future-architect/vuls" target="_blank" rel="noopener">Vuls</a>でおなじみの神戸さんが、<a href="https://go-beginners.connpass.com/event/47481/" target="_blank" rel="noopener">Golangの合宿</a>があるらしいぞと持ちかけてきまして、<a href="https://dotstud.io/blog/go-beginners-camp-report/" target="_blank" rel="noopener">行ってみたらめっちゃ楽しかった</a> です！</p>
<p>Futureでもやろうよ、と言ってみたら、</p>
<blockquote>
<p>いいじゃん、感動した。<br>谷村くん、スゴイわ。<br>こんな天才がいたとは素晴らしいわ。</p>
</blockquote>
<p>とおだてられたので開催してみました。</p>
<h1 id="開催準備"><a href="#開催準備" class="headerlink" title="開催準備"></a>開催準備</h1><p>簡単3stepで、1人で幹事が出来ました。<br>基本的にGo合宿を <del>パク</del> リスペクトしました。</p>
<ol>
<li><a href="http://www.dozenryokan.com" target="_blank" rel="noopener">土善旅館さん</a>に「Go合宿と同じように開催したいんでよろしく。」と伝える</li>
<li>社内で声かけてまわって、人を集める</li>
<li>Go合宿のしおりを <del>パク</del> リスペクトして作成した<a href="https://gist.github.com/tng527/70a3af19aad64c6dfa0f4214868a4a7e" target="_blank" rel="noopener">しおり</a>を参加者に投げる</li>
</ol>
<p>※支払いは、「LINE Payでよろしく！」と自分のIDを晒しておくと、勝手にお金が送られてきます。マジ神</p>
<h1 id="当日まとめ"><a href="#当日まとめ" class="headerlink" title="当日まとめ"></a>当日まとめ</h1><p>土善旅館さんが最高過ぎてブログが長くなるので、先にまとめておきます。</p>
<ul>
<li>開発部屋が広い！20m四方くらい？</li>
<li>NW強い！まさかの冗長化構成！</li>
<li>外部ディスプレイ無料！開発捗る！！</li>
<li>スクリーンでっかい！100インチくらい？深夜からアニメの上映会もやりました。</li>
<li>夕食うまい！しゃぶしゃぶとか寿司とかめっちゃ豪華！</li>
<li>安い安いアンド安い！ 会費13,000円で1泊3食デザート付、お酒飲み放題！</li>
</ul>
<p><img src="/images/20171217/photo_20171217_01.jpeg"></p>
<h1 id="当日の様子詳細"><a href="#当日の様子詳細" class="headerlink" title="当日の様子詳細"></a>当日の様子詳細</h1><p>それでは順を追って合宿の様子を写真メインでお送りします。<br>土善旅館さんは千葉の奥地にあるので、みんなで電車で向かいます。3時間くらい。</p>
<p><img src="/images/20171217/photo_20171217_02.jpeg"><br>移動中から開発合宿は始まっているのだ…！</p>
<p><img src="/images/20171217/photo_20171217_03.jpeg"></p>
<p>最寄りの笹川駅に着いたら、宿に向かう前にご飯を食べます。<br>青柳亭のしじみ丼がこの辺の名産らしい。</p>
<p><img src="/images/20171217/photo_20171217_04.jpeg"><br>ようやく宿に到着です。歴史ある感じですが、清潔で気持ち良いです。</p>
<p><img src="/images/20171217/photo_20171217_05.jpeg"><br>到着したらまずは乾杯、これ基本。</p>
<p><img src="/images/20171217/photo_20171217_06.jpeg"><br>広い机と外部ディスプレイ、電源タップ、それと<a href="https://yogibo.jp" target="_blank" rel="noopener">人を駄目にする枕高級版</a>が用意されています。</p>
<p><img src="/images/20171217/photo_20171217_07.jpeg"><br>物理的な開発環境を整えたら、各々のスタイルで開発を始めます。</p>
<p><img src="/images/20171217/photo_20171217_08.jpeg"><br>お酒片手に。</p>
<p><img src="/images/20171217/photo_20171217_09.jpeg"><br>開発に飽きた頃に、講義とかやってみたり。<br>写真は競技プログラミング部長・塚本さんのアルゴリズム講座です。</p>
<p><img src="/images/20171217/photo_20171217_10.jpeg"><br>今回は各々が好きなものを開発するスタイルの合宿ですが、<br>一人じゃないから質問や相談も可能！！</p>
<p><img src="/images/20171217/photo_20171217_11.jpeg"><br>ハマったタイミングで看板猫が癒やしを運んできます。</p>
<p><img src="/images/20171217/photo_20171217_12.jpeg"><br>開発しているとご飯の時間がやってまいりました。<br>別のお座敷が用意されるという豪華っぷり。</p>
<p><img src="/images/20171217/photo_20171217_13.jpeg"><br>当然ご飯も豪華です。</p>
<p><img src="/images/20171217/photo_20171217_14.jpeg"><br>食後は卓球で腹ごなしです。なんだ、ただの最高の旅館か。</p>
<p><img src="/images/20171217/photo_20171217_15.jpeg"><br>ロデオボーイも完備。スポッチャ以外で乗る日が来るとは。</p>
<p><img src="/images/20171217/photo_20171217_16.jpeg"><br>そしてまたひたすら開発…</p>
<p><img src="/images/20171217/photo_20171217_17.jpeg"><br>てっぺん(24時)を超えたら集合写真です。エンジニアの夜は遅い。</p>
<p><img src="/images/20171217/photo_20171217_18.jpeg"><br>そして開発合宿の朝は早い。朝ごはんは優しいお味で健康的です。</p>
<p><img src="/images/20171217/photo_20171217_19.jpeg"><br><img src="/images/20171217/photo_20171217_20.jpeg"><br>そして開発合宿の締め。成果発表会です。<br>各々の開発成果についてアピールします。</p>
<p><img src="/images/20171217/photo_20171217_21.jpeg"><br>旅館からのサービスのデザートを食べながら真剣に聞いています。<br>なんだただのサービスが最高の旅館か。</p>
<p><img src="/images/20171217/photo_20171217_22.jpeg"><br>投票でスゴかった発表を決め、<del>余っ</del>景品のお酒を贈呈しました。</p>
<h1 id="振り返り"><a href="#振り返り" class="headerlink" title="振り返り"></a>振り返り</h1><h3 id="keep"><a href="#keep" class="headerlink" title="keep"></a>keep</h3><ul>
<li>旅館の環境・サービスが良すぎる</li>
<li>コンテンツ(講義・LT)が充実していた</li>
<li>LT全員強制参加が良かった</li>
</ul>
<h3 id="problem"><a href="#problem" class="headerlink" title="problem"></a>problem</h3><ul>
<li>LINE Payの導入連絡が遅い</li>
<li>お酒の種類が少ない(ビール以外もほしい)</li>
<li>場所が遠かった(電車で片道約3時間)</li>
</ul>
<h3 id="try"><a href="#try" class="headerlink" title="try"></a>try</h3><ul>
<li>土善旅館以外での開催</li>
<li>人数の規模を大幅に拡大して開催</li>
<li>取り組むことのテーマを縛って開催</li>
</ul>
<p>本合宿は2017/6/10-11に実施しました。<br>第2回は2017/12/16-17を予定しています。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://adventar.org/calendars/2449&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;フューチャーアーキテクト裏アドベントカレンダー2017&lt;/a&gt;の16日目です。&lt;/p&gt;
&lt;h1 id=&quot;はじめに&quot;&gt;&lt;a
    
    </summary>
    
      <category term="Culture" scheme="https://future-architect.github.io/categories/Culture/"/>
    
    
      <category term="Camp" scheme="https://future-architect.github.io/tags/Camp/"/>
    
  </entry>
  
  <entry>
    <title>Future IoT デバイス</title>
    <link href="https://future-architect.github.io/articles/20171207/"/>
    <id>https://future-architect.github.io/articles/20171207/</id>
    <published>2017-12-07T04:00:00.000Z</published>
    <updated>2017-12-13T03:25:25.306Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://qiita.com/advent-calendar/2017/future" target="_blank" rel="noopener">フューチャーアーキテクト Advent Calendar 2017</a>の7日目です。</p>
<hr>
<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>2017年、社内のR&amp;Dチームにて開発した汎用IoTデバイスについて紹介したいと思います。<br>デバイスの各辺の長さは約4.5cmの立方体で、異なる拡張モジュールを最大8つ内蔵、もしくはGroveコネクタ準拠のセンサーなどを外付けできる形になっています。</p>
<p>こんな感じのサイコロ型のデバイスです。<br><img src="/images/20171207/photo_20171207_02.jpeg"></p>
<p>アップすると FutureIoT のロゴが印字されています。<br><img src="/images/20171207/asset_20171207_01.png"></p>
<p>ちなみに、なんで鶴なんだ？とよく聞かれるのでこの場をお借りして回答しておきます。</p>
<p><strong>FutureIoTのロゴの由来</strong></p>
<ul>
<li>点はデバイスやセンサーで、線がネットワークで、様々なコネクティッドを表しています</li>
<li>鶴を形どっていて縁起がよく「仲良きことの象徴」の鳥です</li>
<li>鳴き声が共鳴して遠方まで届くことから「天に届く＝天上界に通ずる鳥」といわれるなどのシンボルなので、遠隔のフィールドの情報がネットワークの先（クラウド）まで届くこと祈願してます</li>
</ul>
<p>ロゴデザインは<a href="https://99designs.jp" target="_blank" rel="noopener">99design</a>さん経由にて作成を依頼しました。</p>
<h1 id="作成の目的"><a href="#作成の目的" class="headerlink" title="作成の目的"></a>作成の目的</h1><p>このデバイスは、IoT関連ソリューションのワークショップ、<a href="https://ja.wikipedia.org/wiki/%E6%A6%82%E5%BF%B5%E5%AE%9F%E8%A8%BC" target="_blank" rel="noopener">PoC</a>、プロトタイピング、パイロット導入などの用途をカバーすることを目指して開発したものとなります。<br>直近では、お客様向けのIoT関連研修での教材として10数台程利用しました。</p>
<h1 id="力を入れた点"><a href="#力を入れた点" class="headerlink" title="力を入れた点"></a>力を入れた点</h1><p>とにかく簡易に素早く使えることをコンセプトに、下記の特徴をもたせました。</p>
<ul>
<li>多くの拡張用コネクタ(Groveコネクタx8)をコンパクトな筐体(4.5cm)に格納</li>
<li>ユーザを選ばない幅広いプログラミング環境</li>
<li>フルワイヤレス</li>
<li>追加モジュールによる柔軟な拡張性</li>
</ul>
<h1 id="スペック"><a href="#スペック" class="headerlink" title="スペック"></a>スペック</h1><p>スペックは次の通りです。（随時ブラッシュアップしているので変更される/している可能性があります）</p>
<h2 id="ハードウェア"><a href="#ハードウェア" class="headerlink" title="ハードウェア"></a>ハードウェア</h2><p><img src="/images/20171207/photo_20171207_03.jpeg"></p>
<h3 id="MPU"><a href="#MPU" class="headerlink" title="MPU"></a><a href="https://ja.wikipedia.org/wiki/%E3%83%9E%E3%82%A4%E3%82%AF%E3%83%AD%E3%83%97%E3%83%AD%E3%82%BB%E3%83%83%E3%82%B5" target="_blank" rel="noopener">MPU</a></h3><p>MPUとしては、Wi-Fi、BLEも内蔵した比較的安価なESP32を採用しました。</p>
<ul>
<li><a href="https://ja.wikipedia.org/wiki/ESP32" target="_blank" rel="noopener">ESP32</a></li>
</ul>
<h3 id="通信"><a href="#通信" class="headerlink" title="通信"></a>通信</h3><p>基本的に無線での運用となります。Wi-Fi APやLoRaWAN GW などを経由してクラウドに接続します。</p>
<ul>
<li>有線<ul>
<li>シリアル通信(USB)</li>
</ul>
</li>
<li>無線<ul>
<li><a href="https://ja.wikipedia.org/wiki/Wi-Fi" target="_blank" rel="noopener">Wi-Fi</a></li>
<li><a href="https://ja.wikipedia.org/wiki/Bluetooth_Low_Energy" target="_blank" rel="noopener">BLE</a></li>
<li><a href="https://ja.wikipedia.org/wiki/Long_Term_Evolution" target="_blank" rel="noopener">LTE</a>(オプション)</li>
<li><a href="https://ja.wikipedia.org/wiki/LPWA_(%E7%84%A1%E7%B7%9A" target="_blank" rel="noopener">LoRaWAN</a>#LoRa)(オプション)</li>
</ul>
</li>
</ul>
<h3 id="コネクタ"><a href="#コネクタ" class="headerlink" title="コネクタ"></a>コネクタ</h3><p>コネクタとしては、PCなどの通信や充電のためのmicroUSBと、センサーなどの外部モジュールや内蔵モジュールのためのGroveコネクタがあります。</p>
<ul>
<li>microUSB コネクタ x1<ul>
<li>充電</li>
<li>シリアル通信</li>
</ul>
</li>
<li>Grove コネクタ x8<ul>
<li>デジタル入力</li>
<li>アナログ入力</li>
<li>UART</li>
<li>I2C</li>
</ul>
</li>
</ul>
<h3 id="ストレージ"><a href="#ストレージ" class="headerlink" title="ストレージ"></a>ストレージ</h3><p>基本的に内蔵Flashにプログラムを書き込みますが、大きめのデータを利用したい場合はスロットにmicroSDを差して使用する形になります。</p>
<ul>
<li>内蔵Flash</li>
<li>microSDスロット(対応予定)</li>
</ul>
<h3 id="電源"><a href="#電源" class="headerlink" title="電源"></a>電源</h3><p>充電池を内蔵しているためワイヤレスな利用が可能です。特に低消費電力を考えたプログラミングを行わなくても満充電後数時間は利用できます。プログラム次第になりますがより長時間の運用も可能です。電源を外部から取ることも可能です。</p>
<ul>
<li>内蔵：Li-Poバッテリ</li>
<li>外付：microUSBケーブルでACアダプタ、PC、モバイルバッテリと接続</li>
</ul>
<h2 id="ソフトウェア"><a href="#ソフトウェア" class="headerlink" title="ソフトウェア"></a>ソフトウェア</h2><h3 id="OS"><a href="#OS" class="headerlink" title="OS"></a>OS</h3><p>現在はその完成度の面から下記のOSを利用しています。</p>
<ul>
<li><a href="https://mongoose-os.com/" target="_blank" rel="noopener">Mongoose OS</a></li>
</ul>
<h3 id="プログラミング言語"><a href="#プログラミング言語" class="headerlink" title="プログラミング言語"></a>プログラミング言語</h3><p>非エンジニアまでターゲット層を広げているため、複数のプログラミング環境をWeb上に用意しています。</p>
<ul>
<li>プログラミング初学者向け<ul>
<li><a href="https://developers.google.com/blockly/" target="_blank" rel="noopener">Blockly</a></li>
</ul>
</li>
<li>Web開発者向け<ul>
<li>JavaScript</li>
</ul>
</li>
<li>組込開発者向け<ul>
<li>C++</li>
<li>C</li>
</ul>
</li>
</ul>
<p><img width="400" alt="Blocklyの編集画面" src="/images/20171207/asset_20171207_04.jpeg"></p>
<p><img width="400" alt="JavaScriptの編集画面" src="/images/20171207/asset_20171207_05.png"></p>
<h2 id="クラウド"><a href="#クラウド" class="headerlink" title="クラウド"></a>クラウド</h2><p>弊社のIoTプラットフォームである Future IoT との連携や、AWS、GCP、Azuleなどの主要なクラウドとの連携を行うことができます。</p>
<h2 id="対応プロトコル"><a href="#対応プロトコル" class="headerlink" title="対応プロトコル"></a>対応プロトコル</h2><ul>
<li><a href="https://ja.wikipedia.org/wiki/MQ_Telemetry_Transport" target="_blank" rel="noopener">MQTT</a></li>
<li><a href="https://ja.wikipedia.org/wiki/WebSocket" target="_blank" rel="noopener">WebSocket</a></li>
<li>HTTP</li>
</ul>
<h1 id="今後の展開"><a href="#今後の展開" class="headerlink" title="今後の展開"></a>今後の展開</h1><p>今後、次のような点を進めていく予定です。<br>ぜひ、興味がある方は連絡下さい！</p>
<ul>
<li>追加モジュールの拡充</li>
<li>プログラミング環境のブラッシュアップ</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://qiita.com/advent-calendar/2017/future&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;フューチャーアーキテクト Advent Calendar 2017&lt;/a&gt;の7日目です。&lt;/p&gt;
&lt;
    
    </summary>
    
      <category term="IoT" scheme="https://future-architect.github.io/categories/IoT/"/>
    
    
      <category term="IoT" scheme="https://future-architect.github.io/tags/IoT/"/>
    
  </entry>
  
</feed>
