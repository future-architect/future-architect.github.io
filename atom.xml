<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Future Tech Blog - フューチャーアーキテクト</title>
  <subtitle>フューチャーアーキテクト開発者ブログ</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://future-architect.github.io/"/>
  <updated>2018-10-19T05:43:44.695Z</updated>
  <id>https://future-architect.github.io/</id>
  
  <author>
    <name>Future Architect Consultants</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PostgreSQLパーティションプルーニングの動作を確認する</title>
    <link href="https://future-architect.github.io/articles/20181019/"/>
    <id>https://future-architect.github.io/articles/20181019/</id>
    <published>2018-10-19T07:00:00.000Z</published>
    <updated>2018-10-19T05:43:44.695Z</updated>
    
    <content type="html"><![CDATA[<p>PostgreSQL10までのパーティション機能を利用したプロジェクトにおいて、遅延SQLの調査をするなかで以下のような長い長い実行計画を目にすることがありました。</p>
<details><summary>こちらはサンプルテーブルでそれを再現したものです。長いので畳みました。</summary><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">Update on tr_part tgt  (cost=0.83..115.84 rows=4 width=44)</div><div class="line">  Update on tr_part_p_1809_01 tgt_1</div><div class="line">  Update on tr_part_p_1809_02 tgt_2</div><div class="line">  Update on tr_part_p_1809_03 tgt_3</div><div class="line">  Update on tr_part_p_9912_31 tgt_4</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.01 rows=1 width=38)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 tgt_1  (cost=0.42..8.44 rows=1 width=16)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.61 rows=4 width=26)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.01 rows=1 width=38)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 tgt_2  (cost=0.42..8.44 rows=1 width=16)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.61 rows=4 width=26)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.01 rows=1 width=38)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 tgt_3  (cost=0.42..8.44 rows=1 width=16)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.61 rows=4 width=26)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.57..31.81 rows=1 width=64)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.15..17.55 rows=1 width=58)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 tgt_4  (cost=0.15..5.50 rows=1 width=42)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..14.20 rows=4 width=26)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..4.65 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..4.66 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..4.66 rows=1 width=26)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.22 rows=1 width=78)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div></pre></td></tr></table></figure><br><br></details> 


<p>なぜこのようなことになっているのか、仮に性能問題（SQLの遅延）が発生しているとき、どのような対処が考えられるか説明してきます。</p>
<h2 id="PostgreSQL10での確認"><a href="#PostgreSQL10での確認" class="headerlink" title="PostgreSQL10での確認"></a>PostgreSQL10での確認</h2><p>以下のようにパーティションテーブルを用意しました。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">--drop table tr_part;</span></div><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tr_part(</div><div class="line">    part_date <span class="built_in">date</span></div><div class="line">,   <span class="keyword">key</span> <span class="built_in">numeric</span></div><div class="line">,   <span class="keyword">data</span> <span class="built_in">numeric</span></div><div class="line">)</div><div class="line"><span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">range</span>(part_date);</div><div class="line"><span class="comment">--パーティション作成</span></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> tr_part_p_1809_01 <span class="keyword">partition</span> <span class="keyword">of</span>      tr_part <span class="keyword">for</span> <span class="keyword">values</span> <span class="keyword">from</span> ( <span class="keyword">MINVALUE</span> ) <span class="keyword">to</span> (<span class="string">'20180902'</span>);</div><div class="line"><span class="keyword">alter</span>  <span class="keyword">table</span> tr_part_p_1809_01 <span class="keyword">add</span> <span class="keyword">constraint</span> pk_tr_part_p_1809_01 primary <span class="keyword">key</span>(ymd,<span class="keyword">key</span>);</div><div class="line"></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> tr_part_p_1809_02 <span class="keyword">partition</span> <span class="keyword">of</span>      tr_part <span class="keyword">for</span> <span class="keyword">values</span> <span class="keyword">from</span> ( <span class="string">'20180902'</span> ) <span class="keyword">to</span> (<span class="string">'20180903'</span>);</div><div class="line"><span class="keyword">alter</span>  <span class="keyword">table</span> tr_part_p_1809_02 <span class="keyword">add</span> <span class="keyword">constraint</span> pk_tr_part_p_1809_02 primary <span class="keyword">key</span>(ymd,<span class="keyword">key</span>);</div><div class="line"></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> tr_part_p_1809_03 <span class="keyword">partition</span> <span class="keyword">of</span>      tr_part <span class="keyword">for</span> <span class="keyword">values</span> <span class="keyword">from</span> ( <span class="string">'20180903'</span> ) <span class="keyword">to</span> (<span class="string">'20180904'</span>);</div><div class="line"><span class="keyword">alter</span>  <span class="keyword">table</span> tr_part_p_1809_03 <span class="keyword">add</span> <span class="keyword">constraint</span> pk_tr_part_p_1809_03 primary <span class="keyword">key</span>(ymd,<span class="keyword">key</span>);</div><div class="line"></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> tr_part_p_9912_31 <span class="keyword">partition</span> <span class="keyword">of</span>      tr_part <span class="keyword">for</span> <span class="keyword">values</span> <span class="keyword">from</span> ( <span class="string">'20180904'</span> ) <span class="keyword">to</span> (<span class="string">'99991231'</span>);</div><div class="line"><span class="keyword">alter</span>  <span class="keyword">table</span> tr_part_p_9912_31 <span class="keyword">add</span> <span class="keyword">constraint</span> pk_tr_part_p_9912_31 primary <span class="keyword">key</span>(ymd,<span class="keyword">key</span>);</div><div class="line"></div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tr_part <span class="keyword">select</span> <span class="string">'20180901'</span>,generate_series,<span class="keyword">round</span>(generate_series * random() * <span class="number">100</span>) <span class="keyword">from</span> generate_series(<span class="number">1</span>,<span class="number">100000</span>);</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tr_part <span class="keyword">select</span> <span class="string">'20180902'</span>,generate_series,<span class="keyword">round</span>(generate_series * random() * <span class="number">100</span>) <span class="keyword">from</span> generate_series(<span class="number">100001</span>,<span class="number">200000</span>);</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tr_part <span class="keyword">select</span> <span class="string">'20180903'</span>,generate_series,<span class="keyword">round</span>(generate_series * random() * <span class="number">100</span>) <span class="keyword">from</span> generate_series(<span class="number">200001</span>,<span class="number">300000</span>);</div><div class="line"><span class="keyword">analyze</span> tr_part;</div></pre></td></tr></table></figure></p>
<p>データを投入した <code>p_1809_01</code>、 <code>p_1809_02</code>、 <code>p_1809_03</code> のパーティションに注目すると次のようなイメージです。 <code>part_date</code> の値によってレコードがパーティションに振り分けられて格納されています。</p>
<p><img src="/images/20181019/1.png"></p>
<p>続いて以下のような小さなテーブルを用意します。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">--drop table wk_input;</span></div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> wk_input(</div><div class="line">	<span class="keyword">key</span> <span class="built_in">numeric</span></div><div class="line">,	related_key <span class="built_in">numeric</span></div><div class="line">,	target_date <span class="built_in">date</span></div><div class="line">)</div><div class="line">;</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> wk_input <span class="keyword">select</span> <span class="number">1</span>,<span class="number">1</span>     ,<span class="string">'20180901'</span>;</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> wk_input <span class="keyword">select</span> <span class="number">2</span>,<span class="number">100001</span>,<span class="string">'20180902'</span>;</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> wk_input <span class="keyword">select</span> <span class="number">3</span>,<span class="number">200001</span>,<span class="string">'20180903'</span>;</div><div class="line"><span class="keyword">analyze</span> wk_input;</div></pre></td></tr></table></figure>
<p>そして、以下のようなselectを実行するとどのような動作となるでしょうか。<br>ポイントはパーティションテーブルのパーティションキーに設定した<code>part_date</code>の列が結合条件としてのみ指定されていることです。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">select</span></div><div class="line">	*</div><div class="line"><span class="keyword">from</span></div><div class="line">	wk_input a</div><div class="line">,	tr_part b</div><div class="line"><span class="keyword">where</span> <span class="number">1</span> = <span class="number">1</span></div><div class="line"><span class="keyword">and</span> a.related_key = b.key</div><div class="line"><span class="keyword">and</span> a.target_date = b.part_date</div><div class="line">;</div></pre></td></tr></table></figure>
<p>このとき期待するのは次図の赤線のような動作でしょう。<br><img src="/images/20181019/2.png"></p>
<p>wk_input の target_date が <code>20180901</code> のレコードに対し、tr_partのpart_dateが <code>20180901</code> のパーティションにアクセスし、<br>wk_input の target_date が <code>20180902</code> のレコードに対し、tr_partのpart_dateが <code>20180902</code> のパーティションにアクセスし、<br>wk_input の target_date が <code>20180903</code> のレコードに対し、tr_partのpart_dateが <code>20180903</code> のパーティションにアクセスする。</p>
<p>ORACLEの場合はまさにそのような動作になります。<br>実行計画でみると以下のようになります。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> <span class="keyword">STATEMENT</span></div><div class="line">    <span class="keyword">NESTED</span> LOOPS</div><div class="line">        <span class="keyword">TABLE</span> <span class="keyword">ACCESS</span> <span class="keyword">FULL</span> WK_INPUT</div><div class="line">        <span class="keyword">PARTITION</span> <span class="keyword">RANGE</span> ITERATOR</div><div class="line">            <span class="keyword">TABLE</span> <span class="keyword">ACCESS</span> <span class="keyword">BY</span> <span class="keyword">ROWID</span> TR_PART</div><div class="line">                <span class="keyword">INDEX</span> <span class="keyword">UNIQUE</span> <span class="keyword">SCAN</span> PK_TR_PART</div></pre></td></tr></table></figure>
<p><code>PARTITION RANGE ITERATOR</code>のところがまさに、WK_INPUTの各行に対応するパーティションへのアクセスを示しています。</p>
<p>では、PostgreSQL10ではどのようになるかというと<code>explain analyze</code>で先のselect文を実行すると以下のような出力になりました。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">Nested Loop  (cost=0.42..93.60 rows=1 width=31) (actual time=66.130..275.215 rows=3 loops=1)</div><div class="line">  -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=15) (actual time=16.125..16.261 rows=3 loops=1)</div><div class="line">  -&gt;  Append  (cost=0.42..30.82 rows=4 width=16) (actual time=69.391..86.303 rows=1 loops=3)</div><div class="line">        -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..8.44 rows=1 width=16) (actual time=48.916..51.817 rows=0 loops=3★)</div><div class="line">              Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..8.44 rows=1 width=16) (actual time=17.012..21.819 rows=0 loops=3★)</div><div class="line">              Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..8.44 rows=1 width=16) (actual time=12.144..12.594 rows=0 loops=3★)</div><div class="line">              Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..5.50 rows=1 width=68) (actual time=0.013..0.013 rows=0 loops=3★)</div><div class="line">              Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">Planning time: 118.634 ms</div><div class="line">Execution time: 275.380 ms</div></pre></td></tr></table></figure>
<p>注目は★マークを付けた4,6,8,10行目の <code>loops=3</code> のところでしょうか。どのパーティションにもwk_inputの3行に対し3回のアクセスがあることが確認できます。</p>
<p>図にすると次のようなイメージです。<br><img src="/images/20181019/3.png"></p>
<p>つまりPostgreSQL10ではクエリ実行時にwk_inputのレコードの値をみて、パーティションプルーニングするような動作はできないことがわかります。<br>これを踏まえたうえで、パーティションテーブルとパーティションテーブルの結合を考えてみます。</p>
<p>冒頭の実行計画は以下のクエリをexplainしたものです。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">update</span> tr_part tgt</div><div class="line"><span class="keyword">set</span></div><div class="line">	<span class="keyword">data</span> = b.data *<span class="number">0.8</span></div><div class="line"><span class="keyword">from</span></div><div class="line">	wk_input a</div><div class="line">,	tr_part b</div><div class="line"><span class="keyword">where</span> <span class="number">1</span> = <span class="number">1</span></div><div class="line"><span class="keyword">and</span> a.related_key = b.key</div><div class="line"><span class="keyword">and</span> a.target_date = b.part_date</div><div class="line"><span class="keyword">and</span> b.key = tgt.key</div><div class="line"><span class="keyword">and</span> b.part_date = tgt.part_date</div><div class="line">;</div></pre></td></tr></table></figure>
<p>冒頭の実行計画の先頭部分を抜き出して以下に貼り付けました。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"> Update on tr_part tgt  (cost=0.83..115.84 rows=4 width=44)</div><div class="line">   Update on tr_part_p_1809_01 tgt_1</div><div class="line">   Update on tr_part_p_1809_02 tgt_2</div><div class="line">   Update on tr_part_p_1809_03 tgt_3</div><div class="line">   Update on tr_part_p_9912_31 tgt_4</div><div class="line">   -&gt;  Nested Loop  (cost=0.83..28.01 rows=1 width=38)</div><div class="line">         Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">         -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32)</div><div class="line">               -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16)</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01★ tgt_1  (cost=0.42..8.44 rows=1 width=16)</div><div class="line">                     Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">         -&gt;  Append  (cost=0.42..1.61 rows=4 width=26)</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01● b  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02● b_1  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03● b_2  (cost=0.42..0.48 rows=1 width=26)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31● b_3  (cost=0.15..0.17 rows=1 width=78)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">...省略</div></pre></td></tr></table></figure>
<p>パーティションテーブルであるtr_partに注目します。<br>10行目にあるtr_partテーブルの <code>p_1809_01★</code> に対し、13~19行目のtr_partテーブルの <code>p_1809_01●</code>、 <code>p_1809_02●</code>、 <code>p_1809_03●</code>、 <code>p_9912_31●</code> が参照されています。</p>
<p>パーティションテーブルtr_partに着目すると、期待する動きは次図ですが、、、<br><img src="/images/20181019/4.png"></p>
<p>実際は次図のようになっているということです。<br><img src="/images/20181019/5.png"></p>
<p>パーティション数が多いと、PostgreSQLのこのような動作がかなりな性能遅延を引き起こします。</p>
<p>PostgreSQLでは1テーブルに100を超えるほどのパーティションを定義することはあまり無いでしょう。<br>しかし、例えば1月分のデータを日次のパーティションで保持している場合の約30パーティションのテーブル同士の結合を想定すると、30×30で900通りのパーティション間の結合を試みることになります。<br>これがどれほど非効率かは想像にかたくありません。</p>
<p>PostgreSQLのこのような動作に起因して性能劣化が見られる場合は、ユーザからアクセスすべきパーティションを教えてあげる必要があります。<br>つまり、この例ではアクセス対象のパーティションはwk_inputに保持されているtarget_dateの値で決まっています。<br>そのため、<code>select distinct target_date from wk_iput</code>のように一度target_dateの一覧を抽出します。<br>そのうえで、以下のようにパーティションキーのpart_dateの値を以下のクエリの<code>/*あらかじめ取得した値*/</code>のところで指定してループ実行します。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">update</span> tr_part tgt</div><div class="line"><span class="keyword">set</span></div><div class="line">	<span class="keyword">data</span> = b.data *<span class="number">0.8</span></div><div class="line"><span class="keyword">from</span></div><div class="line">	wk_input a</div><div class="line">,	tr_part b</div><div class="line"><span class="keyword">where</span> <span class="number">1</span> = <span class="number">1</span></div><div class="line"><span class="keyword">and</span> a.related_key = b.key</div><div class="line"><span class="keyword">and</span> a.target_date = b.part_date</div><div class="line"><span class="keyword">and</span> b.key = tgt.key</div><div class="line"><span class="keyword">and</span> b.part_date = tgt.part_date</div><div class="line"><span class="keyword">and</span> b.part_date = <span class="comment">/*あらかじめ取得した値*/</span></div><div class="line">;</div></pre></td></tr></table></figure>
<p>パーティション数が多く、アクセスが非効率になっているような場合は、このようにパーティションをユーザから特定してあげることで大きな改善がみられる場合があります。</p>
<h2 id="PostgreSQL11での確認"><a href="#PostgreSQL11での確認" class="headerlink" title="PostgreSQL11での確認"></a>PostgreSQL11での確認</h2><p>さて、ある日dockerで環境構築をしていてふとPostgreSQL11（β版）がpullできるようになっていることに気づきましたので、<br>ちょろっと触ってみようと思い上記と同様にパーティションプルーニングの動作を確認してみました。</p>
<details><summary>やはりとても長い実行計画が確認できました。。</summary><div><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line">Update on tr_part tgt  (cost=0.83..115.93 rows=4 width=70) (actual time=235.751..235.751 rows=0 loops=1)</div><div class="line">  Update on tr_part_p_1809_01 tgt_1</div><div class="line">  Update on tr_part_p_1809_02 tgt_2</div><div class="line">  Update on tr_part_p_1809_03 tgt_3</div><div class="line">  Update on tr_part_p_9912_31 tgt_4</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.03 rows=1 width=64) (actual time=61.837..72.983 rows=1 loops=1)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32) (actual time=61.790..62.534 rows=1 loops=1)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16) (actual time=17.805..17.811 rows=3 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 tgt_1  (cost=0.42..8.44 rows=1 width=16) (actual time=14.901..14.901 rows=0 loops=3)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.63 rows=4 width=26) (actual time=0.036..10.437 rows=1 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26) (actual time=0.027..10.424 rows=1 loops=1)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.03 rows=1 width=64) (actual time=56.255..96.389 rows=1 loops=1)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32) (actual time=56.211..74.941 rows=1 loops=1)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16) (actual time=0.011..0.020 rows=3 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 tgt_2  (cost=0.42..8.44 rows=1 width=16) (actual time=24.968..24.968 rows=0 loops=3)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.63 rows=4 width=26) (actual time=0.033..21.437 rows=1 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26) (actual time=0.028..21.431 rows=1 loops=1)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_2.part_date) AND (key = tgt_2.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.83..28.03 rows=1 width=64) (actual time=65.428..66.187 rows=1 loops=1)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32) (actual time=65.375..65.377 rows=1 loops=1)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16) (actual time=0.021..0.028 rows=3 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 tgt_3  (cost=0.42..8.44 rows=1 width=16) (actual time=21.778..21.778 rows=0 loops=3)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..1.63 rows=4 width=26) (actual time=0.038..0.794 rows=1 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..0.48 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..0.48 rows=1 width=26) (actual time=0.031..0.785 rows=1 loops=1)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.17 rows=1 width=78) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_3.part_date) AND (key = tgt_3.key))</div><div class="line">  -&gt;  Nested Loop  (cost=0.57..31.83 rows=1 width=90) (actual time=0.039..0.039 rows=0 loops=1)</div><div class="line">        Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">        -&gt;  Nested Loop  (cost=0.15..17.55 rows=1 width=58) (actual time=0.039..0.039 rows=0 loops=1)</div><div class="line">              -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16) (actual time=0.018..0.019 rows=3 loops=1)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 tgt_4  (cost=0.15..5.50 rows=1 width=42) (actual time=0.005..0.005 rows=0 loops=3)</div><div class="line">                    Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">        -&gt;  Append  (cost=0.42..14.22 rows=4 width=26) (never executed)</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01 b  (cost=0.42..4.65 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02 b_1  (cost=0.42..4.66 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03 b_2  (cost=0.42..4.66 rows=1 width=26) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div><div class="line">              -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31 b_3  (cost=0.15..0.22 rows=1 width=78) (never executed)</div><div class="line">                    Index Cond: ((part_date = tgt_4.part_date) AND (key = tgt_4.key))</div></pre></td></tr></table></figure><br><br></div></details>

<p>　</p>
<p>残念…と思いきや<code>explain analyze</code>の結果を見ると動作が改善されていることがわかりました。<br>以下に冒頭部分を抜き出しました。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"> Update on tr_part tgt  (cost=0.83..115.93 rows=4 width=70) (actual time=235.751..235.751 rows=0 loops=1)</div><div class="line">   Update on tr_part_p_1809_01 tgt_1</div><div class="line">   Update on tr_part_p_1809_02 tgt_2</div><div class="line">   Update on tr_part_p_1809_03 tgt_3</div><div class="line">   Update on tr_part_p_9912_31 tgt_4</div><div class="line">   -&gt;  Nested Loop  (cost=0.83..28.03 rows=1 width=64) (actual time=61.837..72.983 rows=1 loops=1)</div><div class="line">         Join Filter: ((a.related_key = b.key) AND (a.target_date = b.part_date))</div><div class="line">         -&gt;  Nested Loop  (cost=0.42..26.34 rows=1 width=32) (actual time=61.790..62.534 rows=1 loops=1)</div><div class="line">               -&gt;  Seq Scan on wk_input a  (cost=0.00..1.03 rows=3 width=16) (actual time=17.805..17.811 rows=3 loops=1)</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01★ tgt_1  (cost=0.42..8.44 rows=1 width=16) (actual time=14.901..14.901 rows=0 loops=3)</div><div class="line">                     Index Cond: ((part_date = a.target_date) AND (key = a.related_key))</div><div class="line">         -&gt;  Append  (cost=0.42..1.63 rows=4 width=26) (actual time=0.036..10.437 rows=1 loops=1)</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_01 on tr_part_p_1809_01● b  (cost=0.42..0.48 rows=1 width=26) (actual time=0.027..10.424 rows=1 loops=1■)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_02 on tr_part_p_1809_02▲ b_1  (cost=0.42..0.48 rows=1 width=26) (never executed▼)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_1809_03 on tr_part_p_1809_03▲ b_2  (cost=0.42..0.48 rows=1 width=26) (never executed▼)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key))</div><div class="line">               -&gt;  Index Scan using pk_tr_part_p_9912_31 on tr_part_p_9912_31▲ b_3  (cost=0.15..0.17 rows=1 width=78) (never executed▼)</div><div class="line">                     Index Cond: ((part_date = tgt_1.part_date) AND (key = tgt_1.key)</div><div class="line">...省略</div></pre></td></tr></table></figure>
<p>10行目のtr_partテーブルの <code>p_1809_01★</code> に対し、13行目の●の同パーティションに、13行目最右の■部分でアクセスがあったことが確認できます。</p>
<p>これに対して、15行目移行の▲で目印をした <code>p_1809_02</code>、 <code>p_1809_03</code>、 <code>p_9912_31</code> のパーティションに対しては、▼部分（never executedと書いていますね）で実際の実行がスキップされていることが確認できます。<br>PostgreSQLがバージョン11になって、パーティション <code>p_1809_02</code>、 <code>p_1809_03</code>、 <code>p_9912_31</code> の結合を試みても仕方のないものとしてスキップを判断できるようになっています。</p>
<p>長年ORACLEを使い倒してきて、ふとPostgreSQLを使うと、こんなこともできないのか、と思うことがあります。<br>しかし、日々成長してきていることも感じられ、愛おしくも思えてくるのがPostgreSQLのいいところですね。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PostgreSQL10までのパーティション機能を利用したプロジェクトにおいて、遅延SQLの調査をするなかで以下のような長い長い実行計画を目にすることがありました。&lt;/p&gt;
&lt;details&gt;&lt;summary&gt;こちらはサンプルテーブルでそれを再現したものです。長いので畳みま
    
    </summary>
    
      <category term="DB" scheme="https://future-architect.github.io/categories/DB/"/>
    
    
      <category term="DB" scheme="https://future-architect.github.io/tags/DB/"/>
    
  </entry>
  
  <entry>
    <title>NLP若手の会 (YANS) 第13回シンポジウム 参加レポート</title>
    <link href="https://future-architect.github.io/articles/20180912/"/>
    <id>https://future-architect.github.io/articles/20180912/</id>
    <published>2018-09-12T08:49:16.000Z</published>
    <updated>2018-09-11T09:23:41.383Z</updated>
    
    <content type="html"><![CDATA[<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>はじめまして！2018年8月半ばからフューチャー株式会社にキャリア採用で入社した田中駿と申します。Strategic AI Groupに所属しています。大学4年次より自然言語処理を専門に研究・開発を行っており、2018年からはNLP若手の会(YANS)の委員も努めています。どうぞよろしくお願いいたします。</p>
<p>さて、弊社は<a href="http://yans.anlp.jp/entry/yans2018" target="_blank" rel="noopener">8/27~29に香川県で開催されたYANS</a>にゴールドスポンサーとして私、貞光、小池の3人で参加してきました！</p>
<p>この記事では、YANSの参加レポートとして以下のトピックを中心に共有します。</p>
<ul>
<li>弊社の発表内容</li>
<li>興味深かったポスター発表</li>
<li>全体的な感想</li>
</ul>
<h1 id="開催場所！！"><a href="#開催場所！！" class="headerlink" title="開催場所！！"></a>開催場所！！</h1><p>2018年のYANSは香川県高松市で開催されました～！花樹海という香川県内ではなかなかなグレード(らしい)の旅館で開催されました。山の上にある(なんとエントランスが8F)落ち着いた佇まいの気品のある旅館でした。</p>
<p>屋外のデッキからは電車が見えて最高でした。</p>
<p><img src="/images/20180912/photo_20180912_01.jpeg"></p>
<h1 id="弊社の発表内容"><a href="#弊社の発表内容" class="headerlink" title="弊社の発表内容"></a>弊社の発表内容</h1><p>弊社の会社についてと、私たちの所属するStrategic AI Groupの紹介を口頭発表・ポスター発表で行いました。<br>スポンサーブースには非常に多くの方が来てくださり、NLPを始めとするAI関連の取り組みに興味を持ってくださりました。弊社はBtoB事業が多く、一般の方には認知されづらい部分が多いので、スポンサー活動などを通してより多くの人に事業内容の面白さ、仕事のやりがいを伝えていきたいなと思いました。</p>
<p><img src="/images/20180912/photo_20180912_02.jpeg"><br>▲ブースターセッションで話してるわたし。オフィスの綺麗さについてアピールしているところ。</p>
<p><img src="/images/20180912/photo_20180912_03.jpeg"><br>▲イベントの紹介をする貞光。年に何回か社外の方を招待してイベントを行っています。</p>
<p><img src="/images/20180912/photo_20180912_04.jpeg"><br>▲ハイテンションでポスター発表する小池。多くの人に足を運んでもらえました。</p>
<p>今回スポンサーブースで展示していたポスターを公開します。<br>NLPに限らず、ワクチン開発や牛の発情タイミング予測など……かなり幅広い案件に取り組んでいます！<br><img src="/images/20180912/photo_20180912_05.jpeg"></p>
<h1 id="面白かった発表"><a href="#面白かった発表" class="headerlink" title="面白かった発表"></a>面白かった発表</h1><p>参加メンバが興味深いと思った発表を紹介します。</p>
<h2 id="テキスト平易化における難易度の制御"><a href="#テキスト平易化における難易度の制御" class="headerlink" title="テキスト平易化における難易度の制御"></a>テキスト平易化における難易度の制御</h2><p>西原大貴, 梶原智之, 荒瀬由紀 (阪大)</p>
<p>文章の平易化手法が多くある中で、本手法は難易度を指定して生成することを可能とする手法です。<br>学習コーパスには、元文と、それに対する言い換え文の難易度が記されているnewselaを用い、その難易度をNMT(Neural Machine Translation)の入力信号として付加する、というシンプルなアプローチでです。<br>従来手法に比べ、各難易度のリファレンス文に対するBLEU（機械翻訳で主に用いられる指標）やSARI（テキスト平易化で主に用いられる手法）で改善を示しています。newselaは１つの元文に対し、すべての難易度での言い換えが行われているわけではない、というのがタスクとしての奥行を感じました。今回は、難易度を単なるラベル情報として入力していますが、難易度の本来意味する連続値として扱うことで、データスパースネスにも頑健に働き得る可能性もあり、今後の発展が楽しみな研究です。</p>
<h2 id="人手による感情ラベル付けにおける応答時間に着目した感情推定難易度の評価"><a href="#人手による感情ラベル付けにおける応答時間に着目した感情推定難易度の評価" class="headerlink" title="人手による感情ラベル付けにおける応答時間に着目した感情推定難易度の評価"></a>人手による感情ラベル付けにおける応答時間に着目した感情推定難易度の評価</h2><p>山下紗苗, 上泰 (明石高専), 加藤恵梨, 酒井健, 奥村紀之 (大手前大学)</p>
<p>感情推定の難易度と、そのアノテーションの時間との間で相関をとろうと試みた意欲的な研究です。<br>今回アノテーションの対象としていたのは著者のツイートで、著者の知人がアノテートするほど、時間は短くなると仮説を持っていたそうですが、実験結果では真逆、つまり著者を全く知らないアノテータのアノテーション時間の方が短い、という傾向が得られたようです。書き手を知っているが故に、いろいろと深く考えてしまうのでしょうか？当初の仮説と逆の結果を率直に発表する、というのも若手の会の面白いところで、良いところだと思います。今回の結果を受けて、今後どのように本研究が展開していくのかとても楽しみな研究でした。</p>
<h2 id="五感に基づく言語表現における個人のバイアスとその補正"><a href="#五感に基づく言語表現における個人のバイアスとその補正" class="headerlink" title="五感に基づく言語表現における個人のバイアスとその補正"></a>五感に基づく言語表現における個人のバイアスとその補正</h2><p>大葉大輔 (東大), 吉永直樹 (東大/生産研), 赤崎智 (東大), 豊田正史 (東大/生産研)</p>
<p>人によって同じ「辛い」という単語でも、個人のバイアスが存在するために人によって捉え方が異なります。この研究では、個人のバイアスの分析をすることによって、「辛い」といった表現に対する捉え方のバイアスを補正することを目的にしています。<br>手法としては、大規模コーパスから分散表現を用いて単語ベクトルを獲得し、個人の文書を用いて追加学習することによりバイアスの補正に使用している。発想が非常に面白く、表現の捉え方の違いによるコミュニケーション齟齬の減少に貢献するのではないかと思いました。</p>
<h2 id="RUSE-文の分散表現を用いた回帰モデルによる機械翻訳の自動評価"><a href="#RUSE-文の分散表現を用いた回帰モデルによる機械翻訳の自動評価" class="headerlink" title="RUSE: 文の分散表現を用いた回帰モデルによる機械翻訳の自動評価"></a>RUSE: 文の分散表現を用いた回帰モデルによる機械翻訳の自動評価</h2><p>嶋中宏希 (首都大), 梶原智之 (阪大), 小町守 (首都大) </p>
<p>機械翻訳における自動評価を提案している研究です。<br>機械翻訳では、BLUE値等の評価指標を使われることが多いですが、これらの評価はN-gramに基づく素性を利用しており、意味的な情報を扱えていません。この研究では、分散表現および回帰モデルを用いて機械翻訳の自動評価を行っており、既存の提案指標よりも人手評価との相関が高いことが報告されています。評価尺度を新しく提案するというアプローチではなく、評価を機械にさせてしまおうという発想は非常に面白いと思いました。</p>
<h1 id="さいごに"><a href="#さいごに" class="headerlink" title="さいごに"></a>さいごに</h1><p>今回のYANSは委員の仕事などで朝が早かったため、なかなかハードでした…。<br>一方で今までお話ししたことのなかった多くの研究者、企業の方々と意見交換ができ、有意義な3日間となりました。栗林公園を散策したり、ボードゲームやインディアンポーカー(独自ルールのやつ)をしたり。。とても盛り上がり、楽しかったです。笑<br>そして最終日には念願の讃岐うどんをいただきました、食べたすぎてたまらなかったので写真とるのを忘れてしまいました。</p>
<p><img src="/images/20180912/photo_20180912_06.jpeg"><br>▲栗林公園の橋。まったりできました</p>
<p>今後も言語処理界隈のイベントには積極的に参加していきます。<br>お会いした時はどうぞよろしくお願いいたします。</p>
<p>フューチャー株式会社では自然言語処理の新卒・キャリア採用、インターンを積極募集中です！</p>
<p>社員の関係がとてもフラット、メンバ間での連携・共有がしっかりと行われており、雰囲気も良く、切磋琢磨できる良い環境だと思っています。そしてなんといってもオフィスがきれいです！！笑</p>
<p>見学や面談など、いつでも受け付けているのでぜひ！:)<br>一緒にNLPしましょうー！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h1&gt;&lt;p&gt;はじめまして！2018年8月半ばからフューチャー株式会社にキャリア採用で入社した田中駿と申します。Strategic AI 
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
      <category term="NLP" scheme="https://future-architect.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>5TB/日 のデータをAWS Glueでさばくためにやったこと（概要編</title>
    <link href="https://future-architect.github.io/articles/20180828/"/>
    <id>https://future-architect.github.io/articles/20180828/</id>
    <published>2018-08-28T04:27:14.000Z</published>
    <updated>2018-08-28T09:23:37.816Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/20180828/photo_20180828_01.png"></p>
<p>みなさん、初めまして、お久しぶりです、こんにちは。<br>フューチャーアーキテクト2018年新卒入社、1年目エンジニアのTIG（Technology Innovation Group）所属の澤田周吾です。大学では機械航空工学を専攻しており、学生時代のインターンなどがキッカケで入社を決意しました。</p>
<p>実は、本記事でフューチャーテックブログの2記事目となります。インターン時代も <a href="https://future-architect.github.io/articles/20170421/">ジャガイモARの記事</a>  を書かせて頂きました。入社してからもこうして業務で学んだIT技術を記事に書くという機会を貰え、なんだか懐かしいやら感慨深いやらの思いで一杯です。</p>
<p>さて、3ヶ月の新人研修後にすぐに配属されたプロジェクトで、AWSを使ったビックデータ分析のための基盤構築をお手伝いしています。わたしは分析のための前処理であるETL（Extract、Transform、Load）処理部分をちょっと変わった性格の先輩方と一緒に開発しており、今回はそれに用いているサービスであるAWS Glueについて紹介いたします。</p>
<p>※記事は2回にわけて発信していきたいと考えています。<br>第一弾として、題名の大規模データを処理するために行った様々な工夫を説明する前に、Glueの概要や開発Tips、制約について書かせていただきます</p>
<h1 id="AWS-Glueとは"><a href="#AWS-Glueとは" class="headerlink" title="AWS Glueとは"></a>AWS Glueとは</h1><p>ご存知の方も多いかと思いますが、簡単にGlueについての説明です。</p>
<blockquote>
<p>AWS Glue は抽出、変換、ロード (ETL) を行う完全マネージド型のサービスで、お客様の分析用データの準備とロードを簡単にします。AWS マネジメントコンソールで数回クリックするだけで、ETL ジョブを作成および実行できます。<br><a href="https://aws.amazon.com/jp/glue/" target="_blank" rel="noopener">引用:AWS公式サイト</a></p>
</blockquote>
<p>簡単に言うと、「データ処理を行うサービス」です。<br>公式サイトにも書かれていますが、Glueの特徴として、5点挙げられます。</p>
<ol>
<li>AWSの一つであること</li>
<li>ETL処理を行うサービスであること</li>
<li>完全マネージド型であること</li>
<li>Scala、Python、Apache Spark を使用できること</li>
<li>並立分散処理ができること</li>
</ol>
<h1 id="今回実現したいこと"><a href="#今回実現したいこと" class="headerlink" title="今回実現したいこと"></a>今回実現したいこと</h1><p>S3やDynamoDBに配備された入力データを、少々複雑な加工ロジックが入ったETL処理を何度か繰り返し、蓄積用のDynamoDBと、分析用のS3に出力することです。<br>入力のマスタデータは日次程度の洗い替えでOK、入力データは10分毎にzip圧縮後で35GB程度がDataLakeに供給され、それらを逐次バッチ的に処理します。ETLの処理ウインドウ時間は10分以内となり、1日では合計5TBに及びます。</p>
<p><img src="/images/20180828/photo_20180828_02.png"></p>
<p>データ量が多いため、Glueが利用できる前だとSpark on EMRで処理することを検討していたと思います。EMRも良いサービスだと認識していますが、10分毎に処理する要件だと、EMRクラスタを常時立ち上げざる得ないため、EMRの保守運用を考えるとマネージドでSparkを扱えるGlueを採用したほうが良いのでは？という判断がなされました。他にもAWS Athenaなども候補に上がりましたが、読み込み時のパーティションは可能なものの、2018.08時点ではクエリ結果をS3に書き込む際に、DynamicPartitionができないという点がネックで採用には至りませんでした。</p>
<p>ETL処理の中身を簡単に言うと…</p>
<ol>
<li>処理対象のフィルター（例: 異常値の排除など）</li>
<li>コンテンツへのエンリッチメント（例: マスタデータと結合し非正規化処理など）</li>
<li>多様な分析軸でのグルーピング（例: ユーザID軸、ユーザの属性軸など）</li>
<li>逆ジオコーディングのために外部サービスにアクセスが必要</li>
<li>k-meansライクなクラスタリング処理が必要</li>
</ol>
<p>などがあります。<br>特に4,5は少し毛色が変わっており、処理特性の違いからETLのパイプラインを当初より少し多めに分割する必要性がでてきました。今回は4つのステップに分割しました。そのため、各ステップは10分よりずっと短い時間で処理を完了させる必要があります。</p>
<p>また、今回の要件だとワークフローは複雑な分岐や待ち合わせが存在せず、前のジョブAが終わったら素直に後ろのジョブBを起動すれば良いだけだったため、StepFunctionなどを導入せずGlueで完結して構築しています。</p>
<h1 id="EMR-と-Glue-比較"><a href="#EMR-と-Glue-比較" class="headerlink" title="EMR と Glue 比較"></a>EMR と Glue 比較</h1><p>EMRとGlueですが、アプリケーションの実装としてはどちらもSparkを利用する以上は差が出ないため、インフラレベルで比較した表となります。どうやら、Glueは内部的にEMRを起動させているようなので、GlueはSparkクラスタを構築せずジョブが実行できるといった仕組みと捉えたほうが理解しやすいかと思います。</p>
<p>EMRの方が細かくチューニングが可能ですが、先に述べたように保守運用性の観点からGlueを採用しました。</p>
<table>
<thead>
<tr>
<th>#</th>
<th>EMR</th>
<th>Glue</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pros</td>
<td>ブートストラップアクション/ステップ処理を通じてOSレベルからの設定変更が可能</td>
<td>ランタイム環境がフルマネージドで提供されるため、クラスタの管理が不要</td>
</tr>
<tr>
<td></td>
<td>コアノード/タスクノード数を調整することでシステムリソース量の調整が可能</td>
<td>サービスとしての単一障害点がない</td>
</tr>
<tr>
<td></td>
<td>Spark以外のツールを利用可能</td>
<td>データカタログを他AWSサービスと共有可能</td>
</tr>
<tr>
<td>Cons</td>
<td>マスタノードが単一障害点（特に常駐起動させておく場合にネック）</td>
<td>ランタイム環境がフルマネージドで提供されるため、OSレベル・クラスタレベルでの設定変更が不可能(システムリソースは調整可能)</td>
</tr>
<tr>
<td></td>
<td>EMRクラスタの設定や起動処理の設計・実装が必要</td>
<td>ー</td>
</tr>
</tbody>
</table>
<h1 id="Glueで行えること"><a href="#Glueで行えること" class="headerlink" title="Glueで行えること"></a>Glueで行えること</h1><p>具体的なイメージが湧きにくいと思いますので、早速ですが、コードベースでどういうことができるかいくつか例示していきたいと思います。<br>今回、コードはPythonで説明していきます。</p>
<h2 id="S3上のファイルの読み書き"><a href="#S3上のファイルの読み書き" class="headerlink" title="S3上のファイルの読み書き"></a>S3上のファイルの読み書き</h2><p>以下のコードでS3に対して読み込み・書き込みが行えます。<br>ファイル形式を変更することで、CSV、JSON、Parquetなどの形式に対応できます。</p>
<p>Glueで定義されたデータ構造のDynamicFrameを使っていきます。<br>使い方はSparkのDataFrameのように扱うことができます。</p>
<figure class="highlight python"><figcaption><span>S3からCSVファイルの読み込み処理</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">df = glueContext.create_dynamic_frame.from_options(</div><div class="line">    connection_type=<span class="string">"s3"</span>,</div><div class="line">    connection_options=&#123;</div><div class="line">        <span class="string">"paths"</span>: [<span class="string">"s3://&#123;0&#125;"</span>.format(<span class="string">"バケット名"</span>)]&#125;,</div><div class="line">    format=<span class="string">"csv"</span>,  <span class="comment"># ファイル形式指定 json, parquet等に変更可</span></div><div class="line">    format_options=&#123;<span class="string">"withHeader"</span>: <span class="keyword">True</span>&#125;  <span class="comment"># 1行目をスキーマ名として認識True</span></div><div class="line">    )</div></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>DynamicFrame(df)をS3にcsv形式で出力処理</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">datasink = glueContext.write_dynamic_frame.from_options(</div><div class="line">        frame=df,  <span class="comment"># 出力するDynamicFrameを指定</span></div><div class="line">        connection_type=<span class="string">"s3"</span>,</div><div class="line">        connection_options=&#123;</div><div class="line">            <span class="string">"path"</span>: [<span class="string">"s3://&#123;0&#125;"</span>.format(<span class="string">"バケット名"</span>)],</div><div class="line">            <span class="string">"partitionKeys"</span>: <span class="string">"パーティションを切るキー名"</span>&#125;,</div><div class="line">        format=<span class="string">"csv"</span>  <span class="comment"># ファイル形式指定</span></div><div class="line">        )</div></pre></td></tr></table></figure>
<p>他のファイル形式については<a href="https://docs.aws.amazon.com/ja_jp/glue/latest/dg/aws-glue-programming-etl-format.html" target="_blank" rel="noopener">AWS Glue の ETL 出力用の形式オプション
</a> を参考ください。CSVであれば区切り文字やヘッダ行出力の有無もオプションで指定できます。</p>
<h2 id="DaynamoDBからの読み込み、書き込み"><a href="#DaynamoDBからの読み込み、書き込み" class="headerlink" title="DaynamoDBからの読み込み、書き込み"></a>DaynamoDBからの読み込み、書き込み</h2><p>DynamoDBへのアクセスはAWS SDK for Pythonなboto3を利用します。<br>2018.08時点では標準のコネクタは存在しないようです。</p>
<figure class="highlight python"><figcaption><span>DynamoDB初期化</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">dynamo_region=<span class="string">"AWSリージョン名"</span></div><div class="line">dynamodb = boto3.resource(</div><div class="line">        <span class="string">'dynamodb'</span>,　</div><div class="line">        region_name=dynamo_region,</div><div class="line">        endpoint_url=<span class="string">'http://dynamodb.'</span> + dynamo_region + <span class="string">'.amazonaws.com'</span></div><div class="line">        )</div></pre></td></tr></table></figure>
<p>あとは素直にget, put すれば読み書きできます。</p>
<figure class="highlight python"><figcaption><span>DynamoDBからデータベースの読み込み</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">table = dynamodb.Table(<span class="string">"テーブル名"</span>)</div><div class="line">response = table.get_item(</div><div class="line">        Key=&#123; <span class="string">'xxx'</span>: xxx, <span class="string">'yyy'</span>: yyy &#125;</div><div class="line">    )</div><div class="line">print(response)</div></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>DynamoDBへデータ出力</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">table = dynamodb.Table(<span class="string">"テーブル名"</span>)</div><div class="line">response = table.put_item(</div><div class="line">    Item=&#123;<span class="string">'xxx'</span>: xxx, <span class="string">'yyy'</span>: yyy&#125;</div><div class="line">)</div><div class="line">print(response)</div></pre></td></tr></table></figure>
<p>もし、DynamoDBへのR/Wを行う場合は、Read/Writeのキャパシティーユニットを確認するとともに、レスポンスでスループット超過時のエラーハンドリングもお忘れないように注意ください。</p>
<h2 id="加工処理"><a href="#加工処理" class="headerlink" title="加工処理"></a>加工処理</h2><p>入力で取得したデータは例えば以下のようにSQLクエリを用いて加工することができます。</p>
<figure class="highlight python"><figcaption><span>SQLを利用した加工処理</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># DynamicFrameをDataFrameに変換</span></div><div class="line">dataFrame = df.toDF()</div><div class="line"></div><div class="line"><span class="comment"># DataFrameにテーブル名を割り当て</span></div><div class="line">dataFrame.registerTempTable(<span class="string">'table_name'</span>)</div><div class="line"></div><div class="line"><span class="comment"># SparkSQL文にてデータ加工</span></div><div class="line">select_table = spark.sql(<span class="string">'SELECT * FROM table_name'</span>)</div></pre></td></tr></table></figure>
<p>今回の開発では情報量が多いことと開発チームメンバーのスキルセットからDataFrameに変換後にSQLで加工する手法を使いました。</p>
<h2 id="ジョブのワークフロー"><a href="#ジョブのワークフロー" class="headerlink" title="ジョブのワークフロー"></a>ジョブのワークフロー</h2><p>GlueのTriggerを利用することで、Glue内でジョブのワークフローを作ることができます。<br>また、起動を制御するためのTriggerは3種類用意されています。</p>
<ol>
<li>Triggerの開始をタイマーで行う ＝ <strong>スケジュール</strong></li>
<li>ジョブイベントが監視対象リストに一致した場合に行う ＝ <strong>ジョブイベント</strong></li>
<li>手動で開始させる ＝ <strong>オンデマンド</strong></li>
</ol>
<p>1のスケジュールトリガー、3のオンデマンドトリガーについてはイメージがつくと思います。<br>2のジョブイベントトリガーについて補足していきます。</p>
<p>ジョブイベントトリガーは、ジョブXが終わったら次のジョブYを起動する、といった依存関係を設定することができます。<br>具体的にはジョブイベントトリガー作成時には以下の項目を選択することができます。</p>
<ul>
<li>監視対象ジョブ（複数可）</li>
<li>トリガーするジョブ（複数可）</li>
<li>「成功、失敗、停止、タイムアウト」の４つのジョブステータス</li>
<li>監視対象ジョブとステータスが全一致でトリガーするか、部分一致でトリガーするか</li>
<li>起動時に渡すパラメータ（セキュリティ設定、ブックマーク、タイムアウト、キー/値）</li>
</ul>
<p>例：ジョブイベントトリガーを利用して以下の様なフローを３つのジョブイベントトリガーを設定することで実現することができます</p>
<p><img src="/images/20180828/photo_20180828_03.png"></p>
<ul>
<li>トリガー1はジョブAが成功したらジョブB, Cを起動</li>
<li>トリガー2はジョブBが成功したらジョブDを起動</li>
<li>トリガー3はジョブC, Dが成功したらジョブEを起動</li>
</ul>
<h1 id="Glue開発Tips"><a href="#Glue開発Tips" class="headerlink" title="Glue開発Tips"></a>Glue開発Tips</h1><p>この1ヶ月で学んだGlueで開発を行う上でのコツをお伝えします</p>
<h2 id="Tips1-Glueデバックについて"><a href="#Tips1-Glueデバックについて" class="headerlink" title="Tips1. Glueデバックについて"></a>Tips1. Glueデバックについて</h2><p>Glueの開発・デバッグには、開発エンドポイントを利用すると便利です。</p>
<p>ローカルのコンソール上から開発エンドポイントへSSH接続することで、Glueに対しpythonやscalaのREPLを使用できます。</p>
<p>エンドポイント作成はGlueのDPU数を指定し、SSH用の鍵を設定するのみです。わずか数クリック、1~2分で完了できます。ただし、エンドポイント作成後は裏でGlue(Spark)のクラスタ構築が行われるため、実際に使用可能となるのはその完了後（約15分ほどかかりました）です。</p>
<p>作成したエンドポイントへApache Zeppelin ノートブックを接続し、ノートブックでの開発も可能なようですが、今回は必要でなかったため使用しませんでした。ノートブックはローカルでもEC2上でも利用可能なようです。詳細は以下をご覧ください。<br><a href="https://docs.aws.amazon.com/ja_jp/glue/latest/dg/dev-endpoint.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/ja_jp/glue/latest/dg/dev-endpoint.html</a></p>
<p>また開発エンドポイントも裏ではGlueのクラスタが上がっているため、通常通りのGlue料金が請求されます。開発エンドポイントのDPU数はデフォルトで5、料金は1DPUあたり$0.44/時かかります。(公式ドキュメントには$0.44/秒と誤記されてますが、そんな高くないです)。</p>
<p>仮に終業時に開発エンドポイントを落とし忘れた場合、翌出社までに以下の金額がかかります。<br>　　5 <em> $0.44 </em> 14時間(20時退社、10時出社) = $30.8<br>1回の飲み代程度です。一晩にしてはなかなかです。自動で落とす方法もちょっと調べてみましたが見つけられず…。<br>忘れずに落とすようにしましょう！</p>
<h3 id="※開発エンドポイントを使わない場合※"><a href="#※開発エンドポイントを使わない場合※" class="headerlink" title="※開発エンドポイントを使わない場合※"></a>※開発エンドポイントを使わない場合※</h3><p>開発開始直後は開発エンドポイントの存在を知らず、Glueコンソール上でソースを編集し実行することで開発をしていました。<br>その当時の開発スタイルの状況も悪い例として記載します。<strong>興味が無い場合はスキップください。</strong></p>
<p><strong>（※開発エンドポインを利用すれば解決できる内容です。開発エンドポイントをぜひ利用しましょう！）</strong></p>
<ul>
<li><p>ジョブ実行に時間かかる</p>
<ul>
<li>Glueジョブは実行の都度リソースを確保しクラスタの構成を行います。そのため純粋なコンピューティング以外で毎回10分ほど待たされます</li>
<li>ソースを編集 → 実行(10分以上かかる) → 結果を確認、ソースを修正 → 実行(10分以上かかる) → …</li>
</ul>
</li>
<li><p>不要なログが多く目的のログに辿り着けない</p>
<ul>
<li>Glueの実行ログはCloudWatch上から確認可能ですが、実行スクリプトのログとSparkのログが同一のログストリーム上に出力されます<ul>
<li>そのため大量のSparkのログに目的のスクリプトログが埋もれます</li>
<li>CloudWatchから目的のログに辿り着けるよう、ログにプレフィクスなどをつけるなど工夫が必要でした</li>
</ul>
</li>
<li>ソースを編集 → 実行(10分以上かかる) → CloudWatchのログストリーム表示 → プレフィクスで目的のログ検索 → 結果を確認 → ソースを修正 → 実行(10分以上かかる) → …</li>
</ul>
</li>
<li><p>ソース修正、実行のための画面遷移が多い</p>
<ul>
<li>ジョブの一覧画面からスクリプトを参照することはできますが、編集するには専用の編集画面で行う必要があります。そのため編集の都度画面遷移が必要です</li>
<li>また、編集画面にはジョブの実行ボタンが存在しますが、このボタンが反応せずほとんど実行開始してくれません(原因は不明)。そのため実行の都度またジョブ一覧画面へと遷移する必要があります</li>
<li>ソースを編集 → 編集画面から実行画面へ → 実行(10分以上かかる) → CloudWatchのログストリーム表示 → プレフィクスで目的のログ検索 → 結果を確認 → 実行画面から編集画面へ → ソースを修正 → 編集画面から実行画面へ → 実行(10分以上かかる) → …</li>
</ul>
</li>
</ul>
<p>どうでしょう？壮絶な開発状況を想像していただけたでしょうか？<br>エンジニアたるもの、あるものは利用して賢く効率よく働きたいと、改めて思いました…</p>
<p>これらの苦労は <strong>開発エンドポイントを利用すれば</strong> その多くが回避できます。ぜひ利用しましょう！</p>
<h2 id="Tips2-AWS-Athenaで簡易的にデータ確認"><a href="#Tips2-AWS-Athenaで簡易的にデータ確認" class="headerlink" title="Tips2. AWS Athenaで簡易的にデータ確認"></a>Tips2. AWS Athenaで簡易的にデータ確認</h2><p>Glueのテーブルを使用する場合は、Athenaのクエリで中身を確認できるため開発が捗りました。</p>
<p>Athenaからテーブルに対して、 <code>SELECT * FROM TABLE</code> などのクエリを投げても良いですし、Glueコンソールのテーブル一覧画面から「アクション」→「データの確認」を選択しても良いです。後者の場合は自動でAthenaから <code>SELECT * FROM TABLE limit 10</code> というクエリを投げてくれます。どちらの場合もAthenaの料金が発生するため、読み取りデータ量には注意が必要です。</p>
<p>また、開発にSparkSQLを用いる場合はAthenaも同じSQLであるため、AthenaでSQLを開発してからSparkへ移植という使い方が可能です。ただし、AthenaはPrestoベース、SparkSQLはHiveSQLのスタイルをベースに開発されており、利用できる構文に微妙な差異がるため注意が必要です。<br>Athenaで開発したSQLをそのまま移植しようとした時に少しハマりもしました。</p>
<p>例えば文字列結合の場合、以下のようなSQLはAthenaでは利用できてSparkSQLでは利用できません。</p>
<figure class="highlight sql"><figcaption><span>AthenaSQLとSparkSQL</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">-- AthenaではOK、SparkSQLではNG</span></div><div class="line"><span class="keyword">SELECT</span> <span class="string">'str1'</span> || <span class="string">'str2'</span> || <span class="string">'str3'</span></div><div class="line"></div><div class="line"><span class="comment">-- SparkSQLでOK</span></div><div class="line"><span class="keyword">SELECT</span> <span class="keyword">CONCAT</span>(<span class="string">'str1'</span>, <span class="string">'str2'</span>, <span class="string">'str3'</span>)</div></pre></td></tr></table></figure>
<p>SparkSQLの詳細については、GlueのVersionが2018.08月時点では2.1.1ですので、下記のガイドを参考ください。<br><a href="https://spark.apache.org/docs/2.1.1/sql-programming-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/2.1.1/sql-programming-guide.html</a></p>
<h2 id="Tips3-Glueのカタログデータについて"><a href="#Tips3-Glueのカタログデータについて" class="headerlink" title="Tips3. Glueのカタログデータについて"></a>Tips3. Glueのカタログデータについて</h2><p>Glueといえばカタログデータ、という印象がありましたが実はカタログが未登録でもETLジョブは実行可能です。</p>
<p>GlueのデータカタログはApache Hive メタストアとの互換性がありますが、EMR以外にもAthenaやRedshift Spectrum でも利用できます。</p>
<p>開発Tips2であるように一部のS3バケットに対してはAthenaのクエリを発行したかったためデータカタログを設定しましたが、ETL処理に閉じて見た場合に恩恵が無いように感じられたため最終的にはデータカタログを利用しませんでした。</p>
<h2 id="Tips4-DataFrameとDynamicFrameについて"><a href="#Tips4-DataFrameとDynamicFrameについて" class="headerlink" title="Tips4. DataFrameとDynamicFrameについて"></a>Tips4. DataFrameとDynamicFrameについて</h2><p>Glueでは2種類のDataFrameを利用することができます。<br>SparkのDataFrameと、Glueで独自に定義されたDynamicFrameです。</p>
<p>両者ともテーブルの構造でデータを持ち、データ操作を行えるという点は共通していますが、DynamicFrameはchoice型を扱うという点で差異があります。choice型とは、同一列に複数のデータ型を持つことができる型です。</p>
<p>例えば、同一列にstringとdoubleを含むデータをDynamicFrameに読み込んだ場合、以下のようなイメージとなります。</p>
<p>取り込み元データ</p>
<table>
<thead>
<tr>
<th style="text-align:left">col1</th>
<th style="text-align:left">col2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">str</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">20.3</td>
</tr>
</tbody>
</table>
<p>取り込み後DynamicFrame</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">root</div><div class="line">┣- col1: int</div><div class="line">┣- col2: choice</div><div class="line">┃┣- string</div><div class="line">┃┣- double</div></pre></td></tr></table></figure>
<p>col2は2種類の型のデータが存在するためchoice型となり、stringとdouble両方を保持します。<br>様々なデータ取り込みに対応可能とするため、このような構造になっているのでしょう。</p>
<p>ただchoice型はこのままでは扱いにくいため、resolveChoice関数によって扱いやすい形へ変換してあげるとよいです。<br>resolveChoice関数では以下のことができます。</p>
<ul>
<li>choice列を任意の型にcastする。(choice列を、例えばstring列へ変換する)</li>
<li>choice列に含まれる型別に、新しく列を生成する。(stringとdoubleの2列を生成する)</li>
<li>choice列に含まれる型を保持できる構造体の列を生成する。(stringとdoubleを保持できる構造体列を1列生成する)</li>
<li>choice列を任意の型にcastした列を生成する。(例えばstring列を1列生成する)</li>
</ul>
<p>今回は、調査時の情報量が圧倒的にSparkのDataFrameの方が多く使いやすいため、データの取り込み後choice列は早々にstringへ変換し、かつSparkのDataFrameへ変換して使用しています。</p>
<p>データの取り込みはDynamicFrameで行い、本格的なデータの加工はSparkのDataFrameで行う、という使い分けが良いかと思います。</p>
<p>DynamicFrameやresolveChoiceの詳細は以下を参考ください。<br><a href="https://docs.aws.amazon.com/ja_jp/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/ja_jp/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html</a> (edited)</p>
<h1 id="Glueの注意点"><a href="#Glueの注意点" class="headerlink" title="Glueの注意点"></a>Glueの注意点</h1><p>Glueを利用して感じた注意点をまとめます。</p>
<h2 id="注意1-既存データに対するUpdate"><a href="#注意1-既存データに対するUpdate" class="headerlink" title="注意1. 既存データに対するUpdate"></a>注意1. 既存データに対するUpdate</h2><p>Glueではカラムの加工、テーブルの新規作成（SQLでいうCreate As Select）、テーブルのJoinなどETL処理ができます。またその特性上、中間データはS3上に配備されることが多いと思います。</p>
<p>しかしS3上にある既存カラムの <strong>Updateはできません</strong> ので注意が必要です。<br>これは裏側で動くSparkがあくまでS3へ追記しか行っていないからでしょう。<br>どうしてもUpdateしたいときは以下で代用できないかなど追加の検討が必要です。</p>
<ol>
<li>Delete &amp; Insert でテーブルやパーティション自体を再作成する</li>
<li>既存データの更新せずデータを追記し、抽出時にDistinctするロジックを追加する</li>
</ol>
<p>どちらにしても、書き込みや読み込みに余分な処理が発生するため処理コストが多くかかってしまいます。<br>最終的な利用元である分析に対して、どれくらいのデータ鮮度が求められるか、費用対効果で考える必要があると思います。</p>
<p>※ちなみに、DynamoDBに対しては書き換えたい情報だけに絞り込んでInsertすることで、実質的にUpdate処理が可能となりますが、DynamoDBのRCU/WCUの費用を考えると利用したいユースケースは少なそうです。</p>
<h2 id="注意2-C言語に依存するパッケージ（Pandas等）が利用不可"><a href="#注意2-C言語に依存するパッケージ（Pandas等）が利用不可" class="headerlink" title="注意2. C言語に依存するパッケージ（Pandas等）が利用不可"></a>注意2. C言語に依存するパッケージ（Pandas等）が利用不可</h2><p><a href="https://docs.aws.amazon.com/ja_jp/glue/latest/dg/aws-glue-programming-python-libraries.html" target="_blank" rel="noopener">リファレンスにも記載</a>がありますが、Glueの仕様でC言語依存パッケージを使うことができません。<br>Scikit_learnを使いたかったのですが、内部パッケージにpandasが使われているため起動できず、Scikit_learnの処理は違うアーキテクチャ（ECS）に切り分けました。すでに使いたいものが決まっている場合は注意してください。</p>
<p>もともとは、PySparkのUDFで処理可能と想定していましたが、Glueのこの制約を見逃していて、半日潰してしまいました。</p>
<p>C言語依存パッケージがGlueで使えない理由としては、コンパイルが絡んでいるのではないかと考えています。今後のGlueの機能拡張で使えるようになってくれると便利さが増しますね。</p>
<h2 id="注意3-ジョブブックマークが対応していない入力"><a href="#注意3-ジョブブックマークが対応していない入力" class="headerlink" title="注意3. ジョブブックマークが対応していない入力"></a>注意3. ジョブブックマークが対応していない入力</h2><p>Glueの機能にジョブブックマークというすでに処理されたデータかどうかを判定し、処理済みであれば次のジョブでは入力データに含めないという機能があります。<br>2018.08時点ではJSON、CSVなどには対応しているものの、zipファイルやParquestには対応していませんでした。<br><a href="https://docs.aws.amazon.com/ja_jp/glue/latest/dg/monitor-continuations.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/ja_jp/glue/latest/dg/monitor-continuations.html</a></p>
<p>そのため、もしジョブブックマーク非対応の入力に対しては、手動でオフセット管理する必要があります。<br>今回、わたしたしは入力ディレクトリに処理済みかどうかのフラグファイルを配備し、Glueジョブ上でその有無を確認することで処理対象とするか判定するロジックを追加しました。</p>
<h2 id="注意4-S3上のソースファイルの実行"><a href="#注意4-S3上のソースファイルの実行" class="headerlink" title="注意4. S3上のソースファイルの実行"></a>注意4. S3上のソースファイルの実行</h2><p>S3にソースファイルを配置する際に複数ファイルの場合は、zip圧縮する必要があります。<br>地味ですが忘れると動かないのでご注意を。</p>
<h2 id="注意5-並立分散処理"><a href="#注意5-並立分散処理" class="headerlink" title="注意5. 並立分散処理"></a>注意5. 並立分散処理</h2><p>Sparkの設定にちょっとした工夫が必要です。<br>第2回目の記事で詳しく説明したいと思います。</p>
<h2 id="注意6-料金について"><a href="#注意6-料金について" class="headerlink" title="注意6. 料金について"></a>注意6. 料金について</h2><p>Glueの料金計算はやや特殊でDPU (Data Processing Unit) という数に基づいて時間（1秒）ごとに課金が発生します。2018.08時点では1DPUでは4vCPU・16GBメモリが提供されます。<br>2018.08時点では <strong>10分の最小期間が設定</strong> されているため、処理時間が10分以下のミニバッチを連続的に起動させたい場合にはコスト的には不利になってしまいます。</p>
<p><a href="https://aws.amazon.com/jp/glue/pricing/" target="_blank" rel="noopener">https://aws.amazon.com/jp/glue/pricing/</a></p>
<p>これを避けるために分析部門にとってはデータの鮮度は下がるものの、20-30分単になどに処理頻度を変更する余地が無いか、費用対効果から見た全体最適の視点で検討中です。</p>
<h2 id="注意7-リソースの確保について"><a href="#注意7-リソースの確保について" class="headerlink" title="注意7. リソースの確保について"></a>注意7. リソースの確保について</h2><p>（2018.08時点、東京リージョンで発生した事象です）Glueのリソースは先に述べたDPUという単位でコンピューティングされます。これを性能検証のために、数十DPUといった比較的大きめに確保しようとするとリソースが確保できず起動できなかったことが何度かありました。<br>2018.08時点ではGlueリソースをリザーブド化することもできず、設定レベルでの回避が難しい状態です。</p>
<p>東京リージョンでGlueが利用可能になったのは<a href="https://aws.amazon.com/jp/about-aws/whats-new/2017/12/aws-glue-is-now-available-in-the-asia-pacific-tokyo-aws-region/" target="_blank" rel="noopener">2017.12</a> と比較的新しく、今後も継続的にリソースの増強などが期待されるため、改善に向かうと予想しています。現時点では実行タイミングによっては確保が難しい場合があるようです。人気のサービスであるという証拠なのかもしれませんね。</p>
<p>他の時間帯・日本国外リージョンなどを試すことや、処理粒度をある程度細かくし急激に大きなDPUを確保しないようにするなどの工夫が必要になってきます。また、どうしても確実に実行できないと困る！という場合は、リザーブドインスタンスでEMRを利用するしか無いようです。</p>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>Glue（直訳：のり）とはよく名を付けたものだと感じています。<br>粗削りで膨大なデータを、使いやすい形に成形してあげる。つまり、データと後続のシステムをうまくつなぎ合わせることができるものがGlueです。</p>
<p>以下にあてはまる方はGlueの導入を考えてみたらどうでしょうか。</p>
<ul>
<li>ビッグデータの処理が必要</li>
<li>起動時間は常時ではなく短い</li>
<li>サーバーを立てる余裕がない</li>
<li>運用、保守する余裕がない</li>
<li>パイオニア精神がある</li>
</ul>
<p>実はGlueの記事はネット上にはまだ多くない状態です。<br>そのため、Glue開発を導いていきたいというパイオニア精神ある方におすすめの領域だと思います。</p>
<h1 id="次回のGlueの記事について"><a href="#次回のGlueの記事について" class="headerlink" title="次回のGlueの記事について"></a>次回のGlueの記事について</h1><p>次回の内容はGlueを用いた性能改善を予定しています。<br>皆さんの参考になれば光栄です。</p>
<p>Glueを検討の方はご気軽にご連絡ください。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/20180828/photo_20180828_01.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;みなさん、初めまして、お久しぶりです、こんにちは。&lt;br&gt;フューチャーアーキテクト2018年新卒入社、1年目エンジニアのTIG（Technology Inno
    
    </summary>
    
      <category term="BigData" scheme="https://future-architect.github.io/categories/BigData/"/>
    
    
      <category term="AWS" scheme="https://future-architect.github.io/tags/AWS/"/>
    
  </entry>
  
  <entry>
    <title>データベースマイグレーション ～OracleからPostgreSQLへ～　－第２回ー</title>
    <link href="https://future-architect.github.io/articles/20180809/"/>
    <id>https://future-architect.github.io/articles/20180809/</id>
    <published>2018-08-09T06:00:36.000Z</published>
    <updated>2018-08-09T05:31:32.626Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-初めに"><a href="#1-初めに" class="headerlink" title="1. 初めに"></a>1. 初めに</h2><p>こんにちは。Technology Innovation Groupの岸田です。</p>
<p>前回に引き続き、データベースのマイグレーションがテーマです。<br>第1回はデータベースマイグレーションについて流れやポイントとなる点について記載してきました。<br>第2回は以下3点について紹介します。</p>
<ul>
<li>移行支援ツールの紹介</li>
<li>実際にシステムで稼働しているデータベース環境とアプリケーション（SQL）を使った評価</li>
<li>データベースマイグレーションの検討の方針</li>
</ul>
<h2 id="2-移行支援ツール"><a href="#2-移行支援ツール" class="headerlink" title="2. 移行支援ツール"></a>2. 移行支援ツール</h2><p>データベースのマイグレーションに際しては、移行ツールがいくつか公開されているため、それを利用するのが良いでしょう。このようなツールを利用してマイグレーションの作業量を削減していくことになります。</p>
<table>
<thead>
<tr>
<th>#</th>
<th>ツール名称</th>
<th>説明</th>
<th>参照先</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>ora2pg</td>
<td>GPLにもとづくオープンソースフリーソフトウェア</td>
<td><a href="http://ora2pg.darold.net/" target="_blank" rel="noopener">ora2pgのHP</a></td>
</tr>
<tr>
<td>2</td>
<td>AWS Schema Conversion Tool</td>
<td>AWSにて公開のツール</td>
<td><a href="https://docs.aws.amazon.com/ja_jp/SchemaConversionTool/latest/userguide/CHAP_Welcome.html" target="_blank" rel="noopener">AWS Schema Conversion Tool</a></td>
</tr>
</tbody>
</table>
<p>簡単ではありますが、2つの移行ツール(ora2pg/AWS Schema Conversion Tool)について紹介します。<br>移行ツールは現利用環境のデータベース定義を別環境に再現（ライセンスに注意！）し、再現環境に接続してツールを実行します。<br>もちろん、現利用環境に直接接続できる場合は別環境を用意する必要はございません。</p>
<h3 id="2-1-ora2pg"><a href="#2-1-ora2pg" class="headerlink" title="2-1 ora2pg"></a>2-1 ora2pg</h3><p>ora2pgはOracle/MySQLからPostgreSQLへの移行を支援するツールです。動作環境はLinuxとなります。</p>
<h4 id="できること"><a href="#できること" class="headerlink" title="できること"></a>できること</h4><p>ora2pgでは以下を行うことが可能です。</p>
<ul>
<li>スキーマ定義のコンバージョン用DDLの作成<ul>
<li>接続した環境からOracleデータベースにあるオブジェクト定義からPostgreSQLで実行可能なDDLの出力</li>
</ul>
</li>
<li>スキーマコンバージョンのレポート出力<ul>
<li>PostgreSQLに変換する際の評価レポート</li>
</ul>
</li>
<li>データ移行SQLの作成<ul>
<li>PostgreSQLで実行可能なINSERT文またはCOPY文の形式によるDMLの出力</li>
</ul>
</li>
<li>SQLファイルのコンバージョン</li>
</ul>
<p>ora2pgの具体的な利用方法は<a href="https://ora2pg.darold.net/documentation.html" target="_blank" rel="noopener">こちら</a>を参照してください。<br>参考までに、本ブログではora2pgはバージョン18.2を利用して確認しております。</p>
<p>コンバージョンの実行は、confファイル内で<code>TYPE</code>句にて指定し、個々に確認していきます。インストール時に用意されているconfファイル内にTYPEで指定できるキーワードが記載されております。（SYNONYMとDBLINKは記載がありませんが、指定が可能です。）<br>TYPEで指定できるキーワードとしては大きく3つの種類に分けられます。</p>
<table>
<thead>
<tr>
<th style="text-align:left">種別</th>
<th style="text-align:left">キーワード</th>
<th style="text-align:left">説明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">TABLE</td>
<td style="text-align:left">テーブル、インデックス、制約等のテーブルに関連するオブジェクト</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">PACKAGE</td>
<td style="text-align:left">パッケージ ⇒ シノニム＋ファンクションに変換</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">VIEW</td>
<td style="text-align:left">ビュー</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">GRANT</td>
<td style="text-align:left">オブジェクトの権限</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">SEQUENCE</td>
<td style="text-align:left">順序</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">TRIGGER</td>
<td style="text-align:left">トリガー</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">FUNCTION</td>
<td style="text-align:left">ファンクション</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">PROCEDURE</td>
<td style="text-align:left">プロシージャ ⇒ ファンクションに変換</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">TABLESPACE</td>
<td style="text-align:left">表領域</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">TYPE</td>
<td style="text-align:left">タイプ</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">PARTITION</td>
<td style="text-align:left">パーティション表 ⇒ 親子型のパーティション表に変換</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">FDW</td>
<td style="text-align:left">外部表</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">MVIEW</td>
<td style="text-align:left">マテリアライズド・ビュー</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">KETTLE</td>
<td style="text-align:left">XMLテンプレート</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">SYNONYM</td>
<td style="text-align:left">シノニム</td>
</tr>
<tr>
<td style="text-align:left">オブジェクト</td>
<td style="text-align:left">DBLINK</td>
<td style="text-align:left">データベース・リンク</td>
</tr>
<tr>
<td style="text-align:left">データ</td>
<td style="text-align:left">INSERT</td>
<td style="text-align:left">データ登録DML（INSERT形式）</td>
</tr>
<tr>
<td style="text-align:left">データ</td>
<td style="text-align:left">COPY</td>
<td style="text-align:left">データ登録DML（COPY形式）</td>
</tr>
<tr>
<td style="text-align:left">SQL</td>
<td style="text-align:left">QUERY</td>
<td style="text-align:left">SQL文の変換</td>
</tr>
</tbody>
</table>
<p>スキーマコンバージョンのレポートは以下のようなコマンドとなります。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/usr/local/bin/ora2pg -c /etc/ora2pg/ora2pg.conf -t SHOW_REPORT --estimate_cost --dump_as_html &gt; report.html</div></pre></td></tr></table></figure></p>
<p>レポートの出力結果例は以下です。</p>
<p><img src="/images/20180809/ora2pg_report.png" alt=""></p>
<p>ora2pgのSQLの変換については、ファイル単位にコンバートしていくため、各SQLファイルに対してora2pgを実行する必要があります。多少手間ではありますが、シェルを組んでディレクトリ内のファイル（例えば拡張子がsqlのものとか）に対してora2pgを実行することになるでしょう。また、コンバートできないものについてはエラーが出力されることなく、そのままの記載内容でアウトプットファイルが出力されますので、個々にコンバージョン結果の確認が必要となります。</p>
<h3 id="2-2-AWS-Schema-Conversion-Tool-AWS-SCT"><a href="#2-2-AWS-Schema-Conversion-Tool-AWS-SCT" class="headerlink" title="2-2 AWS Schema Conversion Tool(AWS-SCT)"></a>2-2 AWS Schema Conversion Tool(AWS-SCT)</h3><p>AWS-SCTは、AWSが提供しているツールです。オンプレのデータベースシステムをAWSクラウドへシフトする支援用のツールとなっていまして、変換可能なデータベースが多数存在します。<br>変換可能なデータベースの対応表は以下の通りです。</p>
<table>
<thead>
<tr>
<th style="text-align:left">変換元データベース</th>
<th style="text-align:left">変換先データベース</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">MS SQL Server</td>
<td style="text-align:left">MySQL/PostgreSQL/Redshift</td>
</tr>
<tr>
<td style="text-align:left">MySQL</td>
<td style="text-align:left">PostgreSQL</td>
</tr>
<tr>
<td style="text-align:left">Oracle</td>
<td style="text-align:left">MySQL/PostgreSQL/Redshift</td>
</tr>
<tr>
<td style="text-align:left">PostgreSQL</td>
<td style="text-align:left">MySQL</td>
</tr>
<tr>
<td style="text-align:left">Greenplum</td>
<td style="text-align:left">Redshift</td>
</tr>
<tr>
<td style="text-align:left">Netezza</td>
<td style="text-align:left">Redshift</td>
</tr>
<tr>
<td style="text-align:left">Vertica</td>
<td style="text-align:left">Redshift</td>
</tr>
</tbody>
</table>
<p>AWS-SCTを起動して、プロジェクト（作業領域）をオープンすると、変換元と変換先を選択することになります。<br>例えば、接続元がOracle(OLTP)を選択すると接続先で選べるのは、以下となっております。</p>
<ul>
<li>Amazon RDS for MySQL</li>
<li>Amazon Aurora (MySQL compatible)</li>
<li>Amazon RDS for PostgreSQL</li>
<li>Amazon Aurora (PostgreSQL compatible)</li>
<li>Amazon RDS for Oracle</li>
</ul>
<h4 id="できること-1"><a href="#できること-1" class="headerlink" title="できること"></a>できること</h4><p>AWS-SCTでは以下を行うことが可能です。</p>
<ul>
<li>スキーマ定義のコンバージョン用DDLの作成<ul>
<li>接続した環境からOracleデータベースのオブジェクト定義からPostgreSQLで実行可能なDDLの出力</li>
</ul>
</li>
<li>スキーマコンバージョンのレポート出力<ul>
<li>PostgreSQLに変換する際の評価レポート</li>
</ul>
</li>
<li>変換先データベースに対するスキーマの作成<ul>
<li>上記DDLを実行して変換先のデータベースにスキーマ及びオブジェクトを作成</li>
</ul>
</li>
<li>SQLファイルのコンバージョン</li>
</ul>
<p>AWS-SCTはGUIでの操作なのでユーザフレンドリーですね。また、レポートについても画面で表示されるため、イメージしやすいです。<br>スキーマのコンバージョンレポートイメージは以下。</p>
<p><img src="/images/20180809/AWS-SCT_user_report.png" alt=""></p>
<p>以下はオブジェクトの詳細イメージ。</p>
<p><img src="/images/20180809/AWS-SCT_object_report.png" alt=""></p>
<p>緑は自動変換可能、グレーっぽいのは単純変換で対応可能、黄色は多少手を加える必要がある、赤は要見直しという感じに考えていただければよいです。</p>
<p>AWS-SCTの中でとても便利な機能はアプリケーションのコンバージョンです。<br>Windows上の指定したフォルダ配下に格納されているファイル（もちろんサブフォルダも対象）の中からSQLを抽出してコンバージョンを評価します。ファイルの形式はJAVA/C++/C#といったものが選択できます。もちろんSQLのみが記載されているファイルでもOKです。スキーマのコンバージョン同様に抽出したSQL単位に対応内容をグルーピングします。レポート結果イメージは以下です。</p>
<p><img src="/images/20180809/AWS-SCT_sql_report.png" alt=""></p>
<p>また、各SQL毎にGUI上でツールによって変換された内容を見ることができます。</p>
<p>下図の例では、4つのSQL文が記述されているファイルに対するコンバージョン結果を表示しております。上部の左上の欄にはソースファイルの内容が表示されます。その下の枠には、ソースファイルから抽出されたSQL文が表示されます。1つのファイルに複数のSQL文が記載されていた場合は、SQLを選択することで個別に表示されます。抽出されたSQLの右の枠にPostgreSQL用に変換されたSQLが表示されます。その枠の上部のApplyボタンで元ファイルへの反映ができ、右上の枠内に反映結果が表示されます。その枠の上部のSaveボタンで反映結果を元のファイルへ保存することも可能です。現時点では評価したファイルすべてに対して一括反映⇒一括保存ができるような機能は無いと思われますので、コンバージョン結果の反映は面倒ですね。（そのような機能があるのかもしれませんが見つけられてません。）</p>
<p><img src="/images/20180809/AWS-SCT_sql_conv.png" alt=""></p>
<p>自動変換ができないものについては、各SQL単位にアドバイスが出力されます。そのアドバイスから、手動でメンテナンスを実施していくことになります。</p>
<h2 id="3-移行支援ツール検証"><a href="#3-移行支援ツール検証" class="headerlink" title="3. 移行支援ツール検証"></a>3. 移行支援ツール検証</h2><p>実際にとあるシステムのDB環境及びアプリケーションに対して移行ツールを使ってみました。</p>
<h3 id="3-1-テーブルのコンバージョン"><a href="#3-1-テーブルのコンバージョン" class="headerlink" title="3-1 テーブルのコンバージョン"></a>3-1 テーブルのコンバージョン</h3><p>実際のテーブルのデータ型についてはツールにより自動変換されます。ora2pgではconfファイル内で、変換ルールを定義することも可能です。<br>あくまでもルールに基づいた変換となっておりますので、システム内での標準化基準があるようでしたら、そちらに従うようにしてください。<br>それぞれのツールでのデフォルトの自動変換ルールは以下の表のようになっております。</p>
<table>
<thead>
<tr>
<th style="text-align:left">属性</th>
<th style="text-align:left">Oracleのデータ型</th>
<th style="text-align:left">ora2pgでの変換後のデータ型</th>
<th style="text-align:left">AWS-SCTでの変換後のデータ型</th>
<th>備考</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">文字列</td>
<td style="text-align:left">CHAR(n)</td>
<td style="text-align:left">char(n)</td>
<td style="text-align:left">character(n)</td>
<td>PostgreSQLのnは文字数</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NCHAR(n)</td>
<td style="text-align:left">char(n)</td>
<td style="text-align:left">character(n)</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">VARCHAR2(n)</td>
<td style="text-align:left">varchar(n)</td>
<td style="text-align:left">character varying(n)</td>
<td>PostgreSQLのnは文字数</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NVARCHAR2(n)</td>
<td style="text-align:left">varchar(n)</td>
<td style="text-align:left">character varying(n)</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">CLOB</td>
<td style="text-align:left">text</td>
<td style="text-align:left">text</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">LONG</td>
<td style="text-align:left">text</td>
<td style="text-align:left">text</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">数値</td>
<td style="text-align:left">NUMBER</td>
<td style="text-align:left">bigint</td>
<td style="text-align:left">double</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n)</td>
<td style="text-align:left">smallint</td>
<td style="text-align:left">numeric(n,0)</td>
<td>n=1～4</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n)</td>
<td style="text-align:left">integer</td>
<td style="text-align:left">numeric(n,0)</td>
<td>n=5～9</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n)</td>
<td style="text-align:left">bigint</td>
<td style="text-align:left">numeric(n,0)</td>
<td>n=10～19</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n)</td>
<td style="text-align:left">decimal</td>
<td style="text-align:left">numeric(n,0)</td>
<td>n=20～38</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n,m)</td>
<td style="text-align:left">real</td>
<td style="text-align:left">numeric(n,m)</td>
<td>n=2～6</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n,m)</td>
<td style="text-align:left">double precision</td>
<td style="text-align:left">numeric(n,m)</td>
<td>n=7～15</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">NUMBER(n,m)</td>
<td style="text-align:left">decimal</td>
<td style="text-align:left">numeric(n,m)</td>
<td>n=16～38</td>
</tr>
<tr>
<td style="text-align:left">日付</td>
<td style="text-align:left">DATE</td>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">timestamp</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">TIMESTAMP</td>
<td style="text-align:left">timestamp</td>
<td style="text-align:left">timestamp</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">バイナリ</td>
<td style="text-align:left">BLOB</td>
<td style="text-align:left">bytea</td>
<td style="text-align:left">bytea</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">RAW</td>
<td style="text-align:left">bytea</td>
<td style="text-align:left">bytea</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">その他</td>
<td style="text-align:left">ROWID</td>
<td style="text-align:left">oid</td>
<td style="text-align:left">character(255)</td>
<td></td>
</tr>
</tbody>
</table>
<p>特殊型については個別に御確認ください。</p>
<h3 id="3-2-アプリケーションソースのコンバージョン"><a href="#3-2-アプリケーションソースのコンバージョン" class="headerlink" title="3-2 アプリケーションソースのコンバージョン"></a>3-2 アプリケーションソースのコンバージョン</h3><p>スキーマ定義のコンバージョンは一定ルールで変更できるのが確認できましたが、マイグレーション検討を行うにあたって一番の壁がアプリケーションソースのコンバージョンです。SQLは各製品で互換性がない部分があるためソース(SQL)を一つ一つ確認していく必要があります。そのためソースの全量を把握することがまず最初の第一歩となります。</p>
<p>今回利用したシステムのアプリケーション(java)はDBに対するクエリはjavaソース内に記載されているのではなく、SQLごとにファイルが作成される実装方式のためSQLファイルを1か所に纏めてコンバージョンの検証を実行することができました。SQLの本数は1,150本です。</p>
<p>ora2pgはアプリケーションコードのファイル単位に実行が必要であり、また実行結果も目視確認しなければならないことから、今回はAWS-SCTの実行事例をご紹介致します。</p>
<p>SQLのコンバージョン結果レポートを出力させると<strong>SQLConversion Actionsタブ</strong>にコンバージョン評価に対するアドバイスの一覧が表示されます。各ISSUEが発生しているSQL数が<strong>Number of occurrences</strong>として記載されており、▶をクリックすることでそのISSUEが発生しているファイルの一覧を表示することができますので、どのSQLでどのISSUEが発生しているか一目でわかるようになっています。また、各ISSUEの部分を見てわかる通り、ISSUEに関連した記載のあるマニュアルのURLも表示されていますので、実際にどのように直すかの参考になります。</p>
<p><img src="/images/20180809/AWS-SCT_sql_conv_detail.png" alt=""></p>
<p>今回コンバージョン検証をでてきた中で見直さなくてはいけないものとしてはどのようなものがあったのでしょう。少し見ていきます。</p>
<h3 id="SQL自動変換"><a href="#SQL自動変換" class="headerlink" title="SQL自動変換"></a>SQL自動変換</h3><p>前回のブログにて記載した内容（外部結合・DECODE関数・HINT句）については自動的に変換されます。<br>加えて、Oracleでは省略が可能な表や列の別名の定義の際に記載する<code>AS</code>の補完等も実施されます。<br>注意が必要なのは、AWS-SCTでの変換は変換後のデータベースをRDSを想定しているため、RDSで用意されているOracle互換用のスキーマ（aws_oracle_ext）で代替が可能な記載については自動的に変換されます。例えば、<code>SYSDATE</code>、<code>ADD_MONTHS</code>、<code>TO_DATE</code>、<code>TO_CHAR</code>等があります。</p>
<h4 id="UPDATE文"><a href="#UPDATE文" class="headerlink" title="UPDATE文"></a>UPDATE文</h4><p>UPDATE文については2種類のISSUEとして挙がってきています。<code>5065</code>と<code>5608</code>です。改修ポイントとして以下に纏めます。それぞれ、最新のPostgreSQLでは修正点は少なくて済みます。</p>
<table>
<thead>
<tr>
<th style="text-align:left">ISSUE番号</th>
<th style="text-align:left">メッセージ</th>
<th style="text-align:left">詳細内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">5065</td>
<td style="text-align:left">PostgreSQL doesn’t support the UPDATE statement for a subquery</td>
<td style="text-align:left">UPDATEの対象テーブルにサブクエリ(VIEW)を指定することができない　⇒　9.1以降であればCTE等を使って改修してください。</td>
</tr>
<tr>
<td style="text-align:left">5608</td>
<td style="text-align:left">Unable to convert the UPDATE statement with multiple-column subquery in SET clause</td>
<td style="text-align:left">SET句に複数行を指定し、更新後の値をサブクエリで指定する方法はサポートされていない  → 9.5以降で記載できるようになったため、そのままで実行可能となります。</td>
</tr>
</tbody>
</table>
<h4 id="結果相違"><a href="#結果相違" class="headerlink" title="結果相違"></a>結果相違</h4><p>OracleとPostgreSQLでの実行で検索結果が異なる可能性があります。<br>ここで挙がったSQLについては改修後のテストで重点的に新旧結果比較をすることをお奨め致します。</p>
<p>今回の検証で挙がった事象としては3種類ありました。</p>
<p>まずは、関数の<code>GREATEST</code>と<code>LEAST</code>です。検証結果としては<code>5271</code>と<code>5272</code>となります。<br>PostgreSQLだと関数で指定した列内にNULLが含まれるとNULLが返ります。そのため、COALESCE関数でNULLを変換する必要がでてきます。</p>
<p>次に、正規表現になります。検証結果としては<code>5617</code>となります。Oracleでは、<code>REGEXP_LIKE</code>関数となりますが、PostgreSQLにはそのような関数が無く、AWS-SCTでは<code>~</code>に自動変換されます。正規表現の判定がOracleとPostgreSQLで異なる可能性があり警告として出力されているものと考えております。</p>
<h4 id="変換不可"><a href="#変換不可" class="headerlink" title="変換不可"></a>変換不可</h4><p>移行支援ツールで自動変換が困難なものが、検証結果の<code>5340</code>、<code>5621</code>と<code>9996</code>にあたります。</p>
<p>MERGE文、PostgreSQLでは実装されていない事前定義関数で代替が困難なもの（今回はSUBSTRB関数）や複雑なSQL文のようなものとなります。コンバージョン結果としては何も変換されずにISSUEで挙がっている文言が記載されます。<br>参考までに、MERGE文については前回のブログを参照していただければ修正ポイントが分かります。</p>
<h4 id="変換結果まとめ"><a href="#変換結果まとめ" class="headerlink" title="変換結果まとめ"></a>変換結果まとめ</h4><p>AWS-SCTのSQLのコンバージョン検証結果を纏めると以下のようになります。</p>
<p><img src="/images/20180809/AWS-SCT_sql_conv_summary2.png" alt=""></p>
<p>AWS-SCTによる分類のうちSUCCESSとLOWがそこまで労力をかけない修正（変換）でPostgreSQLで実行可能となります。また、上記で記載したようにSET句でのサブクエリの指定はPostgreSQL 9.5以降では可能となっているため、そちらを加えると<strong>88.2%</strong>となります。HIGHと分類されている、行ロックについては第1回にて説明しておりますので、そちらを参考にしてセッションパラメータの設定にて対処可能ですので、大きな修正となるのはMERGE文、サブクエリの更新とSUBSTRB関数の実装のみとなり、全体の6%程度に過ぎません。</p>
<p>このように、ソース全体のうち修正が必要なSQL量をざっと確認することができ、かつ修正難易度の目安についても確認できることが分かりました。コンバージョン計画立案時のコスト、スケジュール算出にとしても有用なものとなりそうです。</p>
<h5 id="参考までに"><a href="#参考までに" class="headerlink" title="参考までに"></a>参考までに</h5><p>ora2pgについても、外部結合やDECODE等の変換ができていることは確認しております。驚いたのは、ora2pgの外部結合の変換はすべてLEFT OUTER JOINに統一されているところです。（AWS-SCTは(+)の記載位置によりRIGHT OUTER JOINの変換もあります。）すべてを目視確認する場合はora2pgのSQL変換でも良いかもしれません。</p>
<h2 id="3-データベースマイグレーション検討の方針"><a href="#3-データベースマイグレーション検討の方針" class="headerlink" title="3. データベースマイグレーション検討の方針"></a>3. データベースマイグレーション検討の方針</h2><p>データベースマイグレーションを検討する際には、作業項目毎マイグレーションの難易度をに評価して実現性を検討していくことになります。どのようなシステムがマイグレーションがし易いのか考えてみます。</p>
<h3 id="データベースオブジェクト"><a href="#データベースオブジェクト" class="headerlink" title="データベースオブジェクト"></a>データベースオブジェクト</h3><p>データベースオブジェクトについての変換は第1回でも触れましたが、圧倒的にストアド・サブプログラムが難易度が高いです。その他についてはPostgreSQLで実装されていないOracle固有のオブジェクトを利用している場合は検討が必要となってきます。</p>
<h3 id="アプリケーションソース"><a href="#アプリケーションソース" class="headerlink" title="アプリケーションソース"></a>アプリケーションソース</h3><p>アプリケーションについては、ソースがファイルとして保持しているようであれば、変換ツールを施行することが可能となります。そのため、ソースファイルのような形でツールが利用できない場合は難易度が高くなります。動的にSQLを組んで実行する形やアプリケーション内にSQLが分散している場合などがそれにあたります。</p>
<p>また、ストアド・プログラムが存在する場合は、コンバージョン及びテストの難易度が上がります。<br>PostgreSQLのPL/pgSQLでファンクションとして作成することは可能ですが、コンバージョンのタイミングでアプリケーション側にロジックを寄せる検討をすることも視野に入れてください。<br>PL/SQLをPL/pgSQLにコンバージョンする際に気を付ける点としては、以下となります。</p>
<ul>
<li>PL/pgSQL関数内部ではCOMMITが使用できない</li>
<li>エラーが発生した場合は内部処理はすべてロールバックされる</li>
<li>パッケージが存在しないため、関数を跨った変数を定義できない</li>
<li>変数はすべて宣言する必要がある</li>
</ul>
<p>データベースへの接続方法としては、OracleもPostgreSQLも変わらないため特に意識する必要はありません。</p>
<table>
<thead>
<tr>
<th style="text-align:left">難易度</th>
<th style="text-align:left">アプリケーション実装方法</th>
<th style="text-align:left">システム例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>高</strong></td>
<td style="text-align:left">PL/SQLの実行／動的SQL</td>
<td style="text-align:left">バックグラウンドでの集計処理メインのシステム、分析用の検索システム</td>
</tr>
<tr>
<td style="text-align:left"><strong>低</strong></td>
<td style="text-align:left">実行されるSQLが外出しされている</td>
<td style="text-align:left">定型フォームのオンライン検索システム</td>
</tr>
</tbody>
</table>
<h3 id="非機能要件"><a href="#非機能要件" class="headerlink" title="非機能要件"></a>非機能要件</h3><p>これまでは、アプリケーションについて触れてきましたが、PostgreSQLはシステム内ではデータベースサーバとして稼働していくことになりますので、非機能要件についても要件を満たすかどうかを検討していくことになります。</p>
<p>現行システムで可用性要件や性能要件のためにOracle RAC構成を取っている場合があると思います。<br>Oracle RACはShared Everythingのデータベース構成となっていて、複数台のサーバがアクティブ状態で1つのデータベースを共有して稼動しますので、単体のサーバ障害ではシステムが停止しないということが大きな特徴です。また、複数台がアクティブに稼働しておりますので、分散処理も可能となります。  この要件が崩せない場合は、やはりOracleからのマイグレーションは難しくなります。</p>
<p>PostgreSQLでは、可用性を上げるために3rdベンダのクラスタウェアを利用したActive-Standby構成が考えられます。また、分散処理の一例としてレプリケーション機能を利用したMaster-Slave構成として参照処理をSlave側で実行させるといったことも考えられます。</p>
<p>その他、運用（バックアップ／監視／セキュリティ）等、非機能要件を確認したうえで、マイグレーションの評価をしていってください。OracleのEnterprise Editionの場合、PostgreSQLにはない機能が豊富にふくまれているため細かく確認が必要です。</p>
<p>クラウドサービスの場合、可用性（レプリケーション）、運用（バックアップ、監視、セキュリティ）がDB機能として提供されています（例えば、AWS RDS）のでこの部分のハードルはぐっと低くなると考えています。</p>
<h3 id="データ移行"><a href="#データ移行" class="headerlink" title="データ移行"></a>データ移行</h3><p>データベースのマイグレーションではなくても、システムの刷新時には検討されるのが、データの移行です。すいません、データ移行は非常に様々な検討項目があるため、書くとなると1回分くらいになってしまうかと思いますので、第2回のブログでも割愛させていただきます。<br>ただ、マイグレーションの難易度の評価は割愛せずに必ず実施してください。</p>
<h2 id="4-最後に"><a href="#4-最後に" class="headerlink" title="4. 最後に"></a>4. 最後に</h2><p>ここまでデータベースのマイグレーション（PostgreSQL）について確認してきましたが、ツールを利用したマイグレーションが有用であることが確認できました。また、実際のコンバージョンだけでなくプロジェクト計画を立てる際の一次評価としても有用と言えます。</p>
<p>第1回・第2回とはデータベースのマイグレーションに特化した内容で記載してきましたが、PostgreSQLへのマイグレーションの判断が下された後には、もちろんPostgreSQLとしてのデータベース設計は必要です。パラメータや配置などの物理設計や統計情報取得、VACUUM処理、バックアップ等の運用設計もしっかりとやっていきましょう。</p>
<p>フューチャーでは仮想的なDBチームが形成されており、各プロジェクトへの横断的に支援しております。OracleやPostgreSQLを中心としたノウハウの蓄積・共有なども活発に行っていますので、ご興味ある方は是非一緒に働いていきましょう！！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-初めに&quot;&gt;&lt;a href=&quot;#1-初めに&quot; class=&quot;headerlink&quot; title=&quot;1. 初めに&quot;&gt;&lt;/a&gt;1. 初めに&lt;/h2&gt;&lt;p&gt;こんにちは。Technology Innovation Groupの岸田です。&lt;/p&gt;
&lt;p&gt;前回に引き続き、デ
    
    </summary>
    
      <category term="DB" scheme="https://future-architect.github.io/categories/DB/"/>
    
    
      <category term="Migration" scheme="https://future-architect.github.io/tags/Migration/"/>
    
      <category term="PostgresSQL" scheme="https://future-architect.github.io/tags/PostgresSQL/"/>
    
      <category term="Oracle" scheme="https://future-architect.github.io/tags/Oracle/"/>
    
  </entry>
  
  <entry>
    <title>人工知能学会（JSAI2018）参加報告</title>
    <link href="https://future-architect.github.io/articles/20180723/"/>
    <id>https://future-architect.github.io/articles/20180723/</id>
    <published>2018-07-23T04:56:36.000Z</published>
    <updated>2018-07-23T05:11:28.147Z</updated>
    
    <content type="html"><![CDATA[<h2 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h2><p>みなさんこんにちは。SAIG(Strategic AI Group)の小池です。<br>前回の<a href="https://future-architect.github.io/articles/20180222/">NIPS2017 LT報告</a>以来の登場となります。<br>今回は、2018年6月5日(火)〜6月8日(金)に行われました人工知能学会（JSAI2018）にSAIGで参加して来ましたので、ご報告を致します。フューチャーでは、プラチナスポンサーとしてのブース出展・口頭発表・セッション聴講を行ってまいりました。</p>
<p><img src="/images/20180723/photo_20180723_01.png" class="img-middle-size"></p>
<p>場所は鹿児島県城山ホテル鹿児島にて開催されました。鹿児島空港から鹿児島中央駅までバスで1時間くらい、鹿児島中央駅から城山ホテル鹿児島まで30分程かかりました。会場は山の上にあり移動が大変でしたが、景観が良く、高級感溢れるホテルでした。</p>
<h2 id="口頭発表"><a href="#口頭発表" class="headerlink" title="口頭発表"></a>口頭発表</h2><p>SAIGの貞光九月がAI応用-産業応用(3)にて、「<a href="https://confit.atlas.jp/guide/event/jsai2018/subject/2M2-04/detail?lang=ja" target="_blank" rel="noopener">半教師有りグラフニューラルネットワークを用いたCRUD関係に基づくシステム移行単位の最適化</a>」と題して発表してまいりました。</p>
<h2 id="口頭発表の内容"><a href="#口頭発表の内容" class="headerlink" title="口頭発表の内容"></a>口頭発表の内容</h2><p>大規模なシステムが、社会的あるいは工学的観点で旧式化した際、システムの最新化が必要となります。例えば銀行のシステムが銀行の合併により改修が必要となるといった事例が代表的です。旧システムを無計画に改修していては、効率も悪く、むしろ前よりパフォーマンスを悪化することさえあるでしょう。そこで、どのようなサブセットでシステムを移行すべきか、という「移行単位」を人手で設計します。この際用いるのがCRUD表です。CRUD表とは、図1のように、システム内の機能とテーブルの参照関係を表で表したものです。CRUD表を見ることで、例えば一つのテーブルをcreateする機能と、削除する機能は、同じ移行単位とすべき、といった方針が見いだせます。</p>
<p><img src="/images/20180723/photo_20180723_02.png"><br>図1　CRUD表の例</p>
<p>本研究ではこのCRUD表から自動的に移行単位を推定することを試みています。CRUD表がグラフに対する隣接行列と解釈できることに着目し、半教師あり学習に基づくグラフノードの分類法を用います。具体的には<a href="https://papers.nips.cc/paper/6212-diffusion-convolutional-neural-networks" target="_blank" rel="noopener">Diffusion Convolutional Neural Network</a>を用い、機能とテーブルのファイル配置情報、いわゆるファイルパスを、CRUDグラフと統合して用います。さらにファイルパスの木構造の特性を活かすことができる<a href="https://arxiv.org/pdf/1705.08039.pdf" target="_blank" rel="noopener">Poincarē Embeddings</a>を併用することで、従来法に比べ、人手の移行単位に最も近い結果を得ています(図2)。</p>
<p><img src="/images/20180723/photo_20180723_03.png"></p>
<p>図2 CRUD情報とファイルパスの統合イメージおよびその手法の実験結果</p>
<h2 id="企業ブース"><a href="#企業ブース" class="headerlink" title="企業ブース"></a>企業ブース</h2><p>企業ブースでは、FutureにおけるAI案件の実績を展示いたしました。多くの学生さん、企業の方に来ていただき、大変盛り上がりました。Futureオリジナル清涼タブレットを配っていたのですが、3日を通して払底するなど多くの方に来ていただきました。</p>
<p><img src="/images/20180723/photo_20180723_04.jpeg" class="img-middle-size"></p>
<h2 id="セッション聴講"><a href="#セッション聴講" class="headerlink" title="セッション聴講"></a>セッション聴講</h2><p>口頭発表やインタラクティブセッションに参加してきました。その中でも、面白かったと思うものを紹介したいと思います。(以下の要約には、SAIGメンバーである貞光、藤田、勝村の3名にお手伝いいただきました）</p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/2C1-05/detail?lang=ja" target="_blank" rel="noopener">Word2Vecにおける出力側の重みに注目した文書分類手法の検討　(内田ら)</a></li>
</ul>
<p>近年言語処理において、word2vecのような分散表現が注目されています。分散表現は文書のクラスタリングや検索タスクにおいて、強力に働きます。一般的なword2vecにおける獲得手法は、3層のニューラルネットを用いて第1層の重み（W in)を学習させて利用します。一方で本研究では、第2層の重み(w out)も共起情報量があることからw in, w out両方を用いるべきだと主張しています。W in W outの両ベクトルの平均をとって単語ベクトルとすることで、分類タスクにおいて従来のw2vよりも精度が上がることを示しています。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/2J2-02/detail?lang=ja" target="_blank" rel="noopener">位置情報データによる競合店舗の利用状況の多様性を用いた購買予測手法の提案　（新美ら）</a></li>
</ul>
<p>実店舗での購買を把握する手法として、アンケートを用いた市場調査や、家計簿アプリのレシートスキャン等の利用が試みられてきましたが、これらは回答の偏りや調査対象者の負担という課題があります。<br>本研究では、①大手スーパーマーケットチェーンT社のID-POSデータ、②T社各店舗、および競合と想定される近隣の複数店舗に設定したジオフェンス（※）内への消費者の出入情報をマッチングさせる方法を提案しています。<br>※地域を特定の距離で区切った仮想の区画のこと。消費者が特定の複数のアプリケーションを利用した際にバックグラウンドで取得される位置情報が一般的であるが、GPSに基づく正確な位置情報ではなく、ジオフェンス内への侵入・退出を記録する方法を採用しています。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/1E2-03/detail?lang=ja" target="_blank" rel="noopener">深層学習を用いたユーザ離脱予測 (宮崎ら)</a></li>
</ul>
<p>ユーザ離脱予測に関する既存研究ではユーザ属性情報や行動データなどの特徴量を機械学習のモデルに読み込ませ、将来の一定期間における離脱を予想するものであり、Random ForestやSVMなど様々な手法が使われています。一方で深層学習を用いたユーザ離脱予測に関する研究は多くありません。本研究で、はユーザの離脱予測モデルに時系列相関を取り込むためにLSTMを採用し、既存手法であるRandom Forestと時系列性を考慮しない中間層が4層のMLPとの比較し、既存手法より提案手法が精度が高いことを示しました。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/3A1-03/detail?lang=ja" target="_blank" rel="noopener">低解像度の料理画像を超解像するためのSRGANの応用 (永野ら)</a></li>
</ul>
<p>Deep Learningによって目覚ましい発展を遂げた研究内容の一つに超解像が挙げられます。<br>超解像に関して様々なモデルが提案される中、人間にとっての見た目を評価するMOStestingにおいて他の超解像手法を凌ぐ性能を発揮しているものが、SRGAN[Leding 16]です。しかしSRGANは元画像のjpgノイズや撮影の際に生じたブレを修復できない問題や撮影対象「らしい」テクスチャが生成されない問題があります。本研究では料理画像を対象にこれらの問題に対して、解きたい問題に応じた適切なデータセットを構築するといった基本的なアイデアに基づき定性的・定量的な実験を実施した。SRGANに対しノイズを加えた低解像度画像の作成やドメイン毎にデータを分割してモデルを作成・比較することで、性能が向上することを確認しています。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/3E1-04/detail?lang=ja" target="_blank" rel="noopener">化学構造式のためのハイパーグラフ文法 (梶野)</a></li>
</ul>
<p>任意の特徴を持つ化学分子を設計する際、近年SMILESと呼ばれる文字列表現を元に、VAEを用いる手法が提案されています。(Gomez-Bombarelli 2018) しかしSMILESとVAEを組み合わせた時の欠点として、出力された文字列が分子グラフの要件を満たさないという課題がありました。著者らは独自に、常に原子価を保持するという条件を担保する分子ハイパーグラフ構造(HRG)を定義し、今後VAEを組み合わせることで分子設計が容易になると主張しています。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/3E1-01/detail?lang=ja" target="_blank" rel="noopener">BPE Sequence to Sequence を利用した単変異によるタンパク質耐熱性変化予測　（河野ら）</a></li>
</ul>
<p>工業的に有用なタンパク質の耐熱性を向上させることを目的に、タンパク質を構成する数百のアミノ酸の一つを別のアミノ酸（全20種類）に置換する方法がありますが、実際にどのアミノ酸を変化させると効果があるのか予測が難しいといった課題があります。<br>本研究では、アミノ酸配列全体に関する情報を十分に含む特徴量として、Byte Pair Encoding(BPE) Seq2Seqモデルに、既知の特徴量（実験条件やアミノ酸の性質など）を組み合わせたものを提案し、既存法より高い精度で耐熱性変化を予測できることを示しています。<br><br></p>
<ul>
<li><a href="https://confit.atlas.jp/guide/event/jsai2018/subject/1P3-05/detail?lang=ja" target="_blank" rel="noopener">CADソフトの操作ログ分析による操作スキルの抽出 (新實ら)</a></li>
</ul>
<p>CADの手順操作ログから、決定木を使ってベテランのスキルを検知するという内容。<br>JSAIではこのように、ある程度枯れた技術を産業応用していく、という発表が多くありましたが、本研究は研究課題が明確かつ、有益な効果が得られた、ということがとても分かりやすく示されていました。<br><br></p>
<h2 id="おわりに"><a href="#おわりに" class="headerlink" title="おわりに"></a>おわりに</h2><p>JSAI2018では、多くの収穫があり非常に有益だったと思います。<br>知識もさることながら、優秀な研究者・エンジニアの方との出会いや企業さんとの出会いもありました。<br>今後も学会やイベントに参加していきますので、見かけた際には気軽にお声をかけてくださればと思います。<br>以上小池でした。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h2&gt;&lt;p&gt;みなさんこんにちは。SAIG(Strategic AI Group)の小池です。&lt;br&gt;前回の&lt;a href=&quot;https:
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="MachineLearning8" scheme="https://future-architect.github.io/tags/MachineLearning8/"/>
    
  </entry>
  
  <entry>
    <title>データベースマイグレーション ～OracleからPostgreSQLへ～　ー第１回ー</title>
    <link href="https://future-architect.github.io/articles/20180529/"/>
    <id>https://future-architect.github.io/articles/20180529/</id>
    <published>2018-05-29T02:06:05.000Z</published>
    <updated>2018-05-29T02:44:12.077Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-初めに"><a href="#1-初めに" class="headerlink" title="1. 初めに"></a>1. 初めに</h2><p>こんにちは。Technology Innovation Groupの岸田です。</p>
<p>データベースシステムに対しては、高い信頼性・可用性・安定性が求められることから、データベースとしては、Oracle Database（以降はOracleと記載）やMicrosoft SQL Serverなどの商用製品が採用されてきました。</p>
<p>十数年前より、商用データベースの高いコストに不満を持つ企業において、OSSデータベースに注目が集まってきており、近年では基幹システムにおいてもOSSデータベースが採用されるケースも多くなっています。<br>このような背景から技術者としてもOSSデータベースのスキルは非常に求められている状況かと思います。</p>
<p>元々高額なライセンス料に加え、仮想基盤における課金体系の問題や2016年のOracle Database Standard Edition Oneの廃止などにより、基盤更改などを契機にOSS検討事例が増えてきました。<br>また、クラウド基盤を選択肢に入れた場合にも、柔軟にスケールアップが可能なOSSを採用したいという要望もあります。</p>
<p>OSSデータベースで広く採用されているのはPostgreSQLとMySQLがあります。今回は、エンタープライズ領域においてマイグレーション例が多いOracleからPostgreSQLについて、考慮すべき事項について2回にわたり紹介していきます。</p>
<h2 id="2-マイグレーションの流れ"><a href="#2-マイグレーションの流れ" class="headerlink" title="2. マイグレーションの流れ"></a>2. マイグレーションの流れ</h2><h3 id="データベースの選定（アセスメント）"><a href="#データベースの選定（アセスメント）" class="headerlink" title="データベースの選定（アセスメント）"></a>データベースの選定（アセスメント）</h3><p>単にコスト面に問題を抱えているからといって、安易にOSSデータベースへのマイグレーションを決断することは危険です。そのシステムにおいて提供しているサービスに関して、マイグレーションによって機能要件及び非機能要件を満たせなくなっては元も子もありません。</p>
<p>マイグレーションの検討については、OracleとPostgreSQLの技術的な検討だけでなく、コスト算定（コストメリット）、移行期間やテストなどで多方面での検討が必要となります。この検討については、以下のようにQCDの視点で検討を行う必要があります。</p>
<h4 id="Quality（品質）"><a href="#Quality（品質）" class="headerlink" title="Quality（品質）"></a>Quality（品質）</h4><p>「アプリケーション機能面」「システム非機能面」の両面について実現性を検討し、ノックアウト項目が存在しないかを評価します。システムによっては現行システムとのデータ連携や災害対策目的で別サイトへのレプリケーションを実現する必要が出てきます。この時点で抜け漏れが発生してしまわないように、現行の運用面を含めた細かい評価が必要となります。</p>
<h4 id="Cost（コスト）"><a href="#Cost（コスト）" class="headerlink" title="Cost（コスト）"></a>Cost（コスト）</h4><p>マイグレーションに関連するコスト評価をおこないます。以下の2点が考えられます。</p>
<ol>
<li>マイグレーションに関連する費用：「アプリケーションソース移行」「データベースミドルウェア自体の移行」「データ移行」  </li>
<li>新基盤での費用：「ソフトウェアライセンス、基盤費用」</li>
</ol>
<h4 id="Delivery（納期）"><a href="#Delivery（納期）" class="headerlink" title="Delivery（納期）"></a>Delivery（納期）</h4><p>マイグレーション実現にむけたスケジュールの評価を行います。基盤更改期限がある場合についはその期間内に実現可能かという評価が必要になります。</p>
<p>これらの検討結果を基にデータベースマイグレーションの判断を行います。</p>
<h3 id="マイグレーション決定後の作業イメージ"><a href="#マイグレーション決定後の作業イメージ" class="headerlink" title="マイグレーション決定後の作業イメージ"></a>マイグレーション決定後の作業イメージ</h3><p>データベースのマイグレーションが決定した後は、一例として以下のように進めていきます。</p>
<p><img src="/images/20180529/migration_phase.png"></p>
<p>赤字となっている作業については、実装されているアプリケーションにより作業量が大きく変動するものとなります。<br>データベースのマイグレーションの成功は、現状アプリケーションの正確な把握に直結すると考えておりますので、現状把握については専門技術者を交えてしっかりと実施することをお奨め致します。</p>
<h2 id="3-マイグレーションのポイント"><a href="#3-マイグレーションのポイント" class="headerlink" title="3. マイグレーションのポイント"></a>3. マイグレーションのポイント</h2><p>前章にてマイグレーションの流れを説明しましたが、この章では実際にマイグレーションを実施するにあたりポイントとなる点を記載していきます。ポイントとしては以下の3点となります。</p>
<ol>
<li><strong>スキーマ移行</strong><ul>
<li>データベースの内部環境（オブジェクト等）の移行</li>
<li>2章のDB設計・データベース移行（定義移行）にあたる</li>
</ul>
</li>
<li><strong>アプリケーション移行</strong><ul>
<li>アプリケーションプログラムの移行。主にSQL改修となる</li>
</ul>
</li>
<li><strong>データ移行</strong><ul>
<li>データベースに格納されているデータの移行</li>
</ul>
</li>
</ol>
<p>上記それぞれについてみていきます。（今回はデータ移行は紹介まで）</p>
<h3 id="3-1-スキーマ移行（データベースオブジェクト）"><a href="#3-1-スキーマ移行（データベースオブジェクト）" class="headerlink" title="3-1. スキーマ移行（データベースオブジェクト）"></a>3-1. スキーマ移行（データベースオブジェクト）</h3><p>2章で記載しましたDB設計フェーズにて方針を設計し、データベース移行フェーズにて実装致します。</p>
<p>Oracleに存在する主なオブジェクトについてPostgreSQLの存在有無は以下の通りです。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Oracle</th>
<th style="text-align:left">PostgreSQL</th>
<th style="text-align:left">備考</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">テーブル</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">インデックス</td>
<td style="text-align:left">△</td>
<td style="text-align:left">B-Treeインデックスとパーティションインデックスが存在。逆キーインデックス、ビットマップインデックス等は存在しない。</td>
</tr>
<tr>
<td style="text-align:left">ビュー</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">マテリアライズド・ビュー</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">シノニム</td>
<td style="text-align:left">×</td>
<td style="text-align:left">一部ビューで代替可能。</td>
</tr>
<tr>
<td style="text-align:left">シーケンス</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">トリガー</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">データベースリンク</td>
<td style="text-align:left">×</td>
<td style="text-align:left">dblink関数または、FDW(Foreign Data Wrapper) で代替可能。</td>
</tr>
<tr>
<td style="text-align:left">パッケージ</td>
<td style="text-align:left">×</td>
<td style="text-align:left">3-2で記載。</td>
</tr>
<tr>
<td style="text-align:left">プロシージャ</td>
<td style="text-align:left">×</td>
<td style="text-align:left">3-2で記載。</td>
</tr>
<tr>
<td style="text-align:left">ファンクション</td>
<td style="text-align:left">〇</td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<p>〇：存在　△：一部存在　×：無し</p>
<p>インデックスについては一部の種類のインデックスは存在しませんが、インデックス自体は性能要件を満たすために作成されていますので、インデックスの存在自体がマイグレーションに影響を及ぼすことはありません。ただし、マイグレーション後の性能要件を満たすために別の方法を検討する可能性があります。</p>
<p>その他につきましては、代替機能を含めてPostgreSQLにて実装ができると考えております。</p>
<p>ストアド・パッケージ／プロシージャ／ファンクションについては3-2.アプリケーション移行で記載致します。</p>
<h4 id="データ型の変換"><a href="#データ型の変換" class="headerlink" title="データ型の変換"></a>データ型の変換</h4><p>テーブルのコンバージョンの際には、データ型を意識する必要があります。OracleとPostgreSQLのデータ型の対応表は以下の通りです。数値型にはいくつかのデータ型が存在しますので、システムとしてルールを定めることをお奨め致します。</p>
<table>
<thead>
<tr>
<th style="text-align:left">属性</th>
<th style="text-align:left">Oracleのデータ型</th>
<th style="text-align:left">PostgreSQLのデータ型</th>
<th style="text-align:left">備考</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">文字列</td>
<td style="text-align:left">CHAR/NCHAR/VARCHAR2/NVARCHAR2</td>
<td style="text-align:left">char/varchar</td>
<td style="text-align:left">PostgreSQLの文字型の精度は文字数を意味する</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">CLOB/LONG</td>
<td style="text-align:left">text</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">数値</td>
<td style="text-align:left">NUMBER</td>
<td style="text-align:left">smallint/bigint/integer/decimal/real/double precision</td>
<td style="text-align:left">精度によってデータ型を選択</td>
</tr>
<tr>
<td style="text-align:left">日付</td>
<td style="text-align:left">DATE/TIMESTAMP</td>
<td style="text-align:left">timestamp</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">バイナリ</td>
<td style="text-align:left">BLOB/RAW</td>
<td style="text-align:left">bytea</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">その他</td>
<td style="text-align:left">ROWID</td>
<td style="text-align:left">oid</td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h3 id="3-2-アプリケーション（SQL）移行"><a href="#3-2-アプリケーション（SQL）移行" class="headerlink" title="3-2.アプリケーション（SQL）移行"></a>3-2.アプリケーション（SQL）移行</h3><p>アプリケーションの移行については、同じ製品のバージョンアップをする場合であっても一通りアプリケーションの動作確認（結果の現新一致確認）は実施することが多いと思います。</p>
<p>データベースのマイグレーションの場合は、このアプリケーションの動作確認において結果の不一致による原因究明やアプリケーションの見直しが発生する可能性が高くなることが考えられるため、当初よりアプリケーションの移行作業の作業量を多めに見積もることが大事だと思います。3章で説明したツールなどを利用して自動的なコンバージョンでアプリケーションの改修にかかる時間は削減できますが、アプリケーションの動作確認は必ず実施してください。</p>
<p>アプリケーションの動作確認と同様に重要なのは、性能テストとなります。データベースのマイグレーションでは、データベースの機能の差異がありますので、SQLの処理性能は事前の机上予測が難しいといえます。この性能テストについてもデータベースのバージョンアップ時のテスト作業量よりも多めに見積もっておくことをお奨め致します。</p>
<p>Oracleにて実装されているSQLについて、Oracle独自の記法であったとしてもPostgreSQLにて実装は可能であると考えておりますので、アプリケーション改修によりマイグレーションがNGとなることは無いと思います。</p>
<p>ただし、アプリケーションに依存して、改修作業や改修後の確認テストの作業量が変動していきますので注意が必要です。</p>
<p>アプリケーション移行についての考え方及び対策の一部を以下に紹介していきます。</p>
<h4 id="ストアド・サブプログラム変換"><a href="#ストアド・サブプログラム変換" class="headerlink" title="ストアド・サブプログラム変換"></a>ストアド・サブプログラム変換</h4><p>ストアド・サブプログラムは、Oracleでのストアド・パッケージ、ストアド・プロシージャ、ストアド・ファンクションの総称です。<br>PostgreSQLにはパッケージ及びプロシージャが存在しません。<br>Oracleのストアド・パッケージ及びストアド・プロシージャはPostgreSQLのファンクションで実装することになります。<br>プロシージャ、ファンクションの集合体であるパッケージはスキーマで代用します。<br>まとめると以下のようになります。</p>
<table>
<thead>
<tr>
<th style="text-align:left">Oracle</th>
<th style="text-align:left">PostgreSQL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">パッケージ</td>
<td style="text-align:left">スキーマ</td>
</tr>
<tr>
<td style="text-align:left">プロシージャ</td>
<td style="text-align:left">ファンクション</td>
</tr>
<tr>
<td style="text-align:left">ファンクション</td>
<td style="text-align:left">ファンクション</td>
</tr>
</tbody>
</table>
<p>PostgreSQLのファンクションでOracleと大きく異なる部分は、トランザクションの制御となります。PostgreSQLのファンクションは、呼び出し元のトランザクションに依存するため、ファンクション内でCOMMITの発行はできません。つまり、ファンクション内でエラーが発生した場合は、ファンクション内の処理はすべてロールバックします。</p>
<p>参考までに、Oracleでは、<code>PRAGMA AUTONOMOUS_TRANSACTION</code>を利用して呼び出し元とトランザクションを分離することができます。Oracleのストアド・サブプログラム内でトランザクションを分離している場合は、PostgreSQLではロジックを見直す必要があります。</p>
<h4 id="外部結合"><a href="#外部結合" class="headerlink" title="外部結合"></a>外部結合</h4><p>Oracle独自の記法としては外部結合が挙げられます。外部結合はSQLにおいて結合条件で対応するレコードが存在しない場合でも優先となるテーブルについてはレコードが除外されない結合方法です。</p>
<p>Oracle 9i以降はSQL標準である<code>[LEFT | RIGHT] OUTER JOIN</code>の記述がサポートされるようになり、オラクル社としても同バージョンからOUTER JOINによる記法を<a href="https://docs.oracle.com/cd/E82638_01/SQLRF/Joins.htm#GUID-29A4584C-0741-4E6A-A89B-DCFAA222994A" target="_blank" rel="noopener">マニュアル上でも推奨しています</a>。</p>
<p>とはいえ、Oracleで動かすSQLの外部結合は(+)表記をよく目にします。<br>主な理由としては以下じゃないかなと思ってます。</p>
<ul>
<li>Oracle 8i 以前に作成したSQLが今でも使われている。（バージョンアップを繰り返していて改修していない。）</li>
<li>以前プロジェクトとして作成していたシステムのSQLコーディング基準書では(+)表記で記載することが基準となっており、現状でも(+)表記自体は利用可能であるため、基準書自体を修正することができていない。また、成功したプロジェクトの基準書を横展開している。</li>
<li>プログラムの改修や新機能導入においても、現行で動作しているSQLを踏襲して作成する。</li>
</ul>
<p>PostgreSQLにおいて外部結合は当然SQL標準であるOUTER JOINの記載となりますので、データベースマイグレーションの際にはSQLのコンバージョンが必要です。</p>
<p>(+)表記での外部結合をコンバージョンするときに気を付けなければならないのは、リテラル値に対する外部結合の条件がある場合です。</p>
<p>言葉だけでは分かりづらいと思いますので、Oracleで用意されているサンプルスキーマ（SCOTTユーザ）で見ていきます。テーブルはEMP表とDEPT表を使います。デフォルトの状態から少しだけ値を変えているところはあります。</p>
<p><img src="/images/20180529/demo.png"></p>
<p>2種類の外部結合表記のSQL文を用意しました。</p>
<p>①リテラル条件にも「(+)」を付与したパターン</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> d.deptno, d.dname, e.empno, e.ename, e.comm</div><div class="line"><span class="keyword">FROM</span> emp e, dept d</div><div class="line"><span class="keyword">WHERE</span> e.deptno(+) = d.deptno</div><div class="line"><span class="keyword">AND</span> e.comm(+) = <span class="number">300</span>;</div></pre></td></tr></table></figure>
<p>②リテラル条件には「(+)」を付与しないパターン<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> d.deptno, d.dname, e.empno, e.ename, e.comm</div><div class="line"><span class="keyword">FROM</span> emp e, dept d</div><div class="line"><span class="keyword">WHERE</span> e.deptno(+) = d.deptno</div><div class="line"><span class="keyword">AND</span> e.comm = <span class="number">300</span>;</div></pre></td></tr></table></figure></p>
<p>検索結果は以下の通りです。</p>
<p><img src="/images/20180529/outerjoin_result_SQL.png"></p>
<p>①の場合は結合前にリテラル条件で絞っている、②の場合は結合後にリテラル条件で絞っている、ということです。</p>
<p>これをSQL標準であるOUTER JOINで記載すると以下のようになります。OUTER JOINの条件となるか全体の条件となるかの違いがSQL標準の方が分かりやすいですね。</p>
<p>①<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> d.deptno, d.dname, e.empno, e.ename, e.comm</div><div class="line"><span class="keyword">FROM</span> scott.emp e</div><div class="line">    <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> scott.dept d</div><div class="line">        <span class="keyword">ON</span> (e.deptno = d.deptno <span class="keyword">AND</span> e.comm = <span class="number">300</span>);</div></pre></td></tr></table></figure></p>
<p>②<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> d.deptno, d.dname, e.empno, e.ename, e.comm</div><div class="line"><span class="keyword">FROM</span> scott.emp e</div><div class="line">    <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> scott.dept d</div><div class="line">        <span class="keyword">ON</span> (e.deptno = d.deptno)</div><div class="line"><span class="keyword">WHERE</span> e.comm = <span class="number">300</span>;</div></pre></td></tr></table></figure></p>
<p>念のため結果も。</p>
<p><img src="/images/20180529/outerjoin_result_SQL.png"></p>
<p>PostgreSQLでも。</p>
<p><img src="/images/20180529/outerjoin_result_postgres.png"></p>
<p>同じですね。</p>
<p>将来のことを考えて、外部結合はSQL標準のOUTER JOINで記述していくことにしましょう。</p>
<h4 id="組み込み関数"><a href="#組み込み関数" class="headerlink" title="組み込み関数"></a>組み込み関数</h4><p>組み込み関数はデータベース毎に事前に用意されている関数です。データベース毎に仕様が異なることもありますので、移行の際には注意が必要です。OracleとPostgreSQLの組み込み関数の対比表については<a href="https://www.pgecons.org/wp-content/uploads/PGECons/2012/WG2/10_Built-inFunctionMigrationResearch/10_Appendix_01_Built-inFunctionComparativeTable(Oracle-PostgreSQL).pdf" target="_blank" rel="noopener">こちら</a>に詳しく載っておりますので、参考にしてください。</p>
<p>OracleとPostgreSQLの両方で用意されていますが、機能仕様が全く異なる関数としては<code>DECODE</code>があります。<br>OracleではDECODE関数は条件分岐として使われています。構文としては以下です。</p>
<p><img src="/images/20180529/decode_oracle.png" class="img-middle-size"></p>
<p>PostgreSQLでは、DECODE関数はテキスト表現からバイナリデータを復号する関数となっております。構文としては以下です。（formatオプションはbase64/hex/escapeから選択。）</p>
<p><img src="/images/20180529/decode_postgres.png" class="img-small-size"></p>
<p>OracleでのDECODE関数はPostgreSQLではcase文に変換します。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> empno, ename, deptno,</div><div class="line"><span class="keyword">DECODE</span>( deptno, <span class="number">10</span>, <span class="string">'ACCOUNTING'</span>,</div><div class="line">                <span class="number">20</span>, <span class="string">'RESEARCH'</span>,</div><div class="line">                <span class="number">30</span>, <span class="string">'SALES'</span>,</div><div class="line">                <span class="number">40</span>, <span class="string">'OPERATIONS'</span> ) <span class="keyword">as</span> dname</div><div class="line"><span class="keyword">FROM</span> emp;</div></pre></td></tr></table></figure>
<p>変換後は以下となります。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> empno, ename, deptno,</div><div class="line"><span class="keyword">CASE</span> deptno <span class="keyword">WHEN</span> <span class="number">10</span> <span class="keyword">THEN</span> <span class="string">'ACCOUNTING'</span></div><div class="line">            <span class="keyword">WHEN</span> <span class="number">20</span> <span class="keyword">THEN</span> <span class="string">'RESEARCH'</span></div><div class="line">            <span class="keyword">WHEN</span> <span class="number">30</span> <span class="keyword">THEN</span> <span class="string">'SALES'</span></div><div class="line">            <span class="keyword">WHEN</span> <span class="number">40</span> <span class="keyword">THEN</span> <span class="string">'OPERATIONS'</span> <span class="keyword">END</span> <span class="keyword">as</span> dname</div><div class="line"><span class="keyword">FROM</span> emp;</div></pre></td></tr></table></figure>
<p>DECODEについては以上です。</p>
<p>一部のOracle独自の組み込み関数は、orafceモジュールをインストールすることで、いくつかOracleと同じ関数が実装されます。orafceで実装される関数については<a href="http://orafce.github.io/orafce/index-ja.html" target="_blank" rel="noopener">こちら</a>を参考にしてください。また、AWSのRDSでも事前にOracleからの移行用にモジュール（スキーマ：aws_oracle_ext）が準備されています。</p>
<h4 id="行レベルロック"><a href="#行レベルロック" class="headerlink" title="行レベルロック"></a>行レベルロック</h4><p>一般的に業務ロジック上では競合による不整合を回避するため、<code>SELECT ～ FOR UPDATE</code>により行レベルでロックを取得し、トランザクション中のレコードが他から更新・削除されることを防ぎます。</p>
<p>Oracleにおいては、<code>FOR UPDATE</code>句として<code>[WAIT n|NOWAIT]</code>の記述ができます。どちらも指定しない場合は、行が使用可能になるまで待機した後でSELECT文の結果が戻されます。（そんな記載はできませんが、<code>WAIT ∞</code>の指定のような挙動です。）</p>
<p>PostgreSQLでは、<code>WAIT n</code>が存在しないため、WAITと記載するとOracleにおける<code>WAIT</code>句を未指定とした場合と同様の挙動となります。つまり、Oracleにおいて<code>WAIT n</code>が指定されていた場合は、PostgreSQLではSQLにて実装することができません。</p>
<p>代替となるかは実行形式によりますが、<code>lock_timeout</code>のパラメータを指定することでOracleと同じ挙動となる可能性はあります。このパラメータを設定して<code>SELECT ～ FOR UPDATE</code>を発行しすると、該当レコードがロック状態であった場合は設定したパラメータの時間経過するとエラーとなります。<code>lock_timeout</code>はセッションレベルでの変更も可能ですので、トランザクション開始時にパラメータを設定をしてSQLを実行、トランザクション終了時にパラメータをリセットするという処理仕様とすることもできます。</p>
<h4 id="ヒント句"><a href="#ヒント句" class="headerlink" title="ヒント句"></a>ヒント句</h4><p>OracleにおいてSQLの性能問題は実行計画が原因であることが多いのではないでしょうか。確かに、Oracleにおける性能劣化対策としての「ヒント句を記載して実行計画を固定化する」は、統計情報に依存せずにSQLの性能を安定させる最適な解決策なのかもしれません。（バージョンアップの時にヒントが無くなったり、オプティマイザの機能向上により性能の良い実行計画が選ばれないというデメリット(?)もあります。）また、システムのSQLコーディング基準でヒント句を記載するといったルールがある場合もあるでしょう。</p>
<p>PostgreSQLではバージョン9.1以降<code>pg_hint_plan</code>モジュールをインストールすることでヒント句の記載はできますが、Oracleほど数多くの種類のヒントがあるわけではありません。（参考までにヒント句の種類としては、pg_hint_plan 1.1で23個、Oracle 12cR1で332個あります。）</p>
<p>データベースによりオプティマイザが全く異なりますので、データベースをマイグレーションすると実行計画が変化するのは致し方無いと考えております。そこで、ヒント句の記載のあるSQLについては、コンバージョン時には一旦ヒント句を削除して性能を見ることになります。処理性能が思わしくない場合は個別にチューニングの対応を施すことになります。</p>
<h4 id="NULLと空文字"><a href="#NULLと空文字" class="headerlink" title="NULLと空文字"></a>NULLと空文字</h4><p>OracleではNULLと空文字は同義で、空文字はNULLとして扱われます。PostgreSQLではNULLと空文字は別物です。従いまして、Oracle上で動作するSQL内で条件句として<code>WHERE COL is NULL</code>といった記載がある場合は、マイグレーションにより結果が異なってくる可能性があるので注意が必要です。</p>
<p>PostgreSQLでは、NULLの使用を禁止するといった基準を作った方が良いと思います。その場合は、データの移行時にNULLはすべて空文字に変換することは忘れずに！</p>
<p>NULLの四則演算やNULLと文字列の連結については、すべてNULLとなってしまうため、NULLが格納される可能性がある列を取り扱う場合は、必ず<code>COALESCE</code>関数を使って処理してください。（OracleでいうところのNVL関数ですね。）</p>
<h4 id="MERGE文"><a href="#MERGE文" class="headerlink" title="MERGE文"></a>MERGE文</h4><p>OracleではMERGE文が利用できますが、PosrgreSQLではMERGE文は存在しません。<br>PosrgreSQL 9.5からUPSERT文（INSERT ON CONFLICT）が使用可能となります。</p>
<p>こちらもサンプルスキーマ環境で見てみましょう。<br>empと同じ定義のemp_up表を作成してます。</p>
<p><img src="/images/20180529/merge_empup.png"></p>
<p>emp_up表のレコードを見てemp表にレコードが存在した場合はUPDATEをして、emp表にレコードが無ければINSERTをするというMERGE文を作ってみました。（今回はSAL列とCOMM列だけをUPDATEしてます。）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">MERGE</span> <span class="keyword">INTO</span> emp e</div><div class="line"><span class="keyword">USING</span> emp_up u <span class="keyword">ON</span> (e.empno = u.empno)</div><div class="line">  <span class="keyword">WHEN</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span></div><div class="line">    <span class="keyword">UPDATE</span> <span class="keyword">SET</span> e.sal = u.sal, e.comm = u.comm</div><div class="line">  <span class="keyword">WHEN</span> <span class="keyword">NOT</span> <span class="keyword">MATCHED</span> <span class="keyword">THEN</span></div><div class="line">    <span class="keyword">INSERT</span> <span class="keyword">VALUES</span> ( u.empno, u.ename, u.job, u.mgr, u.hiredate, u.sal, u.comm, u.deptno );</div></pre></td></tr></table></figure>
<p>実行結果です。</p>
<p><img src="/images/20180529/merge_result.png"></p>
<p>empno:7369のsal列が変更され、empno:8000のレコードが作成されていますね。</p>
<p>これをPostgreSQLで実装すると以下のようになります。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> scott.emp</div><div class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> scott.emp_up</div><div class="line"><span class="keyword">ON</span> CONFLICT (empno)</div><div class="line"><span class="keyword">DO</span> <span class="keyword">UPDATE</span> <span class="keyword">SET</span> sal = excluded.sal, comm = excluded.comm ;</div></pre></td></tr></table></figure>
<p>実行結果です。</p>
<p><img src="/images/20180529/upsert_result.png"></p>
<p>同じ結果となりました。</p>
<p>このUPSERT文ですが、PostgreSQL 9におけるパーティションテーブルに対しては機能しません。理由としては、PostgreSQLのパーティションテーブルは親となるテーブルとパーティション単位の個別テーブルを作成して、親テーブルへのDML発行時にはトリガーにより該当のパーティションテーブルを更新しており、親表に対して<code>INSERT ON CONFLICT</code>を発行したとしてもトリガーとしては該当パーティションテーブルに対して<code>INSERT ON CONFLICT</code>を発行しないからです。</p>
<p>それであれば、トリガー内でのパーティションテーブルに対する構文を<code>INSERT ON CONFLICT</code>となるように作り直せば良いかというとそうもいきません。それは通常のINSERT文が発行された際もON CONFLICT付きのINSERTとなってしまうからです。</p>
<p>では、パーティションテーブルに対するMERGE文の変換はどうすればよいかというと、PostgreSQL 9.1で導入されたCommon Table Expression(CTE)を使って代替ができます。SQL文としては以下のようになります。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">WITH insrt AS (<span class="keyword">SELECT</span> * <span class="keyword">FROM</span> scott.emp_up),</div><div class="line">     updt  <span class="keyword">AS</span> (<span class="keyword">UPDATE</span> scott.emp <span class="keyword">set</span> sal = insrt.sal <span class="keyword">FROM</span> insrt</div><div class="line">                             <span class="keyword">WHERE</span> emp.empno = insrt.empno <span class="keyword">RETURNING</span> emp.empno)</div><div class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> scott.emp</div><div class="line">(<span class="keyword">SELECT</span> * <span class="keyword">FROM</span> insrt <span class="keyword">WHERE</span> empno <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> empno <span class="keyword">FROM</span> updt));</div></pre></td></tr></table></figure>
<p>実行結果です。</p>
<p><img src="/images/20180529/insert_CTE_result.png"></p>
<p>通常テーブルであれば、もちろんUPSERTでもCTEでも結果は一緒ですね。</p>
<h3 id="3-3-データ移行について"><a href="#3-3-データ移行について" class="headerlink" title="3-3. データ移行について"></a>3-3. データ移行について</h3><p>データ移行については今回のブログでは詳細な記載は割愛しますが、データ移行もマイグレーションにおいては非常に重要な作業となります。移行時間、キャラクタセットの違いやデータ抽出方法など、移行要件を満たすために様々な検討・設計が必要となります。</p>
<p>データベースの切替時にはデータベースを利用する業務の全面停止が必要にはなりますが、その停止時間を最小限とする要件を持つシステムも多いと思います。その場合は、データの事前移行＋切替直前まで常時レプリケーション⇒切替といった方式で業務停止時間を最短とする案もあります。事前移行方法やレプリケーション方式については機会があれば詳しく書こうとは思いますが、検討する事項はたくさんあります。<br>レプリケーション方式の一例としては、SaaSとして提供されているAWS Database Migration Serviceがあります。こちらのサービスは移行先のデータベースがAWSのPaaSを利用している場合となります。<br>AWS DMSの詳細は<a href="https://aws.amazon.com/jp/dms/" target="_blank" rel="noopener">こちら</a>を参照してください。</p>
<h2 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h2><p>第1回では、データベースマイグレーションの背景や流れと一般的なマイグレーションのポイントとなる点について記載していきました。データベースオブジェクト（スキーマ）、アプリケーション（SQL）について、多くの場合は一定ルールに基づき変更可能であることが分かります。</p>
<p>次回は、一般的に利用されているマイグレーションツールと、実際のアプリケーションにてマイグレーションの評価をおこなった例ついて記載していきたいと思います。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-初めに&quot;&gt;&lt;a href=&quot;#1-初めに&quot; class=&quot;headerlink&quot; title=&quot;1. 初めに&quot;&gt;&lt;/a&gt;1. 初めに&lt;/h2&gt;&lt;p&gt;こんにちは。Technology Innovation Groupの岸田です。&lt;/p&gt;
&lt;p&gt;データベースシステ
    
    </summary>
    
      <category term="DB" scheme="https://future-architect.github.io/categories/DB/"/>
    
    
      <category term="Migration" scheme="https://future-architect.github.io/tags/Migration/"/>
    
      <category term="PostgresSQL" scheme="https://future-architect.github.io/tags/PostgresSQL/"/>
    
      <category term="Oracle" scheme="https://future-architect.github.io/tags/Oracle/"/>
    
  </entry>
  
  <entry>
    <title>IoT/M2M展（音声認識サービス）の展示</title>
    <link href="https://future-architect.github.io/articles/20180522/"/>
    <id>https://future-architect.github.io/articles/20180522/</id>
    <published>2018-05-22T05:20:58.000Z</published>
    <updated>2018-05-22T05:58:15.910Z</updated>
    
    <content type="html"><![CDATA[<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>こんにちは、Strategic AI Group（通称 : SAIG）の岩田です。<br>前回の<a href="https://future-architect.github.io/articles/20180413/">ICLR2018記事</a>に引き続いての投稿になります。</p>
<p>今回は少し話題を変えまして、先日出展したIoT/M2M展のご報告と、そこで披露した音声認識デモについてのご紹介となります。</p>
<h1 id="IoT-M2M-展のご紹介"><a href="#IoT-M2M-展のご紹介" class="headerlink" title="IoT/M2M 展のご紹介"></a>IoT/M2M 展のご紹介</h1><ul>
<li>Japan IT Weekという非常に大きな展示会が年に3回あり、その一つとしてIoT/M2M展があります</li>
<li>IoT/M2M以外にもクラウドやAI・自動化、情報セキュリティ等、企業として関心が高いものブースが沢山あります</li>
<li>国内最大級の展示会イベントなので、とにかく人が多いです！（通路を歩くのもやっと・・）</li>
</ul>
<h1 id="展示のきっかけ"><a href="#展示のきっかけ" class="headerlink" title="展示のきっかけ"></a>展示のきっかけ</h1><ul>
<li>IoT/M2M展には弊社のグループ会社である <a href="https://www.trexedge.co.jp/" target="_blank" rel="noopener"><strong>TrexEdge</strong></a> が出展を決めました</li>
<li>共同出展という形で、ブース内の一部を借りてFutureのAIサービスも紹介を行なう運びとなりました</li>
</ul>
<p><img src="/images/20180523/photo_20180523_01.jpg" class="img-middle-size"></p>
<h2 id="出展物"><a href="#出展物" class="headerlink" title="出展物"></a>出展物</h2><p>2017年より開発していた音声認識サービスを展示してきました。<br>今回は展示（音声認識サービス）としてのポイントをいくつかご紹介します。</p>
<p><strong>※そもそも音声認識とは？</strong> という方は是非こちらを参照ください。<br><a href="https://www.slideshare.net/c5tom/ss-56184353" target="_blank" rel="noopener">自称・世界一わかりやすい音声認識入門</a></p>
<p><img src="/images/20180523/photo_20180523_02.jpeg" class="img-middle-size"></p>
<h3 id="サービス化への道のり"><a href="#サービス化への道のり" class="headerlink" title="サービス化への道のり"></a>サービス化への道のり</h3><p>音声認識サービスとしては、Google Cloud Platform の <a href="https://cloud.google.com/speech/?hl=ja" target="_blank" rel="noopener"><strong>Speech API</strong></a>を用いています。<br>自分たちでスクラッチでゼロから開発するのではなく、クラウド機械学習API を利用するという選択をしました。<br>理由は以下の2点があります。</p>
<ol>
<li>Google/Amazon/Microsoft/Apple等々のITガリバーと認識エンジンレベルで競い合うことはハードルが高く、差別化も難しい</li>
<li>クイックにデモサービスを作成し、実用段階を確かめたい（音声データの収集、認識エンジン作成の手間を削減する）</li>
</ol>
<p>Google APIの採用理由は、自社内で比較した中で最も精度が高かったためです。</p>
<p>早速、これを用いて「プロタイプ」を作り、触ってもらって実用レベルなのかを判断してみました（わくわく）。</p>
<p>…しかし、社内での判断としては「<strong>クラウドAPIを単純に呼び出すレベルでは実用としてはまだ遠い</strong>」というものでした。</p>
<p>そのため、クラウドAPIを活用し、その上に肉付けすることによって、「<strong>プロトタイプ</strong>」ラインから「<strong>現場で使える</strong>」ラインに昇華するという戦略を考えました。</p>
<p><img src="/images/20180523/photo_20180523_03.png"></p>
<p>「<strong>現場で使える</strong>」ラインにするために出した答えとしては、音声認識結果に対する<strong>後処理校正機能</strong>でした。<br>業界に関連するコーパスと作成した言語モデルを用いて、誤りと思われる部分を可視化してあげます。</p>
<p>また、Google側では現状として結果に句読点が入らないなど、実用で考えたときに足りない部分がいくつかあります。その部分を補い、そして顧客に合った形でサービス(UI/UX・AI/システムの機能運用)をカスタマイズする。</p>
<p>もちろん理想としては、人の手を煩わせない（どんな音声でも正解率100%）ですが録音ユースケースによっては、満点を求められることはないケースもあります。</p>
<p>したがって、ミニマムとしてそのケースの品質は担保し徐々に適用できるケースを広げていくというのがサービスの始まりになっています。</p>
<h3 id="サービス名"><a href="#サービス名" class="headerlink" title="サービス名"></a>サービス名</h3><p>本サービスとしての名称は <strong>Future Transcribe AI Platform</strong> として展開を実施していきます。</p>
<p><img src="/images/20180523/photo_20180523_04.png" class="img-middle-size"></p>
<p>こうしてサービスの名前も決まり、そして縁がありIoT/M2M展示会に出展できることとなりました。<br>当日は多くの質問や引き合いがあり、嬉しい限りでした。</p>
<h3 id="※注意"><a href="#※注意" class="headerlink" title="※注意"></a><strong>※注意</strong></h3><p>上記の書き方だと、特に考えもなしにプロトタイプを作った感があるので、少し補足しておきます。<br>音声認識には下記2点で優遇すべきポイントがあると認識しています。</p>
<ul>
<li>音声認識（文字おこし）をバリバリ使ってますという実例報告は多くない</li>
<li>顧客によってはインプットとなる音声の録音ユースケースが多い</li>
</ul>
<p>この2点を踏まえて、作ってみる価値があるという結論になってからの Go としています。</p>
<h3 id="業務観点として必要なマイクの選定"><a href="#業務観点として必要なマイクの選定" class="headerlink" title="業務観点として必要なマイクの選定"></a>業務観点として必要なマイクの選定</h3><p>サービスとして、認識エンジンの精度を向上させるのはもちろんですが、インプットとなる音声自体の質も重要です。<br>単一指向性や全指向性、付属するマイクの数、そして価格帯等を観点として複数マイクをならべた検証も現場で利用する上では必要です。</p>
<p>この録音ユースケースに対してマイクの自体の比較と、認識結果の比較の展示も行いました。</p>
<p>例えば、こちらはインタビューに対しての低価格帯レコーダー vs 高価格帯レコーダーの認識結果です。</p>
<p><img src="/images/20180523/photo_20180523_05.png"></p>
<p>一部変換ミスもありますが、言葉尻などの細かい部分で高価格帯は正しく認識することができています。</p>
<p>では、この結果から例えば</p>
<ul>
<li>低価格帯のマイクでもどのように録音できれば精度が向上するのか</li>
<li>マイクの組み合わせによって、より精度を向上させることができるのか</li>
</ul>
<p>といった疑問やアイデアがでてくるかと思います。<br>この点は社内で研究している最中ですので、何か有益な結果がでましたらまた公開できればと思います。</p>
<h2 id="おわりに"><a href="#おわりに" class="headerlink" title="おわりに"></a>おわりに</h2><p>今回は展示（音声認識サービス）にスポットを当てた話となりました。</p>
<p>サービスとしてのご紹介となってしまいましたが、弊社のSAIGグループでは、「AI導入の検討」から「カスタマイズされたAIシステム構築・運用」まで一貫してお手伝いさせていただきます。<br>お問い合わせは<a href="https://www.future.co.jp/contact_us/" target="_blank" rel="noopener"><strong>こちら</strong></a>からお願いします。</p>
<p>また、10月にございます<a href="http://expo.nikkeibp.co.jp/xtech/ex/ai/index.html" target="_blank" rel="noopener"><strong>人工知能／ビジネスAI 2018</strong></a>にもブースを出展する予定ですので是非お立ち寄りいただければと思います。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h1&gt;&lt;p&gt;こんにちは、Strategic AI Group（通称 : SAIG）の岩田です。&lt;br&gt;前回の&lt;a href=&quot;https
    
    </summary>
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/categories/MachineLearning/"/>
    
    
      <category term="IoT" scheme="https://future-architect.github.io/tags/IoT/"/>
    
  </entry>
  
  <entry>
    <title>ICLR2018 LT大会</title>
    <link href="https://future-architect.github.io/articles/20180413/"/>
    <id>https://future-architect.github.io/articles/20180413/</id>
    <published>2018-04-12T16:17:13.000Z</published>
    <updated>2018-04-12T16:52:46.755Z</updated>
    
    <content type="html"><![CDATA[<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>みなさん、こんにちは。<br>Strategic AI Group（通称 : SAIG）の岩田です。</p>
<p>今回は以前に社内開催された<a href="https://future-architect.github.io/articles/20180222/">NIPS2017のLT大会</a>につづき<br>ICLR2018の社内LT大会の模様を紹介いたします！</p>
<h1 id="ICLRとは"><a href="#ICLRとは" class="headerlink" title="ICLRとは"></a>ICLRとは</h1><ul>
<li>International Conference on Learning Representations の略</li>
<li>2013年度に初開催されたカンファレンスで、今年で6回目</li>
<li>2018年本大会ページは<a href="https://iclr.cc/doku.php?id=iclr2018:main" target="_blank" rel="noopener">こちら</a></li>
<li>表現学習という意味では採択される内容も広めです。ざっと投稿された表題に目を通しましたが、やはり研究内容としてHOTであるadversarial（敵対的な）に関連する研究は多かったように見受けられました。</li>
</ul>
<h2 id="社内勉強会の様子"><a href="#社内勉強会の様子" class="headerlink" title="社内勉強会の様子"></a>社内勉強会の様子</h2><p>SAIGグループに閉じた開催ではなく、社内で興味のあるメンバーを募って実施しています。<br>開催時間帯としては、一番メンバーの集まりやすいお昼時に開催です！</p>
<p><img src="/images/20180403/photo_20180403_01.jpeg"><br><img src="/images/20180403/photo_20180403_02.jpeg"></p>
<p>実演も交えた形で初学習者にもわかりやすい内容になっていました。<br>（プレゼンへのこだわりは、たとえLTスタイルだとしてもFutureらしさがあります）<br><img src="/images/20180403/photo_20180403_03.jpeg"></p>
<h1 id="論文の選択"><a href="#論文の選択" class="headerlink" title="論文の選択"></a>論文の選択</h1><p><a href="https://openreview.net/group?id=ICLR.cc/2018/Conference" target="_blank" rel="noopener">Conference Track</a> の「Oral Papers」「Poster Papers」から各々が興味あるものを選択し、披露しました。<br>私個人としては分散表現に興味があるので、関連するテーマを選択しています。</p>
<h1 id="LTの内容"><a href="#LTの内容" class="headerlink" title="LTの内容"></a>LTの内容</h1><p>簡単にではありますが、それぞれの発表内容を記載します！</p>
<h2 id="1-Diffusion-Convolutional-Recurrent-Neural-Network-Data-Driven-Traffic-Forecasting"><a href="#1-Diffusion-Convolutional-Recurrent-Neural-Network-Data-Driven-Traffic-Forecasting" class="headerlink" title="1. Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting"></a>1. <a href="https://openreview.net/pdf?id=SJiHXGWAZ" target="_blank" rel="noopener">Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</a></h2><ul>
<li>発表者：貞光</li>
</ul>
<h3 id="手法-HOW"><a href="#手法-HOW" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>従来の時系列予測問題に対しては、ARIMAモデルやカルマンフィルタ等が用いられていますが、交通量のネットワーク自体がグラフの性質を持つため、グラフをより適切に扱える方法が要求されます。</p>
<p>加えて問題となるのがエッジ重みの非対称性です。ホリデーシーズン等を思い浮かべるとわかりやすいと思いますが、上り線と下り線でまったく交通量が異なりますよね。そのため、同じエッジであっても、エッジの方向によって重みが異なる非対称性を扱う必要が生じます。（図１参照）</p>
<p>近年では、グラフをニューラルネットワークで扱うための手法が多く提案されており、その手法は大きく2種類に大別できます。</p>
<p>1つ目はグラフスペクトル情報を用いた系で、Graph Convolutional Network (GCN) [Bruna+2014] が代表的です。[Yu+2017]は、GCNに対し系列データを扱えるように拡張しましたが、無向グラフしか扱えないという問題があり、交通量のようなエッジ重みの非対称性には対応できませんでした。</p>
<p>2つ目は明示的にグラフスペクトル情報を用いない手法で、 Diffusion CNN (DCNN) が代表的です。<br>著者らはDCNNを時系列変化に適応させたDCRNNを提案し、交通量予測を高精度に実現しました。</p>
<p>実験の結果、ARIMAやFull Connected LSTMに比べ大幅な予測精度改善を実現しています。（図2参照）<br>また、コードは以下のgithubで公開されていますので興味のある方は是非試してみてください。<br><a href="https://github.com/liyaguang/DCRNN" target="_blank" rel="noopener">https://github.com/liyaguang/DCRNN</a></p>
<h6 id="図１"><a href="#図１" class="headerlink" title="図１"></a>図１</h6><p><img src="/images/20180403/photo_20180403_04.png" class="img-small-size"></p>
<h6 id="図2"><a href="#図2" class="headerlink" title="図2"></a>図2</h6><p><img src="/images/20180403/photo_20180403_05.png" class="img-small-size"></p>
<h6 id="Bruno2014-Joan-Bruna-Wojciech-Zaremba-Arthur-Szlam-and-Yann-LeCun-Spectral-networks-and-locally-connected-networks-on-graphs-In-ICLR-2014"><a href="#Bruno2014-Joan-Bruna-Wojciech-Zaremba-Arthur-Szlam-and-Yann-LeCun-Spectral-networks-and-locally-connected-networks-on-graphs-In-ICLR-2014" class="headerlink" title="[Bruno2014] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014"></a>[Bruno2014] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In ICLR, 2014</h6><h6 id="Yu2017-Bing-Yu-Haoteng-Yin-and-Zhanxing-Zhu-Spatio-temporal-graph-convolutional-neural-network-A-deep-learning-framework-for-traffic-forecasting-arXiv-preprint-arXiv-1709-04875-2017a"><a href="#Yu2017-Bing-Yu-Haoteng-Yin-and-Zhanxing-Zhu-Spatio-temporal-graph-convolutional-neural-network-A-deep-learning-framework-for-traffic-forecasting-arXiv-preprint-arXiv-1709-04875-2017a" class="headerlink" title="[Yu2017] Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting. arXiv preprint arXiv:1709.04875, 2017a"></a>[Yu2017] Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting. arXiv preprint arXiv:1709.04875, 2017a</h6><h2 id="2-A-New-Method-of-Region-Embedding-for-Text-Classification"><a href="#2-A-New-Method-of-Region-Embedding-for-Text-Classification" class="headerlink" title="2. A New Method of Region Embedding for Text Classification"></a>2. <a href="https://openreview.net/pdf?id=BkSDMA36Z" target="_blank" rel="noopener">A New Method of Region Embedding for Text Classification</a></h2><ul>
<li>発表者：岩田</li>
</ul>
<h3 id="手法-HOW-1"><a href="#手法-HOW-1" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>小単位での語順表現を取得する際にCNN等の手法を使わずに、Local Context Unitを用いてembeddingを表現する手法です。文章に対して適用されるregion（範囲）を決めて、取得できた文字列を通常のembedding結果 (e) に対してアダマール積算をします。<br>（例：The food is not very good in this hotel.という文章があった場合、中心語:not、region:2とすると<br>food is not very good　が取得されます、これが今回でいうところの Local Context Unit (K) になります）</p>
<p>その積算結果 P を各コンテキスト毎に抽出し、Max pooling層を通して、最大値となるものを抽出します<br>（アーキテクチャとしては下図参照）その結果を用いて、課題の分類問題に当てはめていくというのが大枠になります。</p>
<p>精度としては、8つのデータセットに対して6つが最高精度を収めたという記載があり、Local Context Unitを用いた結果、より語彙の重要性を表現できた、係り受けを明確にできたため正しいsentimentを取得することができたという結果になっています。</p>
<p><img src="/images/20180403/22aed6d5-3534-d5c6-4b57-d282fd89e609.png" class="img-middle-size"></p>
<h2 id="3-Don’t-Decay-the-Learning-Rate-Increase-the-Batch-Size"><a href="#3-Don’t-Decay-the-Learning-Rate-Increase-the-Batch-Size" class="headerlink" title="3. Don’t Decay the Learning Rate, Increase the Batch Size "></a>3. <a href="https://openreview.net/pdf?id=B1Yy1BxCZ" target="_blank" rel="noopener">Don’t Decay the Learning Rate, Increase the Batch Size </a></h2><ul>
<li>発表者：小池</li>
</ul>
<h3 id="手法-HOW-2"><a href="#手法-HOW-2" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>分散学習の際に、精度を落とさずに学習を早く行う手法を示しています。以前からImageNetのような大量のデータを学習するには時間がかかることが言われていました。理論的にはバッチサイズを大きくすれば学習が早く終わりますが、バッチサイズが大きいと精度の面でなかなか学習がうまくいかないことが以前から問題となっていました。</p>
<p>一般的なDeep Learningの学習では、学習が進むにつれLearning rateを小さくしていきますよね。ただしこれの手法は時間が非常にかかります。</p>
<p>本論文では、学習段階で学習率を小さくする代わりにバッチサイズを途中で増していくと学習がうまくいくことを主張しています。これの何が良いかというと、バッチサイズを大きくできる＝早く学習ができるというメリットがあります。論文では、数値実験で学習率を減衰させた場合とバッチサイズを増やした場合を比較しており、両者ともに精度面では有意な差がないことを示しています。なお、数値実験ではWide ResNetでCIFAR10の学習を行っています。</p>
<p><img src="/images/20180403/photo_20180403_06.png"></p>
<h2 id="4-Learning-Deep-Mean-Field-Games-for-Modeling-Large-Population-Behavior"><a href="#4-Learning-Deep-Mean-Field-Games-for-Modeling-Large-Population-Behavior" class="headerlink" title="4. Learning Deep Mean Field Games for Modeling Large Population Behavior"></a>4. <a href="https://openreview.net/pdf?id=HktK4BeCZ" target="_blank" rel="noopener">Learning Deep Mean Field Games for Modeling Large Population Behavior</a></h2><ul>
<li>発表者：西森</li>
</ul>
<h3 id="手法-HOW-3"><a href="#手法-HOW-3" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>Collective Behavior（集団行動）中でも人口分布をモデル化し、その時間的発展を予測する手法です。</p>
<p>具体的な方法としては、Mean Field Game(以下MFG)のコスト関数（＝報酬関数）を逆強化学習によってデータから学習する手法を提案したものになります。</p>
<p>MFGはコスト関数をヒューリスティックに定める必要があります。しかし、現実の集合行動に対してコスト関数を設計するのは困難なため、これらの実験はトイプロブレム（現実には役に立たない範囲）に留まっていました。<br>しかし、MFGをマルコフ決定過程（以下、MDP）として表すことでベルマン最適方程式を用いたシングルエージェントの強化学習で解くことができたという論文になります。</p>
<p>実験として、Twitterの人気トピック予測を行い、既存手法であるVAR,RNNよりも高い精度を示すことを確認しています。（下図参照）。また、図(a)より、提案手法では「人気のトピックが、朝から夜にかけて人気になっていく」という特徴をとらえることができています。</p>
<p><img src="/images/20180403/91612b55-3f56-a567-5341-9c5502188575.png" class="img-middle-size"></p>
<h2 id="5-Towards-Neural-Phrase-based-Machine-Translation"><a href="#5-Towards-Neural-Phrase-based-Machine-Translation" class="headerlink" title="5. Towards Neural Phrase-based Machine Translation"></a>5. <a href="https://openreview.net/pdf?id=HktJec1RZ" target="_blank" rel="noopener">Towards Neural Phrase-based Machine Translation</a></h2><ul>
<li>発表者：平賀</li>
</ul>
<h3 id="手法-HOW-4"><a href="#手法-HOW-4" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>attentionを使わずに、ニューラルネットワークを用いた機械翻訳手法です。<br>Sleep-WAke Networks (以下SWAN) にsoft reorderingを適応させた Neural Phrase-based Machine Translation (以下NPMT) の提案になります。</p>
<p>NPMTの構造としては、下図のようになります。input (ドイツ語) をoutput (英語) に翻訳する際に、最初のポイントとしてはsoft reorderingの実施になります。</p>
<p>SWANはinputとoutputの繋がりが単調である必要があるため、単語ごとにマッピングを行いinとoutで一致度が高い順に、語を並び替える作業を実施します。その結果をBi-directional RNNに適用し、得られた結果の集合を次ステップであるSWANに適用し、最終的な翻訳結果の出力となります。</p>
<p><img src="/images/20180403/008b8c0c-6330-edfc-2806-7a94c3c50564.png" class="img-middle-size"></p>
<p>実験結果としては、既存手法としてattentionを使ったsequence-to-sequenceモデルと比較したときに<br>BLEU評価で精度向上が見られました。</p>
<h2 id="6-Learning-how-to-explain-neural-networks-PatternNet-and-PatternAttribution"><a href="#6-Learning-how-to-explain-neural-networks-PatternNet-and-PatternAttribution" class="headerlink" title="6. Learning how to explain neural networks: PatternNet and PatternAttribution"></a>6. <a href="https://openreview.net/pdf?id=Hkn7CBaTW" target="_blank" rel="noopener">Learning how to explain neural networks: PatternNet and PatternAttribution</a></h2><ul>
<li>発表者：明官</li>
</ul>
<h3 id="手法-HOW-5"><a href="#手法-HOW-5" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>ニューラルネットワークを用いた出力結果は「どうやってその結果を導いた」のかという根拠が不明確です。<br>（つまりブラックボックスである）本論文はその根拠を推定する新しい手法となります。</p>
<p>従来手法としては、重みとして表現される W を分析するということが考えられます。つまり、ネットワークとしてどういうところに特徴があるのかをポイントで探ったり、出力層から追いかけて各層の出力と重みから貢献度はどの程度なのかを識別したりと W に関する様々な手法があります。</p>
<p>しかし、重みを分析しても結局は出力に寄与する値を導けないという内容を本論文では主張しています。というのも、本質的に W はノイズを消すように更新されるものであるということからです。</p>
<p>本手法では、まず始めにノイズ削減に着目します。ノイズを最小にする操作に注目することでどのような操作がクリティカルなのかを見極めます。またその操作が線形的な場合に限らず活性化関数である ReLU のような場合でも、パターン分け（ReLUの場合 x &gt; 0 の場合とそれ以外の2パターン）を考慮することで対応が可能であることを示しています。</p>
<p>本提案手である Pattern Net and Pattern Attribution を用いた結果を下図になります。<br>従来手法に対して、人が理解できる表現抽出が可能になっていることが理解できます。</p>
<p><img src="/images/20180403/9da73f3b-069c-f600-95ea-c38aec4b67d4.png" class="img-middle-size"></p>
<h2 id="7-A-Neural-Representation-of-Sketch-Drawings"><a href="#7-A-Neural-Representation-of-Sketch-Drawings" class="headerlink" title="7. A Neural Representation of Sketch Drawings "></a>7. <a href="https://openreview.net/pdf?id=Hy6GHpkCW" target="_blank" rel="noopener">A Neural Representation of Sketch Drawings </a></h2><ul>
<li>発表者：石橋</li>
</ul>
<h3 id="手法-HOW-6"><a href="#手法-HOW-6" class="headerlink" title="手法(HOW):"></a>手法(HOW):</h3><p>本研究は人が作成したスケッチをインプットとして、特定のクラスの手書き風スケッチをアウトプットする取り組みです。データセットとして、オンラインで提供されている、スケッチ認識サービスQuickDrawから得られた75クラス各7万枚のスケッチを利用しています。</p>
<p>研究の特徴としては、スケッチのデータが画像ではなく、(Δx,Δy,p1,p2,p3)で表される点のリストSで構成されている点が挙げられます。Δx,Δyは前の点からの移動、 p1,p2,p3はペンの状態(接触しているか、終わりか)を表すパラメータとなっており、画像というよりも書き順に近いデータを保持していることがわかります。</p>
<p>このような点のリストSを入力として、Sequence-to-Sequence Variational Autoencoderモデルを用いた学習を行い、Sketch-RNNを作成しています。（猫や豚のクラスに近づけて手書き風スケッチが出力されている）</p>
<p><img src="/images/20180403/8788edcb-0b74-334c-8b15-1e290c1e0b6c.png"></p>
<p>また、特定のクラスの手書き風スケッチをアウトプットするだけではなく、２クラスの補間を行うスケッチや、スケッチの加減算も可能となります。</p>
<p><img src="/images/20180403/8810c4d3-59f9-446d-1bde-310f01357ae6.png"></p>
<h2 id="おわりに"><a href="#おわりに" class="headerlink" title="おわりに"></a>おわりに</h2><p>今回も前回のNIPSと同様に、非常に学びのある勉強会となりました。</p>
<p>少し宣伝となりますが、フューチャーのSAIGチームでは、プロジェクトに所属しながらも自分の興味のある領域を常にインプットし、ビジネスにつなげる思いを持つメンバーが揃っています。</p>
<p>Futureでの働き方に興味がある方は是非<a href="https://www.future.co.jp/recruit/" target="_blank" rel="noopener">こちら</a>を参照してみてください。<br>エントリーお待ちしてます！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h1&gt;&lt;p&gt;みなさん、こんにちは。&lt;br&gt;Strategic AI Group（通称 : SAIG）の岩田です。&lt;/p&gt;
&lt;p&gt;今回は以
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>Future AI EXPO 開催！</title>
    <link href="https://future-architect.github.io/articles/20180301/"/>
    <id>https://future-architect.github.io/articles/20180301/</id>
    <published>2018-03-01T03:01:22.000Z</published>
    <updated>2018-03-05T04:17:12.848Z</updated>
    
    <content type="html"><![CDATA[<p>こんにちは、「元」フューチャーアーキテクトR&amp;Dチームの貞光です。</p>
<p>実は、我々R&amp;Dチームの所属が、<b>2018年3月1日</b> に少しだけ変わりましたので、今回はそのお知らせを。</p>
<p>フューチャーアーキテクト株式会社から、フューチャー持株会社の下へと移動し、<br>今後は”<b>Strategic AI Group</b>“ という名前で活動します。</p>
<p>さて、そんな枕詞を挟みつつ、今回の記事では2月末に開催した “<b>Future AI EXPO</b>“ を紹介します。<br>Future AI EXPO は、今回が初開催となるAI/IoT技術の社内向け展示イベントです。</p>
<p>弊社の１４階に広々としたリビングルームがあるのですが、この日はイベント開催のために人の波で埋め尽くされました。参加者数は社内だけにも関わらず300人以上！</p>
<p>本イベントをきっかけに、新たなコラボレーションが生まれたり、技術相談会が始まったり、<br>今後も継続開催希望！というポジティブな声をたくさんいただきました。</p>
<p>少しだけ展示の内容を紹介しますと、</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">・ノイズのある伝票画像における文字認識</div><div class="line">・数万点の小売アイテムに対する需要予測</div><div class="line">・システム開発効率化のための深層学習適用</div><div class="line">・日本の打ち上げた衛星「みちびき」と通信するモジュールの展示</div><div class="line">・自治体と協力したLoRaWanを用いたデータ収集　(株式会社TrexEdge)</div><div class="line">・画像解析を用いたプロ野球選手のトラッキング　(ライブリッツ株式会社)</div></pre></td></tr></table></figure>
<p>などなど盛り沢山。<br>今後はこれらの技術を、本blogや講演、学会活動等を通じ社外へも発信していく予定ですので、どうぞお楽しみに！</p>
<p>このように、特徴的なデータと人工知能関連技術とを組み合わせ、SAIGは新しい領域に日々チャレンジしていきます。</p>
<p>フューチャーは、データと人材のメルティング・ポット。渾沌、活気、未知、未来が集まっています。<br>フューチャー/SAIGにご興味を持たれた方はぜひお気軽にご連絡ください！</p>
<p>なお、３月に言語処理学会全国大会、６月に人工知能学会全国大会にスポンサーとしても参加する予定ですので、<br>もし参加される方がいらっしゃいましたら是非お話しましょう！</p>
<p><img src="/images/20180301/photo_20180301_01.jpeg"></p>
<p><img src="/images/20180301/photo_20180301_02.jpeg"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;こんにちは、「元」フューチャーアーキテクトR&amp;amp;Dチームの貞光です。&lt;/p&gt;
&lt;p&gt;実は、我々R&amp;amp;Dチームの所属が、&lt;b&gt;2018年3月1日&lt;/b&gt; に少しだけ変わりましたので、今回はそのお知らせを。&lt;/p&gt;
&lt;p&gt;フューチャーアーキテクト株式会社から、フュー
    
    </summary>
    
      <category term="Custure" scheme="https://future-architect.github.io/categories/Custure/"/>
    
    
      <category term="Conference" scheme="https://future-architect.github.io/tags/Conference/"/>
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>NIPS2017 LT報告</title>
    <link href="https://future-architect.github.io/articles/20180222/"/>
    <id>https://future-architect.github.io/articles/20180222/</id>
    <published>2018-02-22T01:18:28.000Z</published>
    <updated>2018-02-22T01:41:18.569Z</updated>
    
    <content type="html"><![CDATA[<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>みなさんこんにちは。<br>データサイエンスチームの小池です。<a href="https://future-architect.github.io/articles/20170526/">前回の記事</a>を書いてから半年ぶりの登場となります。<br>データサイエンスチームでは定期的に勉強会を開催しているのですが、特別イベントとして有志でNIPS 2017 LT大会を開催しました。今回はLT大会の様子と発表された論文を紹介いたします。</p>
<h1 id="NIPSとは"><a href="#NIPSとは" class="headerlink" title="NIPSとは"></a>NIPSとは</h1><p>ところでみなさんNIPSをご存知でしょうか。<br><a href="https://nips.cc/" target="_blank" rel="noopener">NIPS（Neural Information Processing Systems）</a>は機械学習（AI）系のトップカンファレンスで、<br>今最も熱い学会です。最近のAIブームもあり、提出論文数は年々増え続け2017年には3,240本もの論文が提出されました。（論文は<a href="https://papers.nips.cc/book/advances-in-neural-information-processing-systems-30-2017" target="_blank" rel="noopener">こちら</a>から読むことができます。）</p>
<p>そんな中、データサイエンスチームは今年の新人を誘ってNIPS2017LT大会を実施しました。<br>（ちなみにNIPS2017のaccepted paperの紹介がメインですが、NIPS以外に読みたい論文があれば、その紹介もOKとしました）</p>
<h1 id="NIPS2017-LT大会の様子"><a href="#NIPS2017-LT大会の様子" class="headerlink" title="NIPS2017 LT大会の様子"></a>NIPS2017 LT大会の様子</h1><p>LT大会はメンバーが多いため3回に分けて行われました。<br>数式ベースから紹介するものやデモを用いたもの、ITコンサルタントとしてそれを業務にどう活かしていくかという話題もあり、私個人にとっても非常に有益でした。</p>
<p><img src="/images/20180222/photo_20180222_01.jpeg"></p>
<p>大会中は熱い議論交わされも行われ 大いに盛り上がりました。</p>
<p><img src="/images/20180222/photo_20180222_02.jpeg"></p>
<h1 id="NIPS2017LT大会で紹介された論文の概要"><a href="#NIPS2017LT大会で紹介された論文の概要" class="headerlink" title="NIPS2017LT大会で紹介された論文の概要"></a>NIPS2017LT大会で紹介された論文の概要</h1><p>ここからは今大会で発表された論文をいくつか紹介いたします。<br>なお論文のまとめには新人の田中さんに手伝っていただきました。</p>
<h2 id="NIPS-2017論文"><a href="#NIPS-2017論文" class="headerlink" title="NIPS 2017論文"></a>NIPS 2017論文</h2><ul>
<li>Learned in Translation: Contextualized Word Vectors(Bryan McCann et. al. NIPS2017)  </li>
</ul>
<p>NLPのタスクではword vectorが用いられますが、contextの表現能力が十分ではありません。そこでcontextualized word vector(CoVe)を提案しています。CoVeは機械翻訳で学習されたseq2seq LSTM encoderから得られます。実験の結果、様々なNLPタスクでCoVeを用いることで良い結果が得られました。なお、余談になりますが先日ELMoという新しいembedding法が提案されました。CoVeとの比較もされており、より精度が高いとされています。</p>
<p><img src="/images/20180222/photo_20180222_03.png"></p>
<p><br><br><br></p>
<ul>
<li>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?(Alex Kendall et. al. NIPS 2017)</li>
</ul>
<p>画像処理における不確実性には偶発的不確実性(Aleatoric uncertainty)と認知的不確実性(Epistemic uncertainty)の2種類があります。偶発的不確実性とは、訓練データが不足するために生じる不確かさで、認知的不確実性とは訓練データがどれだけ多くあっても十分に説明できない情報に対する不確かさです。従来はどちらか一方しか考慮できませんでしたが、本論文では偶発的不確実性と認知的不確実性の両方を考慮可能な Bayesian deep learning frameworkの提案をしています。Bayesian deep learningでは、事後分布を推定することができるため不確実性の度合いを認識できます。 偶発的不確実性と認知的不確実性を従来の手法にBayesian neural networksを加えることによって考慮できるようにしています。画像判別タスクにこのNNを用いた結果、従来の手法より精度が向上しました。</p>
<p><img src="/images/20180222/photo_20180222_04.png"><br><br></p>
<ul>
<li>Best of Both Worlds: Transferring Knowledge from　Discriminative Learning to a Generative Visual Dialog Model(Jiasen Lu et al. NIPS2017)  </li>
</ul>
<p>画像に対しての質問をすると、対話の中で質問に受け答えするようなモデルの提案を行っています。 従来手法は対話生成にRNNを用いている一方、本手法ではGANを使用していることが特徴となります。ただし、ここで用いているGANは一般的なGANとは異なっています。Discriminator は、会話のリストを受け取り、受け答えとして自然なものは高いスコアをつけるように学習します。一方Generatorは、自分の生成した会話をDiscriminator に高いスコアを付けてもらえるような会話を生成しようと試みます。本論文では従来手法よりも高い精度で対話の応答ができたという結果が示されています。</p>
<p><img src="/images/20180222/photo_20180222_05.png"></p>
<p><br></p>
<ul>
<li>Stabilizing Training of Generative Adversarial Networks through Regularization (Kevin Roth et al. NIPS2017)</li>
</ul>
<p>GANの学習は設定パラメーターに対してセンシティブに過ぎることが多く、質が良いアウトプットを出すことは困難です。この課題を解決するためにDiscriminatorにノイズを入れる方法が知られています。本論文ではノイズ追加などを数式的に分析し、偽データに対するDiscriminatorの勾配を正規化するよう手法を提案しています。これは余談になるのですが、Githubに<a href="https://github.com/rothk/Stabilizing_GANs" target="_blank" rel="noopener">コード</a>も公開されていたため、モデルに実装してみたところ、確かにDiscriminatorとGeneratorの学習がうまい具合に進みました。GANに興味がある方はぜひトライしてみてください。</p>
<p><br></p>
<ul>
<li>Poincaré Embeddings for Learning Hierarchical Representations(Maximilian Nickel et al. NIPS2017)</li>
</ul>
<p>一般的な機械学習における表現獲得にはユークリッド空間が利用されますが、本論文では表現獲得にPoincaré空間の利用を試みています。Poincaré空間は双曲空間の一種であり、距離の定義が式1で表される空間のことです。Poincaré空間を用いる利点として、ユークリッド空間と比較して、超球の外にいけばいくほど距離が密になるので効率良く空間を利用できることがあげられます。Poincaré空間にembeddingすることで、ユークリッド空間で200次元が必要だったタスクがたったの5次元で同精度を得ることができました。<br><img src="/images/20180222/photo_20180222_06.png" class="img-small-size"></p>
<p><img src="/images/20180222/photo_20180222_07.png"></p>
<p><br></p>
<h2 id="その他論文"><a href="#その他論文" class="headerlink" title="その他論文"></a>その他論文</h2><ul>
<li>A Comprehensive Survey on Cross-modal Retrieval(Kaiye Wang et al. IEEE2016)</li>
</ul>
<p>画像から文字を検索したり、文字から動画を検索したりするcross modal検索の既存手法を整理した論文です。<br>cross modal検索の手法はいくつか提案されていますが、各手法の得手・不得手に加え、各特徴をまとめています。 例えば、画像および文書をベクトル化する手法には、Binaryにする手法と実数値を用いる手法がありますが、前者は検索が早い一方、後者は精度が良いという傾向が示されています。各手法に対し、オープンタスクでの精度の比較も行っています。<br><br></p>
<ul>
<li>Comicolorization: Semi-Automatic Manga Colorization(Chie Furusawa et al. SIGGRAPH Asia 2017)</li>
</ul>
<p>白黒漫画１タイトル＋参照画像と呼ばれるキャラクターのカラー画像を入力とし、白黒漫画をディープラーニングで自動着色する論文です。<br>白黒写真を自動彩色するCNN（<a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf）から以下の3点を変更することによって、キャラクターを分類した上で、キャラクター毎に鮮やかに自動着色しています。" target="_blank" rel="noopener">http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf）から以下の3点を変更することによって、キャラクターを分類した上で、キャラクター毎に鮮やかに自動着色しています。</a></p>
<ul>
<li>キャラクターを分類しやすくするため 、クラス分類を行うネットワークのラベルをキャラクターのラベルに変更</li>
<li>色彩特徴量の効果を促進するため、CNNでは使用されていなかったDiscriminatorを追加</li>
<li>インタラクティブな彩色を可能にするため、学習時に参照画像内のキャラクターに使われている色の有無を示すベクトルを抽出し追加<br><img src="/images/20180222/photo_20180222_08.png"></li>
</ul>
<h1 id="終わりに"><a href="#終わりに" class="headerlink" title="終わりに"></a>終わりに</h1><p>今回初の試みであるNIPS2017LT大会は、最新の機械学習の動向をメンバー間で共有することができました。<br>機械学習の最先端である手法を学び、それらを業務にどう活かしていくかという議論もでき非常に有益でした。<br>フューチャーアーキテクトのデータサイエンスチームでは、最先端の機械学習を用いて顧客の課題を解決できるエンジニアを募集しています。<br>興味がある方は<a href="https://www.future.co.jp/recruit/" target="_blank" rel="noopener">こちら</a>からエントリーをお願いします。より良い未来を一緒に作っていきましょう！</p>
<h2 id="予告"><a href="#予告" class="headerlink" title="予告"></a>予告</h2><p>次回はICLR2018のLT大会を行います。お楽しみに。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h1&gt;&lt;p&gt;みなさんこんにちは。&lt;br&gt;データサイエンスチームの小池です。&lt;a href=&quot;https://future-archite
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>エンプラ&amp;オンプレでもAnsible導入成功したのでユーザー会で発表してきた</title>
    <link href="https://future-architect.github.io/articles/20180209/"/>
    <id>https://future-architect.github.io/articles/20180209/</id>
    <published>2018-02-09T02:12:14.000Z</published>
    <updated>2018-02-09T02:40:19.532Z</updated>
    
    <content type="html"><![CDATA[<p>エンプラ&amp;オンプレでもAnsible導入成功したのでユーザー会で発表してきた<br>こんにちは。齋場です。<br>少し前ですが、弊社でAnsibleを導入した事例をAnsibleユーザ会で発表してきました。(どちらも5minのLTですが) Ansibleを導入したおかげか、PJは問題なくリリースすることができ、やっと落ち着いたのでブログを書きたいと思います。</p>
<p><strong>キラキラした最新技術</strong>をエンプラ&amp;オンプレPJに<strong>泥臭ーく</strong>導入した話です。</p>
<h1 id="発表内容"><a href="#発表内容" class="headerlink" title="発表内容"></a>発表内容</h1><ul>
<li>2017/8/28(月) 3社共同企画 Ansible 夏祭り<br><iframe src="//www.slideshare.net/slideshow/embed_code/key/1HlAM9KhjgeejI" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/taroshun/3ansible" title="新卒3年目のぼくが、でぶおぷす???なインフラおじさん方にAnsibleを導入してみた" target="_blank">新卒3年目のぼくが、でぶおぷす???なインフラおじさん方にAnsibleを導入してみた</a> </strong> from <strong><a href="https://www.slideshare.net/taroshun" target="_blank">Shuntaro Saiba</a></strong> </div></li>
</ul>
<ul>
<li>2017/12/21(木) Ansible Night in Tokyo<br><iframe src="//www.slideshare.net/slideshow/embed_code/key/BuEj51P21HL3yF" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/taroshun/3ojisanansible" title="新卒3年目のぼくが、でぶおぷす???なエンプラ金融PJにAnsibleを導入してみた" target="_blank">新卒3年目のぼくが、でぶおぷす???なエンプラ金融PJにAnsibleを導入してみた</a> </strong> from <strong><a href="https://www.slideshare.net/taroshun" target="_blank">Shuntaro Saiba</a></strong> </div></li>
</ul>
<p>どちらの内容も多くの皆様に共感いただくことができた気がします。。!!特にエンプラPJでのサーバ管理のツラミについて</p>
<h1 id="そもそもAnsibleとは"><a href="#そもそもAnsibleとは" class="headerlink" title="そもそもAnsibleとは"></a>そもそもAnsibleとは</h1><p>Ansible？な方に対して、軽く説明させていただきます。Ansibleを説明するにはまず、<strong>Infrastracture as Code</strong>を説明する必要があります。</p>
<h2 id="Infrastructure-as-Codeとは"><a href="#Infrastructure-as-Codeとは" class="headerlink" title="Infrastructure as Codeとは"></a>Infrastructure as Codeとは</h2><ul>
<li>Infrastructure as Codeとは「自動実行可能なコードの形でインフラの状態を記述し、インフラ構築を自動化するプロセス」です。</li>
</ul>
<p><img src="/images/20180209/photo_20180209_03.png"></p>
<ul>
<li>インフラ構築を3つに分類すると以下のようになり、それぞれの分類に対応する自動化ツールがあります。</li>
</ul>
<p><img src="/images/20180209/photo_20180209_04.png"></p>
<h2 id="Ansibleとは"><a href="#Ansibleとは" class="headerlink" title="Ansibleとは"></a>Ansibleとは</h2><ul>
<li>Ansibleとはオープンソースのサーバ構成管理ツールです。</li>
<li>上図のように主にOS/ミドルウェア設定に主に用いられます。 (実際はサーバ管理だけではなく多くの機能があります)</li>
</ul>
<p><img src="/images/20180209/photo_20180209_05.png"></p>
<ul>
<li>よくAnsibleと比較されるのが以下の2つです。<ul>
<li>Ansibleは<strong>エージェントレス</strong>ということもあり、比較的に簡単に導入することができます</li>
<li>あとは定義ファイルの形式がyamlかjsonかの違いというのも大きなポイントです</li>
</ul>
</li>
</ul>
<p><img src="/images/20180209/photo_20180209_06.png"></p>
<ul>
<li>Ansibleの使い方は非常にシンプルです。基本の構成要素は以下になります<ul>
<li><strong>Invetroy</strong>ファイル: (管理対象のサーバを定義するファイル)</li>
<li><strong>Playbook</strong>ファイル: (サーバのあるべき状態を定義するファイル)</li>
</ul>
</li>
</ul>
<p><img src="/images/20180209/photo_20180209_07.png"></p>
<p>Ansibleでサーバのあるべき状態をコードとして定義して、Playbookを実行するとサーバは定義した状態に収束します。(冪統性がある)<br>なんとも便利そうです。これなら数百台あるサーバであっても<strong>構築の自動化</strong>と<strong>高品質な構成管理</strong>ができそうですね。 以下のようにInfrastructure as Codeを導入すれば今までの課題が解決することは明らかだと思います。</p>
<p><img src="/images/20180209/photo_20180209_08.png"></p>
<h1 id="エンプラ-amp-オンプレへの導入へのチャレンジ"><a href="#エンプラ-amp-オンプレへの導入へのチャレンジ" class="headerlink" title="エンプラ&amp;オンプレへの導入へのチャレンジ!!"></a>エンプラ&amp;オンプレへの導入へのチャレンジ!!</h1><p>ここからが本題です。</p>
<h2 id="導入したいけど、、"><a href="#導入したいけど、、" class="headerlink" title="導入したいけど、、"></a>導入したいけど、、</h2><p>Infrastructure as Codeでインフラ構築の自動化！高品質な構成管理！ ができるとは言えど、<strong>実際導入するのは色々と障壁がありますよね。</strong><br>以下のような声もPJから聞こえてきそうです。(エンタープライズ&amp;オンプレだと特に)</p>
<ul>
<li><strong>便利だけど、使うのは(ソースコード書くのは)難しんでしょ？？</strong></li>
<li><strong>みんながAnsible使えるようにならないといけないんでしょ？？</strong></li>
<li><strong>今までの運用手順から変わってしまうんでしょ？？</strong></li>
<li><strong>セキュリティ面とか大丈夫？？</strong></li>
</ul>
<p>先述しましたが、Ansibleはエージェントレス型であったため障壁はかなり下がりました。 ただ、それだけでは導入はうまくいきませんでした。</p>
<h2 id="エンプラ-amp-オンプレ領域って特に難しい、、"><a href="#エンプラ-amp-オンプレ領域って特に難しい、、" class="headerlink" title="エンプラ&amp;オンプレ領域って特に難しい、、??"></a>エンプラ&amp;オンプレ領域って特に難しい、、??</h2><p>個人的に、<strong>エンプラ&amp;オンプレの領域って、Infrastructure as Codeの導入が一番困難だと思っています。</strong></p>
<p><img src="/images/20180209/photo_20180209_09.png"></p>
<p>まず、<strong>オンプレ</strong> これが相性が悪い。その大きな要因は<strong>Mutable(可変)</strong>であるためです。<br>クラウドであればサーバを都度破棄して、新規に構築する <strong>“Immutable”</strong>であるので、Infrastructure as Code本来の使い方に準じて利用できると思うのですが、オンプレだと作り直しができない”Mutable”であり、手入れをしながら長く付き合っていくことになります。その点を考慮してInfrastructure as Codeを利用しなければいけません。</p>
<p>(※ オンプレでも仮想マシンで、かつ”Immutable”な使い方をすれば話は別です)<br>(※ 逆に言えばクラウドでもMutableな使い方をしていると相性悪いと思います)</p>
<p><img src="/images/20180209/photo_20180209_10.png"></p>
<p>そして、<strong>エンプラ</strong> これも相性が悪い。<strong>“制約”</strong>とか<strong>“いままでの文化”</strong>が導入の大きな障壁になります。簡単に言うなら大きくて動きずらい。</p>
<p>日本ではあまり見かけないのですが、エンプラ&amp;ウェブ系って以下のように比喩されることが多いそうです。<strong>“ユニコーン”と”馬”</strong>って、、なんか納得感。。</p>
<blockquote>
<p>Facebook、Twitter、EtsyなどのWeb企業がDevOpsについて語るとき、彼らはユニコーンのような魔法の世界に住んでいます。クラウドベースであり、1つのサービスのみを提供することに重点を置いている企業は、共通のプラットフォームを利用して、誰もが同じページで作業できるようにします。<br>フォード、ゼネラルエレクトリック、ゼネラル・ダイナミクスなどの企業には、多くのチーム、数千の製品とサービス、そして豊富な技術を提供するさまざまなテクノロジーがあります。そのため、チーム全体を共通のページに置くことが困難になり、大きなDevOps転換を実行するのはより大きな課題です。</p>
</blockquote>
<p>出典: <a href="https://www.cloudtp.com/doppler/where-is-the-ea-in-devops/" target="_blank" rel="noopener">Where is the “EA” in DevOps?</a></p>
<p><img src="/images/20180209/photo_20180209_11.png"></p>
<h1 id="どのようにして導入したか"><a href="#どのようにして導入したか" class="headerlink" title="どのようにして導入したか"></a>どのようにして導入したか</h1><p>今回、PJ(<strong>エンプラ</strong>&amp;<strong>オンプレ</strong>)にInfrastructure as Code導入成功したわけですが、やはり一筋縄にはいかないわけで。。ここからは導入の過程で工夫した点をご紹介します。</p>
<h2 id="目指したサーバ管理方法"><a href="#目指したサーバ管理方法" class="headerlink" title="目指したサーバ管理方法"></a>目指したサーバ管理方法</h2><p>まず、Infrastructure as Codeを導入することでどのようなサーバ管理方法を実現するかのビジョンを定めました。</p>
<p><strong>1. インフラへの変更はソースコード経由で行い、手作業での変更は原則禁止する</strong><br><strong>2. 実環境とのソースコードの定義は自動突合を実施し、不整合を検知可能にする</strong></p>
<p>オンプレ(Mutable)だからこその考慮点がこれで、変更履歴を付けずに誰かが手でサーバを葬っちゃうともう<strong>Infrastructure as Codeの意味ってなくなっちゃう</strong>んですよね。クラウド(Immutable)だと、常に破棄→新規作成の繰り返しなのでこの点は考慮しなくてもよいですから。<br>私自身も含め<strong>「ちょっとの変更だし、あとで直せばよいから手で設定いじっちゃお。変更履歴は適当なメモで」</strong>って考えでサーバに変更を加える輩は絶対に存在します。だからこそ、そこを拾えるように実環境とソースコードの自動突合の部分にもこだわりました。</p>
<p><img src="/images/20180209/photo_20180209_12.png"></p>
<h2 id="作り上げたフレームワーク"><a href="#作り上げたフレームワーク" class="headerlink" title="作り上げたフレームワーク"></a>作り上げたフレームワーク</h2><ul>
<li>こんな感じのフレームワーク(と言っては大げさかもしれませんが)を作りました。</li>
<li>Ansibleの使い方だけではなく、PJに導入するために以下も考えました。<ul>
<li>ビジョン</li>
<li>手順書</li>
<li>ワークフロー</li>
</ul>
</li>
</ul>
<p><img src="/images/20180209/photo_20180209_13.png"></p>
<ul>
<li>構成は以下のようなイメージです。</li>
<li>JenkinsとGitlabを駆使して、継続的にAnsible実行&amp;フィードバックができるような仕組みを心掛けました。</li>
</ul>
<p><img src="/images/20180209/photo_20180209_14.png"></p>
<p>後述する”特に工夫したこと”で詳細記載します。</p>
<h2 id="特に工夫したこと"><a href="#特に工夫したこと" class="headerlink" title="特に工夫したこと"></a>特に工夫したこと</h2><p>いろいろな面で工夫しました。ほとんどは発表資料に記載しているので、ここでは特に工夫をしたことを紹介します。大きく三つあります。</p>
<ul>
<li><strong>1. ただ新しい考え方を押し付けるのはダメ。今までの考え方に歩み寄る</strong></li>
<li><strong>2. メンバーみんなにキャッチアップを求められない。みんなが利用できる仕組みを作る</strong></li>
<li><strong>3. あきらめない</strong></li>
</ul>
<h3 id="1-考え方を押し付けるのはダメ。今までの考え方に歩み寄る"><a href="#1-考え方を押し付けるのはダメ。今までの考え方に歩み寄る" class="headerlink" title="1. 考え方を押し付けるのはダメ。今までの考え方に歩み寄る"></a><strong>1. 考え方を押し付けるのはダメ。今までの考え方に歩み寄る</strong></h3><h4 id="Ansibleで全部やろうとしない"><a href="#Ansibleで全部やろうとしない" class="headerlink" title="Ansibleで全部やろうとしない"></a>Ansibleで全部やろうとしない</h4><ul>
<li>インフラ構築すべてをAnsibleで実装。は理想ですが、ベンダが構築する部分があったりと、すべてをソースコード化するのは受け入れ側も導入側にもパワーが足りなかったので以下の用途で使用を留めました。しかし、これだけでも効果はかなり大きかったです。<ul>
<li>ファイル配布</li>
<li>ディレクトリ作成</li>
<li>ユーザグループ作成</li>
<li>パッケージインストール</li>
</ul>
</li>
</ul>
<h4 id="Ansibleのベストプラクティスが私たちにベストとは限らない"><a href="#Ansibleのベストプラクティスが私たちにベストとは限らない" class="headerlink" title="Ansibleのベストプラクティスが私たちにベストとは限らない"></a>Ansibleのベストプラクティスが私たちにベストとは限らない</h4><ul>
<li>Ansibleのベストプラクティスに<strong>あえて従わなかった</strong>ことで今までの考え方に歩み寄りできるようにしました。</li>
<li>私たちが使うAnsibleの構成は下図のように、<strong>“role”=”サーバ種”</strong>となっています。(本来ならば”role”=”mysql”とか)</li>
<li>また、Ansibleでのファイル管理に関しては、<strong>“動的ファイル(Jinja Templating)”は一切使っていません。</strong></li>
<li>インフラをソースコード化するというプロセスに<strong>拒否反応を抱かせないよう</strong>、今までの考え方に親和するようにしました。</li>
</ul>
<p>※ このせいで、taskファイルや静的ファイルが二重管理になってしまいますが、それは後述のtaskファイル自動生成で補うことができました。</p>
<p><img src="/images/20180209/photo_20180209_15.png"></p>
<h3 id="2-メンバーみんなにキャッチアップを求められない。みんなが利用できる仕組みを作る"><a href="#2-メンバーみんなにキャッチアップを求められない。みんなが利用できる仕組みを作る" class="headerlink" title="2. メンバーみんなにキャッチアップを求められない。みんなが利用できる仕組みを作る"></a><strong>2. メンバーみんなにキャッチアップを求められない。みんなが利用できる仕組みを作る</strong></h3><h4 id="Ansibleファイルが書けない-それならExcelから自動生成しよう"><a href="#Ansibleファイルが書けない-それならExcelから自動生成しよう" class="headerlink" title="Ansibleファイルが書けない? それならExcelから自動生成しよう"></a>Ansibleファイルが書けない? それならExcelから自動生成しよう</h4><ul>
<li>導入当初はAnsibleソースを書けるのは私だけの状態。</li>
<li>他のインフラ構築メンバにも使ってもらわないと、全然回らないと感じてExcelに書いた定義から<strong>Ansibleファイルを自動生成</strong>するような仕組みを作りました。</li>
<li>AnsibleがわからなくてもExcelという<strong>慣れ親しんだインターフェース</strong>を設けたことで、Ansibleでの構築はかなり軌道に乗りました。<br>※ Excel1行 → YAML形式 と変換するだけだったので簡単なPythonスクリプトで済んだのも助かりました。</li>
</ul>
<h4 id="Gitが使えない-それならJenkinsにコミットしてもらおう"><a href="#Gitが使えない-それならJenkinsにコミットしてもらおう" class="headerlink" title="Gitが使えない? それならJenkinsにコミットしてもらおう"></a>Gitが使えない? それならJenkinsにコミットしてもらおう</h4><ul>
<li>PJは、Gitを使える人は数人いるかどうか、という状態でした。</li>
<li>ただでさえAnsibleという新しいツールを導入するのに、Gitも、、は厳しそうだったのでGit操作はJenkinsに任せることにしました。</li>
<li>歩み寄って、AnsibleファイルもSVNで管理、、、も考えましたがここだけは譲れませんでした。</li>
</ul>
<p><img src="/images/20180209/photo_20180209_16.png"></p>
<h3 id="3-あきらめない"><a href="#3-あきらめない" class="headerlink" title="3. あきらめない"></a><strong>3. あきらめない</strong></h3><h4 id="絶対にサーバを手で葬らせない"><a href="#絶対にサーバを手で葬らせない" class="headerlink" title="絶対にサーバを手で葬らせない"></a>絶対にサーバを手で葬らせない</h4><p>あとは執念です。。</p>
<p><strong>“環境がカオスになったのを直していく苦労 &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Ansibleを導入する苦労”</strong><br>であると私は確信していたので最後まであきらめませんでした。</p>
<p><strong>「このディレクトリ、時間ないので手で作っちゃってもいいですか？」</strong><br><strong>「新しい設定ファイル必要になったので、手で作っちゃっていいですか？」</strong></p>
<p>他のインフラ構築メンバからそんな声がたくさん聞こえてきます。たとえ作業をし終わった後でもAnsibleのソースコード化をしないと、その構築範囲は<strong>構成管理ができなくなります。</strong>それでは意味がありません。なので、Ansibleを使える人だけで人が行った手作業をソースコード化を継続するのが非常に大変でした。。しかし、あきらめてはダメです。(途中からExcelソース自動化を導入したのでだいぶ楽にはなりましたが、軌道にのるまでは辛かった)<br>※Ansibleは冪統性があるので、いったん手で作業したところをソースコード化しても問題ない</p>
<h4 id="分かってもらえるまで説明する"><a href="#分かってもらえるまで説明する" class="headerlink" title="分かってもらえるまで説明する"></a>分かってもらえるまで説明する</h4><p>インフラチームにはすんなり受け入れられたのですが、チーム全体にInfrastructure as Codeという全く新しい考え方をわかってもらうのもなかなか時間がかかりました。<br>しかし、<strong>開発だけではなく運用でも</strong>使っていってもらうためには保守・運用チーム含めチーム全体に考え方を理解してもらう必要があります。これもあきらめてはダメです。そのためにチーム各所に何度も何度もプレゼンを行いました。<br>Ansibleは簡単に使いやすいとは言え、いままでのサーバ管理の考え方とはガラッと変わってしまうため、最初のほうは？？？？？が連発ですが、継続的に普及活動を行ったので腹落ちして理解してもらえたと思います。資料にも記載しましたが、上の人から攻めていくのがおすすめです。あとは、実演して見せることも。</p>
<ul>
<li>導入のために作った説明資料の数々</li>
</ul>
<p><img src="/images/20180209/photo_20180209_17.png"></p>
<ul>
<li>併せてDevOps勉強会も実施しました</li>
</ul>
<p><img src="/images/20180209/photo_20180209_18.png"></p>
<h1 id="導入の成果"><a href="#導入の成果" class="headerlink" title="導入の成果"></a>導入の成果</h1><p>なによりPJが無事にリリースできたのが大きな成果でしたが、以下も功績が非常に大きかったです。</p>
<h2 id="作業自動化"><a href="#作業自動化" class="headerlink" title="作業自動化"></a>作業自動化</h2><p>今までサーバへの変更は以下のような<strong>“ネ伸Excel”</strong>で手順を書いて、レビューしてもらって、<strong>手作業</strong>で実施していたものが、Ansibleによって<strong>自動化</strong>することができました。これにより、工数も大きく削減できましたし、ヒューマンエラーもなくなりました。なにより、手順書作るのも、作業実施するのもとにかく<strong>“楽”</strong>になりましたね。</p>
<ul>
<li>今までの手順書と新しい手順書</li>
</ul>
<p><img src="/images/20180209/photo_20180209_19.png"><br><img src="/images/20180209/photo_20180209_20.png"></p>
<h2 id="構成管理"><a href="#構成管理" class="headerlink" title="構成管理"></a>構成管理</h2><p>今までは<strong>SVNのExcel上</strong>でしかできていなかったMWやOSの設定ファイル、またディレクトリの権限の構成管理は、<strong>ソースコードとして管理</strong>することができるようになりました。(ファイル、ディレクトリのパーミッション含め)<br>アプリの世界では当たり前ですが、コミットメッセージに<strong>変更の経緯</strong>も残せますし、Gitのマージリクエストで<strong>承認依頼</strong>も出せます。当たり前のことがやっとできるようになったって感じです。<br>また、Git上にファイルをすべて管理しているので各サーバに入って設定値確認、なんて効率の悪いことは一切必要ありません。</p>
<ul>
<li>Postfixの設定を変更した際の例</li>
</ul>
<p><img src="/images/20180209/photo_20180209_21.png"></p>
<h1 id="補足：導入までの時間"><a href="#補足：導入までの時間" class="headerlink" title="補足：導入までの時間"></a>補足：導入までの時間</h1><p><img src="/images/20180209/photo_20180209_22.png"></p>
<p>導入までの歩みをまとめてみました。開発(Dev)は一人でガンガン作りこんでいってしまえばよいだけなのでそんなに大変ではありませんでしたが、運用(Ops)でも使えるように、いろいろな関係者を巻き込んでいくのはなかなか骨が折れました。ただ、保守・運用チームが積極的に協力してくれたので、なんとか運用でも使えるようになるまでこぎつけることができました。感謝です。</p>
<h1 id="最後に"><a href="#最後に" class="headerlink" title="最後に"></a>最後に</h1><p>Ansibleのようなツールはどんどん新しいものが出てきて、日に日に便利になっていきます。<br>私たちのPJは”ユニコーン”ではないので<strong>工夫と時間</strong>がかなり必要ですが、それでもその恩恵を受けることはできます。<br>実際に導入までこぎつけて、<strong>“先進的なツール”と”今までのPJのやり方や文化”、これをどう結びつけるかが重要だと感じました。</strong></p>
<p>また、ここまでの仕組みを整えられたのもPJメンバの協力があったからだと感じています。<br>この仕組み作りの重要性を理解し「どんどんやっちゃっていいよー」とGOをくれたリーダに感謝です。<br>また、一緒にワークフローを検討し運用での利用の仕組みを整えてくれた保守・運用チームにも感謝です。<br>勉強会の交流会や、Twitterでは「Ansible広めたいけど、チームの誰も賛同してくれない」と言っている人はたくさんいました。(うちっていい会社ですね！)</p>
<p>と言っても、まだまだこの仕組みは使い始められて間もないのでこれからさらに改良を重ねていきたいと考えています。</p>
<p>(注意)<br>資料に”おじさん”という表現がありますが、プレゼンでのウケを狙ったものです。本当は”おじさん”とか思っていません。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;エンプラ&amp;amp;オンプレでもAnsible導入成功したのでユーザー会で発表してきた&lt;br&gt;こんにちは。齋場です。&lt;br&gt;少し前ですが、弊社でAnsibleを導入した事例をAnsibleユーザ会で発表してきました。(どちらも5minのLTですが) Ansibleを導入したお
    
    </summary>
    
      <category term="Infrastructure" scheme="https://future-architect.github.io/categories/Infrastructure/"/>
    
    
      <category term="Infrastructure" scheme="https://future-architect.github.io/tags/Infrastructure/"/>
    
      <category term="Ansible" scheme="https://future-architect.github.io/tags/Ansible/"/>
    
  </entry>
  
  <entry>
    <title>第1回Future開発合宿</title>
    <link href="https://future-architect.github.io/articles/20171217/"/>
    <id>https://future-architect.github.io/articles/20171217/</id>
    <published>2017-12-17T05:28:42.000Z</published>
    <updated>2017-12-17T05:40:06.696Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://adventar.org/calendars/2449" target="_blank" rel="noopener">フューチャーアーキテクト裏アドベントカレンダー2017</a>の16日目です。</p>
<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>開発合宿とは…<br>新しいナニカを生み出すための儀式である…</p>
<p>みなさまこんにちは。早2年目になりました谷村です。</p>
<p>2017年はWeb業界で開発合宿が何かと話題になる年でしたね。流れに乗ってFutureでも合宿を開催してみたので、今後合宿を開催してみたい方や、合宿行ってみたいけど迷ってる方向けにレポートします。</p>
<p>なお、Futureでいう開発合宿は、各々がプライベートで開発したいものを開発する有志活動です。</p>
<h1 id="開催のきっかけ"><a href="#開催のきっかけ" class="headerlink" title="開催のきっかけ"></a>開催のきっかけ</h1><p>ある日、<a href="https://github.com/future-architect/vuls" target="_blank" rel="noopener">Vuls</a>でおなじみの神戸さんが、<a href="https://go-beginners.connpass.com/event/47481/" target="_blank" rel="noopener">Golangの合宿</a>があるらしいぞと持ちかけてきまして、<a href="https://dotstud.io/blog/go-beginners-camp-report/" target="_blank" rel="noopener">行ってみたらめっちゃ楽しかった</a> です！</p>
<p>Futureでもやろうよ、と言ってみたら、</p>
<blockquote>
<p>いいじゃん、感動した。<br>谷村くん、スゴイわ。<br>こんな天才がいたとは素晴らしいわ。</p>
</blockquote>
<p>とおだてられたので開催してみました。</p>
<h1 id="開催準備"><a href="#開催準備" class="headerlink" title="開催準備"></a>開催準備</h1><p>簡単3stepで、1人で幹事が出来ました。<br>基本的にGo合宿を <del>パク</del> リスペクトしました。</p>
<ol>
<li><a href="http://www.dozenryokan.com" target="_blank" rel="noopener">土善旅館さん</a>に「Go合宿と同じように開催したいんでよろしく。」と伝える</li>
<li>社内で声かけてまわって、人を集める</li>
<li>Go合宿のしおりを <del>パク</del> リスペクトして作成した<a href="https://gist.github.com/tng527/70a3af19aad64c6dfa0f4214868a4a7e" target="_blank" rel="noopener">しおり</a>を参加者に投げる</li>
</ol>
<p>※支払いは、「LINE Payでよろしく！」と自分のIDを晒しておくと、勝手にお金が送られてきます。マジ神</p>
<h1 id="当日まとめ"><a href="#当日まとめ" class="headerlink" title="当日まとめ"></a>当日まとめ</h1><p>土善旅館さんが最高過ぎてブログが長くなるので、先にまとめておきます。</p>
<ul>
<li>開発部屋が広い！20m四方くらい？</li>
<li>NW強い！まさかの冗長化構成！</li>
<li>外部ディスプレイ無料！開発捗る！！</li>
<li>スクリーンでっかい！100インチくらい？深夜からアニメの上映会もやりました。</li>
<li>夕食うまい！しゃぶしゃぶとか寿司とかめっちゃ豪華！</li>
<li>安い安いアンド安い！ 会費13,000円で1泊3食デザート付、お酒飲み放題！</li>
</ul>
<p><img src="/images/20171217/photo_20171217_01.jpeg"></p>
<h1 id="当日の様子詳細"><a href="#当日の様子詳細" class="headerlink" title="当日の様子詳細"></a>当日の様子詳細</h1><p>それでは順を追って合宿の様子を写真メインでお送りします。<br>土善旅館さんは千葉の奥地にあるので、みんなで電車で向かいます。3時間くらい。</p>
<p><img src="/images/20171217/photo_20171217_02.jpeg"><br>移動中から開発合宿は始まっているのだ…！</p>
<p><img src="/images/20171217/photo_20171217_03.jpeg"></p>
<p>最寄りの笹川駅に着いたら、宿に向かう前にご飯を食べます。<br>青柳亭のしじみ丼がこの辺の名産らしい。</p>
<p><img src="/images/20171217/photo_20171217_04.jpeg"><br>ようやく宿に到着です。歴史ある感じですが、清潔で気持ち良いです。</p>
<p><img src="/images/20171217/photo_20171217_05.jpeg"><br>到着したらまずは乾杯、これ基本。</p>
<p><img src="/images/20171217/photo_20171217_06.jpeg"><br>広い机と外部ディスプレイ、電源タップ、それと<a href="https://yogibo.jp" target="_blank" rel="noopener">人を駄目にする枕高級版</a>が用意されています。</p>
<p><img src="/images/20171217/photo_20171217_07.jpeg"><br>物理的な開発環境を整えたら、各々のスタイルで開発を始めます。</p>
<p><img src="/images/20171217/photo_20171217_08.jpeg"><br>お酒片手に。</p>
<p><img src="/images/20171217/photo_20171217_09.jpeg"><br>開発に飽きた頃に、講義とかやってみたり。<br>写真は競技プログラミング部長・塚本さんのアルゴリズム講座です。</p>
<p><img src="/images/20171217/photo_20171217_10.jpeg"><br>今回は各々が好きなものを開発するスタイルの合宿ですが、<br>一人じゃないから質問や相談も可能！！</p>
<p><img src="/images/20171217/photo_20171217_11.jpeg"><br>ハマったタイミングで看板猫が癒やしを運んできます。</p>
<p><img src="/images/20171217/photo_20171217_12.jpeg"><br>開発しているとご飯の時間がやってまいりました。<br>別のお座敷が用意されるという豪華っぷり。</p>
<p><img src="/images/20171217/photo_20171217_13.jpeg"><br>当然ご飯も豪華です。</p>
<p><img src="/images/20171217/photo_20171217_14.jpeg"><br>食後は卓球で腹ごなしです。なんだ、ただの最高の旅館か。</p>
<p><img src="/images/20171217/photo_20171217_15.jpeg"><br>ロデオボーイも完備。スポッチャ以外で乗る日が来るとは。</p>
<p><img src="/images/20171217/photo_20171217_16.jpeg"><br>そしてまたひたすら開発…</p>
<p><img src="/images/20171217/photo_20171217_17.jpeg"><br>てっぺん(24時)を超えたら集合写真です。エンジニアの夜は遅い。</p>
<p><img src="/images/20171217/photo_20171217_18.jpeg"><br>そして開発合宿の朝は早い。朝ごはんは優しいお味で健康的です。</p>
<p><img src="/images/20171217/photo_20171217_19.jpeg"><br><img src="/images/20171217/photo_20171217_20.jpeg"><br>そして開発合宿の締め。成果発表会です。<br>各々の開発成果についてアピールします。</p>
<p><img src="/images/20171217/photo_20171217_21.jpeg"><br>旅館からのサービスのデザートを食べながら真剣に聞いています。<br>なんだただのサービスが最高の旅館か。</p>
<p><img src="/images/20171217/photo_20171217_22.jpeg"><br>投票でスゴかった発表を決め、<del>余っ</del>景品のお酒を贈呈しました。</p>
<h1 id="振り返り"><a href="#振り返り" class="headerlink" title="振り返り"></a>振り返り</h1><h3 id="keep"><a href="#keep" class="headerlink" title="keep"></a>keep</h3><ul>
<li>旅館の環境・サービスが良すぎる</li>
<li>コンテンツ(講義・LT)が充実していた</li>
<li>LT全員強制参加が良かった</li>
</ul>
<h3 id="problem"><a href="#problem" class="headerlink" title="problem"></a>problem</h3><ul>
<li>LINE Payの導入連絡が遅い</li>
<li>お酒の種類が少ない(ビール以外もほしい)</li>
<li>場所が遠かった(電車で片道約3時間)</li>
</ul>
<h3 id="try"><a href="#try" class="headerlink" title="try"></a>try</h3><ul>
<li>土善旅館以外での開催</li>
<li>人数の規模を大幅に拡大して開催</li>
<li>取り組むことのテーマを縛って開催</li>
</ul>
<p>本合宿は2017/6/10-11に実施しました。<br>第2回は2017/12/16-17を予定しています。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://adventar.org/calendars/2449&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;フューチャーアーキテクト裏アドベントカレンダー2017&lt;/a&gt;の16日目です。&lt;/p&gt;
&lt;h1 id=&quot;はじめに&quot;&gt;&lt;a
    
    </summary>
    
      <category term="Culture" scheme="https://future-architect.github.io/categories/Culture/"/>
    
    
      <category term="Camp" scheme="https://future-architect.github.io/tags/Camp/"/>
    
  </entry>
  
  <entry>
    <title>Future IoT デバイス</title>
    <link href="https://future-architect.github.io/articles/20171207/"/>
    <id>https://future-architect.github.io/articles/20171207/</id>
    <published>2017-12-07T04:00:00.000Z</published>
    <updated>2017-12-13T03:25:25.306Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://qiita.com/advent-calendar/2017/future" target="_blank" rel="noopener">フューチャーアーキテクト Advent Calendar 2017</a>の7日目です。</p>
<hr>
<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>2017年、社内のR&amp;Dチームにて開発した汎用IoTデバイスについて紹介したいと思います。<br>デバイスの各辺の長さは約4.5cmの立方体で、異なる拡張モジュールを最大8つ内蔵、もしくはGroveコネクタ準拠のセンサーなどを外付けできる形になっています。</p>
<p>こんな感じのサイコロ型のデバイスです。<br><img src="/images/20171207/photo_20171207_02.jpeg"></p>
<p>アップすると FutureIoT のロゴが印字されています。<br><img src="/images/20171207/asset_20171207_01.png"></p>
<p>ちなみに、なんで鶴なんだ？とよく聞かれるのでこの場をお借りして回答しておきます。</p>
<p><strong>FutureIoTのロゴの由来</strong></p>
<ul>
<li>点はデバイスやセンサーで、線がネットワークで、様々なコネクティッドを表しています</li>
<li>鶴を形どっていて縁起がよく「仲良きことの象徴」の鳥です</li>
<li>鳴き声が共鳴して遠方まで届くことから「天に届く＝天上界に通ずる鳥」といわれるなどのシンボルなので、遠隔のフィールドの情報がネットワークの先（クラウド）まで届くこと祈願してます</li>
</ul>
<p>ロゴデザインは<a href="https://99designs.jp" target="_blank" rel="noopener">99design</a>さん経由にて作成を依頼しました。</p>
<h1 id="作成の目的"><a href="#作成の目的" class="headerlink" title="作成の目的"></a>作成の目的</h1><p>このデバイスは、IoT関連ソリューションのワークショップ、<a href="https://ja.wikipedia.org/wiki/%E6%A6%82%E5%BF%B5%E5%AE%9F%E8%A8%BC" target="_blank" rel="noopener">PoC</a>、プロトタイピング、パイロット導入などの用途をカバーすることを目指して開発したものとなります。<br>直近では、お客様向けのIoT関連研修での教材として10数台程利用しました。</p>
<h1 id="力を入れた点"><a href="#力を入れた点" class="headerlink" title="力を入れた点"></a>力を入れた点</h1><p>とにかく簡易に素早く使えることをコンセプトに、下記の特徴をもたせました。</p>
<ul>
<li>多くの拡張用コネクタ(Groveコネクタx8)をコンパクトな筐体(4.5cm)に格納</li>
<li>ユーザを選ばない幅広いプログラミング環境</li>
<li>フルワイヤレス</li>
<li>追加モジュールによる柔軟な拡張性</li>
</ul>
<h1 id="スペック"><a href="#スペック" class="headerlink" title="スペック"></a>スペック</h1><p>スペックは次の通りです。（随時ブラッシュアップしているので変更される/している可能性があります）</p>
<h2 id="ハードウェア"><a href="#ハードウェア" class="headerlink" title="ハードウェア"></a>ハードウェア</h2><p><img src="/images/20171207/photo_20171207_03.jpeg"></p>
<h3 id="MPU"><a href="#MPU" class="headerlink" title="MPU"></a><a href="https://ja.wikipedia.org/wiki/%E3%83%9E%E3%82%A4%E3%82%AF%E3%83%AD%E3%83%97%E3%83%AD%E3%82%BB%E3%83%83%E3%82%B5" target="_blank" rel="noopener">MPU</a></h3><p>MPUとしては、Wi-Fi、BLEも内蔵した比較的安価なESP32を採用しました。</p>
<ul>
<li><a href="https://ja.wikipedia.org/wiki/ESP32" target="_blank" rel="noopener">ESP32</a></li>
</ul>
<h3 id="通信"><a href="#通信" class="headerlink" title="通信"></a>通信</h3><p>基本的に無線での運用となります。Wi-Fi APやLoRaWAN GW などを経由してクラウドに接続します。</p>
<ul>
<li>有線<ul>
<li>シリアル通信(USB)</li>
</ul>
</li>
<li>無線<ul>
<li><a href="https://ja.wikipedia.org/wiki/Wi-Fi" target="_blank" rel="noopener">Wi-Fi</a></li>
<li><a href="https://ja.wikipedia.org/wiki/Bluetooth_Low_Energy" target="_blank" rel="noopener">BLE</a></li>
<li><a href="https://ja.wikipedia.org/wiki/Long_Term_Evolution" target="_blank" rel="noopener">LTE</a>(オプション)</li>
<li><a href="https://ja.wikipedia.org/wiki/LPWA_(%E7%84%A1%E7%B7%9A" target="_blank" rel="noopener">LoRaWAN</a>#LoRa)(オプション)</li>
</ul>
</li>
</ul>
<h3 id="コネクタ"><a href="#コネクタ" class="headerlink" title="コネクタ"></a>コネクタ</h3><p>コネクタとしては、PCなどの通信や充電のためのmicroUSBと、センサーなどの外部モジュールや内蔵モジュールのためのGroveコネクタがあります。</p>
<ul>
<li>microUSB コネクタ x1<ul>
<li>充電</li>
<li>シリアル通信</li>
</ul>
</li>
<li>Grove コネクタ x8<ul>
<li>デジタル入力</li>
<li>アナログ入力</li>
<li>UART</li>
<li>I2C</li>
</ul>
</li>
</ul>
<h3 id="ストレージ"><a href="#ストレージ" class="headerlink" title="ストレージ"></a>ストレージ</h3><p>基本的に内蔵Flashにプログラムを書き込みますが、大きめのデータを利用したい場合はスロットにmicroSDを差して使用する形になります。</p>
<ul>
<li>内蔵Flash</li>
<li>microSDスロット(対応予定)</li>
</ul>
<h3 id="電源"><a href="#電源" class="headerlink" title="電源"></a>電源</h3><p>充電池を内蔵しているためワイヤレスな利用が可能です。特に低消費電力を考えたプログラミングを行わなくても満充電後数時間は利用できます。プログラム次第になりますがより長時間の運用も可能です。電源を外部から取ることも可能です。</p>
<ul>
<li>内蔵：Li-Poバッテリ</li>
<li>外付：microUSBケーブルでACアダプタ、PC、モバイルバッテリと接続</li>
</ul>
<h2 id="ソフトウェア"><a href="#ソフトウェア" class="headerlink" title="ソフトウェア"></a>ソフトウェア</h2><h3 id="OS"><a href="#OS" class="headerlink" title="OS"></a>OS</h3><p>現在はその完成度の面から下記のOSを利用しています。</p>
<ul>
<li><a href="https://mongoose-os.com/" target="_blank" rel="noopener">Mongoose OS</a></li>
</ul>
<h3 id="プログラミング言語"><a href="#プログラミング言語" class="headerlink" title="プログラミング言語"></a>プログラミング言語</h3><p>非エンジニアまでターゲット層を広げているため、複数のプログラミング環境をWeb上に用意しています。</p>
<ul>
<li>プログラミング初学者向け<ul>
<li><a href="https://developers.google.com/blockly/" target="_blank" rel="noopener">Blockly</a></li>
</ul>
</li>
<li>Web開発者向け<ul>
<li>JavaScript</li>
</ul>
</li>
<li>組込開発者向け<ul>
<li>C++</li>
<li>C</li>
</ul>
</li>
</ul>
<p><img width="400" alt="Blocklyの編集画面" src="/images/20171207/asset_20171207_04.jpeg"></p>
<p><img width="400" alt="JavaScriptの編集画面" src="/images/20171207/asset_20171207_05.png"></p>
<h2 id="クラウド"><a href="#クラウド" class="headerlink" title="クラウド"></a>クラウド</h2><p>弊社のIoTプラットフォームである Future IoT との連携や、AWS、GCP、Azuleなどの主要なクラウドとの連携を行うことができます。</p>
<h2 id="対応プロトコル"><a href="#対応プロトコル" class="headerlink" title="対応プロトコル"></a>対応プロトコル</h2><ul>
<li><a href="https://ja.wikipedia.org/wiki/MQ_Telemetry_Transport" target="_blank" rel="noopener">MQTT</a></li>
<li><a href="https://ja.wikipedia.org/wiki/WebSocket" target="_blank" rel="noopener">WebSocket</a></li>
<li>HTTP</li>
</ul>
<h1 id="今後の展開"><a href="#今後の展開" class="headerlink" title="今後の展開"></a>今後の展開</h1><p>今後、次のような点を進めていく予定です。<br>ぜひ、興味がある方は連絡下さい！</p>
<ul>
<li>追加モジュールの拡充</li>
<li>プログラミング環境のブラッシュアップ</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://qiita.com/advent-calendar/2017/future&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;フューチャーアーキテクト Advent Calendar 2017&lt;/a&gt;の7日目です。&lt;/p&gt;
&lt;
    
    </summary>
    
      <category term="IoT" scheme="https://future-architect.github.io/categories/IoT/"/>
    
    
      <category term="IoT" scheme="https://future-architect.github.io/tags/IoT/"/>
    
  </entry>
  
  <entry>
    <title>最新テクノロジーでスポーツアイデアソンをやって中高生に教わった大事なこと</title>
    <link href="https://future-architect.github.io/articles/20171113/"/>
    <id>https://future-architect.github.io/articles/20171113/</id>
    <published>2017-11-13T04:08:13.000Z</published>
    <updated>2017-11-13T05:55:45.429Z</updated>
    
    <content type="html"><![CDATA[<h2 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h2><p>こんにちは、フューチャーアーキテクト入社2年目の姜です。</p>
<p>2017年8月2日（水）に、有志で集まった社内オリンピック事業企画チーム「Future Tokyo2020」のメンバーを中心に、中学生・高校生向けの「<strong>スポーツアイデアソン</strong>」を開催しました。</p>
<h2 id="「スポーツアイデアソン」とは？"><a href="#「スポーツアイデアソン」とは？" class="headerlink" title="「スポーツアイデアソン」とは？"></a>「スポーツアイデアソン」とは？</h2><p>「スポーツアイデアソン」とは、東京オリンピック・パラリンピック大会など、数年後東京で行われる大規模なスポーツイベントを、VRやMRなどの最新技術を使って課題解決するアイデアを生み出すイベントです。<br><a href="http://fif.jp/kidsyouth/report/it/ideathon2017.html" target="_blank" rel="noopener">スポーツアイデアソンの報告はこちら</a></p>
<p><img src="/images/20171113/photo_20171113_01.jpg"><br><img src="/images/20171113/photo_20171113_02.jpg"></p>
<p>当日来てくれた中高生には、より良いアイデアを生み出してもらうために、<strong>MRなど最新の”XRテクノロジー”</strong>を実際に体験してもらいました。<br><img src="/images/20171113/photo_20171113_03.jpg"></p>
<p>また、教育向けビジュアルプログラミング言語「<strong>Blockly</strong>」でVRプログラミングにも挑戦してもらいました。（なんてチャレンジング！）<br><img src="/images/20171113/photo_20171113_04.jpg"></p>
<p>最新テクノロジーに見て、触って、そこから生まれたアイデアはどれも魅力的なアイデアばかりでした。<br>ここでは私が企画を担当し、技術的にもチャレンジングな内容だった<strong>「VRプログラミング体験」</strong>の様子を、ご紹介致します。</p>
<h2 id="会場"><a href="#会場" class="headerlink" title="会場"></a>会場</h2><p>イベントは先日オープンしたばかりの、フューチャーアーキテクトの新オフィスで開催致しました！<br><img src="/images/20171113/photo_20171113_05.jpeg"><br><img src="/images/20171113/photo_20171113_06.jpeg"></p>
<h2 id="イベント開始"><a href="#イベント開始" class="headerlink" title="イベント開始"></a>イベント開始</h2><p>ついにイベントがスタート！<br>会場を見てみると集まってくれた中高生はざっと20人程度。<br>IT経験者は5名ほど、後は全員IT初心者。男女比もちょうどよいバランスで集まってくれました。</p>
<p>イベントのタイムテーブルです。<br><img src="/images/20171113/photo_20171113_10.jpg"></p>
<p>1日にしては濃厚なスケジュール。みんなついてこれるでしょうか？</p>
<h2 id="さっそくVRプログラミング体験スタート！"><a href="#さっそくVRプログラミング体験スタート！" class="headerlink" title="さっそくVRプログラミング体験スタート！"></a>さっそくVRプログラミング体験スタート！</h2><p>簡単なアイスブレイクとXRテクノロジーの紹介を行い、さっそくVRプログラミング体験です。今回、体験の教材として採用したのは <strong>CoSpaces</strong> というアプリケーションです。</p>
<p><img src="/images/20171113/asset_20171113_01.png" class="img-small-size"></p>
<p><a href="https://www.google.co.jp/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwju-6D1wuLVAhXHWrwKHR2LB6gQFggoMAA&amp;url=https%3A%2F%2Fcospaces.io%2F&amp;usg=AFQjCNHg0xT23Te0sb92fFshLQJr3xo-xg" target="_blank" rel="noopener">Cosapces公式ページはこちらから</a></p>
<p>CoSpacesとは、ドイツのミュンヘンとロシアのサンクトペテルブルグを拠点とするスタートアップ、Delightex社が開発したアプリケーションで、開発者がプログラミングやデザインのスキルを持っていなくてもVR環境を構築出来るアプリケーションです。</p>
<p><img src="/images/20171113/asset_20171113_02.png"></p>
<p>動物やテキストボックスをステージ上にアレンジ出来たり、新しいオブジェクトをよりシンプルにカスタマイズ、統合出来る環境を提供しています。<br>なにより、CoSpacesはプログラミングを直接書かなくても、命令文やコマンド名のパズルを組み合わせる <strong>Blockly</strong> という開発言語で、プログラムを作ることができます。</p>
<p><img src="/images/20171113/asset_20171113_03.png"><br><a href="https://developers.google.com/blockly/" target="_blank" rel="noopener">Blockly公式ページはこちらから</a></p>
<p>作成したプログラムはVRアプリケーションとして、iPadなどで実行する事ができます。<br>プログラミングが初体験の中高生でも、<strong>パズルで遊ぶような感覚でVRプログラミングができるスグレモノ</strong>です。</p>
<p>今回は中高生にCoSpacesを使って <strong>スゴい迷路</strong> を作ってもらいながら、合間にHoloLensでMR体験をしてもらいました。</p>
<blockquote>
<p><strong>知らない方の為に補足:</strong><br> <a href="http://www.moguravr.com/holorens-mr-matome/" target="_blank" rel="noopener">HoloLens</a>とは、Microsoftが提供するワイヤレスで頭につけるタイプのホログラフィックコンピューティングです。自分がその場にいながらバーチャルな空間と融合した世界が体験できます。</p>
</blockquote>
<p>次々とHoloLensを体験した子どもたちが順番を待っている子たちに興奮しながら感想を言ったり、積極的に社員に質問しながらCoSpacesで迷路を構築したりして、会場は一気に盛り上がりました！</p>
<h2 id="スゴい迷路の発表"><a href="#スゴい迷路の発表" class="headerlink" title="スゴい迷路の発表!"></a>スゴい迷路の発表!</h2><p>最終的に出来上がった中高生たちの「スゴい迷路」がこちらです。</p>
<p><img src="/images/20171113/photo_20171113_07.png"></p>
<p>私の感想は「ちゃんと迷路に個性が出ている・・・！！」という驚きです。<br>最初のデフォルトの迷路に比べて、短時間でこれだけ様子が変わったことに中高生の底力を感じました。<br>ほとんどの中高生がプログラミング自体が初めてだったのにも関わらず、しっかりとオリジナリティのある迷路を作り上げていたのです。</p>
<p>最後に、中高生のみんなでどれが一番「スゴい」か投票した結果、選ばれた迷路がこちら！<br><img src="/images/20171113/photo_20171113_08.jpg"></p>
<p>なにが「スゴい」かというと、迷路の壁を<strong>半透明</strong>にして、迷路をする人が戸惑うような仕掛けを加えたところです。<strong>ちなみに壁を透明にできることは、講義では教えていません。</strong> 中高生が自分たちで実際にCoSpacesを触って、試して、見つけて、生み出した工夫です。短時間でプログラミングの概念が身につくCoSpacesと、中高生たちの発想の柔軟さがぶつかって「スゴい迷路」が誕生しました。</p>
<p>私はこういうIT技術と人のアイデアが具体的な形になる瞬間が大好きです。</p>
<p>その後のアイデアソンでも、中高生たちの柔軟な発想力が光るアイデアが沢山生まれ、無事イベントは終了しました!<br><img src="/images/20171113/photo_20171113_09.jpg"></p>
<h2 id="さいごに"><a href="#さいごに" class="headerlink" title="さいごに"></a>さいごに</h2><p>私自身、CoSpacesを触ったのが今回が初めてというハラハラドキドキ状態でしたが、なんとか中高生のフォローができてほっとしました。</p>
<p>中高生と接して感じたのは、 <strong>新しい技術に好奇心と興味をもって取り組めば、IT経験の有無は関係なくモノを作り出すことはできる</strong> ということです。<br>そして <strong>何より「楽しむ」こと！これがモノを作る上で一番の原動力であること</strong> を中高生に教わりました。</p>
<p>今後もFuture Tokyo2020は、スポーツを舞台にして子供とテクノロジーを繋ぐイベント・企画をどんどん進めていきます！またみなさんにレポートできる機会があると思いますので、乞うご期待下さい！</p>
<p>最後まで読んでいただいた方、本当にありがとうございました。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h2&gt;&lt;p&gt;こんにちは、フューチャーアーキテクト入社2年目の姜です。&lt;/p&gt;
&lt;p&gt;2017年8月2日（水）に、有志で集まった社内オリン
    
    </summary>
    
      <category term="VR" scheme="https://future-architect.github.io/categories/VR/"/>
    
    
      <category term="CoSpaces" scheme="https://future-architect.github.io/tags/CoSpaces/"/>
    
  </entry>
  
  <entry>
    <title>最新で最強、AlphaGo Zero の解説</title>
    <link href="https://future-architect.github.io/articles/20171030/"/>
    <id>https://future-architect.github.io/articles/20171030/</id>
    <published>2017-10-30T04:35:51.000Z</published>
    <updated>2017-10-30T04:56:47.315Z</updated>
    
    <content type="html"><![CDATA[<h2 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h2><p>こんにちは。データサイエンスチームの李(碩)です。<br>以前、<a href="https://future-architect.github.io/articles/20170804/">古典的ゲームAIを用いたAlphaGo解説</a>に紹介したAlphaGoの最新バージョンが2017年10月19日に発表されました。<br>この記事では、最新のAlphaGo、AlphaGo Zero の仕組みについて紹介です。<br>本文を直接読みたい方はスライドをご覧になってください。</p>
<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/11fEDJ3WaXrLww" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/suckgeunlee/alphago-zero" title="AlphaGo Zero 解説" target="_blank">AlphaGo Zero 解説</a> </strong> from <strong><a href="https://www.slideshare.net/suckgeunlee" target="_blank">suckgeun lee</a></strong> </div></p>
<h2 id="AlphaGo-のバージョン"><a href="#AlphaGo-のバージョン" class="headerlink" title="AlphaGo のバージョン"></a>AlphaGo のバージョン</h2><p>AlphaGo には下記の4つのバージョンがあります。</p>
<pre><code>1. AlphaGo Fan 
  ・2015年10月にヨーロッパの囲碁チャンピオンFanに勝利
2. AlphaGo Lee 
  ・2016年3月に過去世界最強と呼ばれたイ・セドルに勝利
3. AlphaGo Master 
  ・2017年1月、オンラインで世界最強の棋士たちに60:0で勝利
4. AlphaGo Zero 
  ・2017年10月に論文発表
</code></pre><p>前回の<a href="https://future-architect.github.io/articles/20170804/">古典的ゲームAIを用いたAlphaGo解説</a>に紹介したAlphaGoのバージョンは「AlphaGo Fan」になります。AlphaGo ZeroはAlphaGo Fanとアーキテクチャレベルから大きく異なります。Fanの場合、二つのディープニューラルネットワーク(DNN)で構成され、その他にも結構複雑なアーキテクチャになっています。しかし、Zeroの場合は一つのDNNだけで、学習プロセスもすごく簡単になりました。簡単になったけど、学習は早く、性能も強力になったのです。</p>
<h2 id="AlphaGo-Zeroのすごいポイント"><a href="#AlphaGo-Zeroのすごいポイント" class="headerlink" title="AlphaGo Zeroのすごいポイント"></a>AlphaGo Zeroのすごいポイント</h2><h3 id="1-人がプレイしたデータを必要としない"><a href="#1-人がプレイしたデータを必要としない" class="headerlink" title="1. 人がプレイしたデータを必要としない"></a>1. 人がプレイしたデータを必要としない</h3><p>AlphaGo Zeroのすごいところは、以前のAlphaGoと違い、人がプレイしたデータを一切必要としないことです。以前のAlphaGoは、まず人がプレイした数百万の囲碁のデータで学習して、その後に自己対局を通じて強くなる形でした。しかしAlphaGo Zeroは最初から人のプレイデータ無しで、自己対局だけで学習していきます。AIを作る時に一番苦労をするのが、良質のデータを手に入れることです。多くの場合、データを集めるのがすごく大変だったり、データの質がよくなかったり、そもそもデータが無かったりします。AlphaGoはその苦労無しで学習してくれるのです。</p>
<h3 id="2-手作りインプットの削除"><a href="#2-手作りインプットの削除" class="headerlink" title="2. 手作りインプットの削除"></a>2. 手作りインプットの削除</h3><p>以前のAlphaGoはインプットに囲碁の背景知識が必要なデータを人が手作りして入力してました。しかし、AlphaGo Zeroのインプットは石の配置履歴だけです。つまり、AlphaGo Zeroは囲碁の背景知識が全くない状況で学習を始めるのです。背景知識なしで問題を解決するこの進化により、囲碁でない他の問題でも、AlphaGo Zeroは活用できると予測されています。</p>
<h3 id="3-圧倒的なパフォーマンス"><a href="#3-圧倒的なパフォーマンス" class="headerlink" title="3. 圧倒的なパフォーマンス"></a>3. 圧倒的なパフォーマンス</h3><p><img src="/images/20171030/photo_20171030_01.png"><br>AlphaGo Zeroはアーキテクチャが簡単になったわりにAlphaGo FanやLeeより圧倒的なパフォーマンスを誇ります。学習時間も、計算速度も比べられるものではありません。たった36時間で数か月学習したAlphaGo Leeを超えて、40日でAlphaGo Masterを超える。驚異的なスピードです。</p>
<h2 id="本資料の狙い"><a href="#本資料の狙い" class="headerlink" title="本資料の狙い"></a>本資料の狙い</h2><p>本資料ではAlphaGo Zeroの仕組みを分かりやすく解説します。AlphaGo Zeroはどう作られているかが知りたい方はぜひご覧になってください。</p>
<h2 id="本資料の目次"><a href="#本資料の目次" class="headerlink" title="本資料の目次"></a>本資料の目次</h2><ol>
<li>AlphaGo Zeroを構成する二つのパーツ <ol>
<li>ニューラルネットワーク(DNN)</li>
<li>モンテカルロ木探索(MCTS)</li>
</ol>
</li>
<li>AlphaGo Zeroの学習プロセス<ol>
<li>MCTSによる自己対局</li>
<li>DNNの学習</li>
<li>学習前後の性能比較</li>
</ol>
</li>
<li>AlphaGoの各種バージョン</li>
<li>AlphaGo Fan vs. AlphaGo Zero</li>
<li>AlphaGo Zeroの性能評価</li>
<li>教師あり学習 vs. 強化学習</li>
<li>人の動き予測</li>
<li>まとめ</li>
<li>参照論文</li>
</ol>
<p><a href="https://www.slideshare.net/suckgeunlee/alphago-zero?qid=c9839b2d-a0e1-4feb-b72c-063282fe9fc5&amp;v=&amp;b=&amp;from_search=1" target="_blank" rel="noopener">https://www.slideshare.net/suckgeunlee/alphago-zero?qid=c9839b2d-a0e1-4feb-b72c-063282fe9fc5&amp;v=&amp;b=&amp;from_search=1</a></p>
<h2 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h2><p>AlphaGo Zeroは人のデータを必要としない、そして囲碁の背景知識を全く使わないことで、他の領域でも活用できると思われています。今後、AlphaGo Zeroを元にどんな面白い課題を解決していくのか、すごく楽しみですね！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h2&gt;&lt;p&gt;こんにちは。データサイエンスチームの李(碩)です。&lt;br&gt;以前、&lt;a href=&quot;https://future-archit
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>社内ヘルプデスクをＡＩで！</title>
    <link href="https://future-architect.github.io/articles/20171005/"/>
    <id>https://future-architect.github.io/articles/20171005/</id>
    <published>2017-10-05T09:24:22.000Z</published>
    <updated>2017-10-06T06:04:36.957Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><ul>
<li>社内ヘルプデスク（Redmine）における管理対応の業務を効率化し、サービスレベルを上げたい</li>
<li>現状の課題<ul>
<li>起票されたチケットの解決にかかる時間が長い</li>
<li>原因の一つは、正しい担当者にチケットが割当てられず滞留することがあること<ul>
<li>処理されないチケットは、カテゴリが正しく設定されていないものが多かった</li>
<li>弊社の運用としてカテゴリ単位で専門的な担当者が割り当てられているので、カテゴリを間違うとやり取りが増え、解決までに時間がかかってしまいます</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>Redmineについては下記を参照下さい<br><a href="http://redmine.jp/overview/" target="_blank" rel="noopener">http://redmine.jp/overview/</a></p>
</blockquote>
<h1 id="作ったもの"><a href="#作ったもの" class="headerlink" title="作ったもの"></a>作ったもの</h1><p><img src="/images/20171006/photo_20171006_01.jpeg" class="img-small-size"></p>
<p>Redmineにチケットが新規に起票すると、過去のデータから自動的にカテゴリを設定する仕組みをDeepLearningを用いて作成しました。<br>これにより正しい担当者にチケットが割り当てられ、チケットの平均解決時間の向上を狙います。<br>この仕組に対して、親しみを持たせたいということで <strong>あいちゃん</strong> と命名しました。</p>
<p>例えば下記のような動きです。（※社内情報に触れそうなところは隠しています）</p>
<p><img src="/images/20171006/photo_20171006_02.png"></p>
<p>さらに、カテゴリが自動設定されたことに驚かないよう、振り分けた旨のコメントもセットで投稿するようにしました。</p>
<p><img src="/images/20171006/photo_20171006_03.png" class="img-middle-size"></p>
<p>万が一、間違った振り分けをしても許してもらえそうな新人さんキャラクターを演出しています。<br>今のところクレームは届いていないのですが、彼女の貢献も大きいと思います。</p>
<h1 id="採用技術"><a href="#採用技術" class="headerlink" title="採用技術"></a>採用技術</h1><ul>
<li>Python パッケージ<ul>
<li>conda (4.3.11) # pythonのパッケージ管理</li>
<li>python-redmine (2.0.2)</li>
<li>Keras (2.0.5)</li>
<li>tensorflow (1.2.0)</li>
<li>Janome (0.2.8) # 形態素解析</li>
</ul>
</li>
<li>Ruby パッケージ<ul>
<li>faraday(0.13.0) # HTTP client library</li>
</ul>
</li>
<li>ジョブ系<ul>
<li>Jenkins (2.7.4)</li>
</ul>
</li>
<li>ミドル<ul>
<li>Docker (17.03.1-ce)</li>
</ul>
</li>
</ul>
<h1 id="処理の流れと構成"><a href="#処理の流れと構成" class="headerlink" title="処理の流れと構成"></a>処理の流れと構成</h1><p>チケットの自動カテゴリ設定の処理フローです。</p>
<ol>
<li>Redmineにチケットが起票される</li>
<li>Redmineのweb hook pluginを使ってJenkinsジョブを呼び出す</li>
<li>JenkinsはKeras Dockerコンテナを起動</li>
<li>Kerasでチケットのカテゴリを判定を行う</li>
<li>カテゴリの判定結果をRedmineのWebAPI経由でチケットを更新</li>
<li>カテゴリに紐付いたヘルプデスク担当者に、Redmine経由で通知がなされる</li>
</ol>
<p><img src="/images/20171006/photo_20171006_04.png"></p>
<p>あいちゃんの実体は、Dockerコンテナ上のKeras(Tensorflow）＋ 連携用のRubyスクリプトです。<br>ユーザからはRedmineのカテゴリが、あいちゃんユーザから更新されたかのように見えます。</p>
<h1 id="あいちゃんを作成"><a href="#あいちゃんを作成" class="headerlink" title="あいちゃんを作成"></a>あいちゃんを作成</h1><p>まずは、 <strong>あいちゃん</strong> のコアとなるAI部分を開発します。<br><img src="/images/20171006/photo_20171006_05.png"></p>
<p>今回はKerasライブラリを使います。<br>Kerasで学習＆判定させるときに必要なフローは以下の1～3です。</p>
<ol>
<li>教師データを準備する</li>
<li>モデルを用意する</li>
<li>学習させる</li>
<li>判定させる</li>
</ol>
<h2 id="1-教師データの作成"><a href="#1-教師データの作成" class="headerlink" title="1. 教師データの作成"></a>1. 教師データの作成</h2><p>元となるデータは運用中のRedmineが利用するDBに蓄積されている3000件のデータです。</p>
<p>まずは教師データを作成します。<br>教師データとは、入力データとそれに対応した正解データ(バイナリ)のタプルです。</p>
<p>$$ 教師データ = (X(入力データ), Y(正解データ)) $$</p>
<p>今回は、「チケットの件名」と「チケットの内容」の文字列からカテゴリを出したいので、以下の形式です。</p>
<p>$$ X(入力データ) ＝　チケットの件名 + 内容の文字列 $$ $$ Y(正解データ) ＝　カテゴリ $$</p>
<p>入力データ（X)と正解データ（Y)の作成フローを下図にまとめました。</p>
<p><img src="/images/20171006/photo_20171006_06.png"></p>
<figure class="highlight py"><figcaption><span>サンプルコード.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 題名と本文を結合</span></div><div class="line">text = subject + description</div><div class="line"></div><div class="line"><span class="comment"># 文字列を形態素解析して、名詞、品詞等にわける</span></div><div class="line">tokenizedTexts = JanomeTokenizer().tokenize(text)</div><div class="line"></div><div class="line"><span class="comment"># 出現頻度で数字に変換し、配列化</span></div><div class="line">tokenizer = KerasTokenizer()</div><div class="line">seq = tokenizer.fit_on_texts(tokenizedTexts)</div><div class="line"></div><div class="line"><span class="comment"># 配列のパディング</span></div><div class="line">X = sequence.pad_sequences(seq)</div><div class="line"></div><div class="line"><span class="comment"># カテゴリＩＤを1次元配列に変換</span></div><div class="line"><span class="comment"># 実はKerasで教師データを作ると、バイナリデータしか扱えないため、ここで変換します。</span></div><div class="line">Y = np_utils.to_categorical(categoryId)</div><div class="line"><span class="comment">#例： </span></div><div class="line"><span class="comment">#1 →  1000000000000</span></div><div class="line"><span class="comment">#13 → 0000000000100</span></div></pre></td></tr></table></figure>
<h2 id="2-モデルを用意する"><a href="#2-モデルを用意する" class="headerlink" title="2.　モデルを用意する"></a>2.　モデルを用意する</h2><p>CNN、RNN、LSTMで技術検証を行った結果、最も正解率が高かったCNNを採用しました。<br>CNNを利用した実装は下記のようなイメージになります。</p>
<figure class="highlight py"><figcaption><span>モデルサンプル.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">model = Sequential()</div><div class="line"></div><div class="line"><span class="comment"># 入力層</span></div><div class="line">model.add(Embedding(max_features, </div><div class="line">    embedding_dims, <span class="comment">#50</span></div><div class="line">    input_length = maxlen,</div><div class="line">    dropout = <span class="number">0.2</span>))</div><div class="line"></div><div class="line"><span class="comment"># Convolution1D層</span></div><div class="line">model.add(Convolution1D(nb_filter = <span class="number">250</span>, </div><div class="line">    filter_length = <span class="number">3</span>, </div><div class="line">    border_mode = <span class="string">"valid"</span>,</div><div class="line">    activation = <span class="string">"relu"</span>,</div><div class="line">    subsample_length = <span class="number">1</span>))</div><div class="line"></div><div class="line"><span class="comment"># GlobalMaxPooling1D層:</span></div><div class="line">model.add(GlobalMaxPooling1D())</div><div class="line"></div><div class="line"><span class="comment"># 隠れ層</span></div><div class="line">model.add(Dense(hidden_dims))</div><div class="line">model.add(Dropout(<span class="number">0.2</span>))</div><div class="line">model.add(Activation(<span class="string">"relu"</span>))</div><div class="line"></div><div class="line"><span class="comment"># 出力層</span></div><div class="line">model.add(Dense(<span class="number">10</span>)) </div><div class="line">model.add(Activation(<span class="string">"sigmoid"</span>))</div><div class="line"></div><div class="line"><span class="comment"># 学習過程の設定</span></div><div class="line">model.compile(loss = <span class="string">"categorical_crossentropy"</span>,</div><div class="line">    optimizer = <span class="string">"adam"</span>,</div><div class="line">    metrics = [<span class="string">"accuracy"</span>])</div></pre></td></tr></table></figure>
<h2 id="3-modelで学習させる"><a href="#3-modelで学習させる" class="headerlink" title="3.　modelで学習させる"></a>3.　modelで学習させる</h2><p>教師データを設定します。<br>Kerasを用いると、モデルを作成後、<code>fix function</code> に <code>X(入力データ)</code> , <code>Y(正解データ)</code> , <code>epoch数</code>を渡すだけで学習できます。<br>※poch数とは、学習をさせる回数です。この回数を増やすと限界はありますが重みづけが最適化されていきます。</p>
<figure class="highlight py"><figcaption><span>教師データの設定＆学習サンプル</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.fit(X, Y, epoch...)</div></pre></td></tr></table></figure>
<h2 id="4-判定する"><a href="#4-判定する" class="headerlink" title="4. 判定する"></a>4. 判定する</h2><p>学習は終わっているので、あとはX(入力データ)を与えると判定できます！<br>また、後続で使うRedmine操作用に、結果を加工しておきます。</p>
<figure class="highlight py"><figcaption><span>カテゴリ判定サンプル</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">result = model.predict(predictX)</div><div class="line"></div><div class="line">dfPredicts = pds.DataFrame(retPredict, columns = uniqueCategoryIds)</div><div class="line">issues = &#123;&#125;</div><div class="line"><span class="keyword">for</span> (i, dfPredict) <span class="keyword">in</span> dfPredicts.iterrows():</div><div class="line">    <span class="comment"># 可能性が高いものから順から４つだけを取り出します。</span></div><div class="line">    sPredicts = dfPredict.sort_values(ascending = <span class="keyword">False</span>).nlargest(<span class="number">4</span>)</div><div class="line">    <span class="comment"># このredmine操作で使うので、issueオブジェクトに結果を格納します。</span></div><div class="line">    issue = &#123;</div><div class="line">        <span class="string">"subject"</span>: subject,</div><div class="line">        <span class="string">"category_names"</span>: [],</div><div class="line">        <span class="string">"category_ids"</span> : [],</div><div class="line">        <span class="string">"confidences"</span>: []</div><div class="line">        &#125;</div><div class="line">    <span class="keyword">for</span> (j, sPredict) <span class="keyword">in</span> enumerate(sPredicts):</div><div class="line">        category_name = re.split(<span class="string">'\[|\]| '</span>,sPredicts.index[j])[<span class="number">2</span>]</div><div class="line">        category_id = re.split(<span class="string">'\[|\]| '</span>,sPredicts.index[j])[<span class="number">1</span>]</div><div class="line">        issue[<span class="string">"category_names"</span>].append(category_name)</div><div class="line">        issue[<span class="string">"category_ids"</span>].append(int(category_id)),</div><div class="line">        issue[<span class="string">"confidences"</span>].append(sPredicts[j])</div></pre></td></tr></table></figure>
<h1 id="Redmineのチケットを更新"><a href="#Redmineのチケットを更新" class="headerlink" title="Redmineのチケットを更新"></a>Redmineのチケットを更新</h1><p>続いて、Redmine APIを使って、対象のチケットを更新します。<br><img src="/images/20171006/photo_20171006_07.png"></p>
<p>Python-RedmineとAPIキーを使って、対象のチケットを新しいカテゴリIDで更新します。<br>これをコンテナの最後の処理に差し込めば、Redmineのチケットが更新されます。</p>
<figure class="highlight rb"><figcaption><span>Redmineチケット更新サンプル.ruby</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">from redminelib import Redmine</div><div class="line"></div><div class="line">redmine = Redmine(<span class="string">'http://localhost/redmine'</span>, key=<span class="string">'***********************'</span>)</div><div class="line">issue = redmine.issue.get(issue_id)</div><div class="line">issue.category_id = category_id</div><div class="line">issue.save()</div></pre></td></tr></table></figure>
<p>以下を参考にしました</p>
<ul>
<li><a href="https://python-redmine.com/" target="_blank" rel="noopener">https://python-redmine.com/</a></li>
<li><a href="http://www.redmine.org/projects/redmine/wiki/Rest_Issues" target="_blank" rel="noopener">http://www.redmine.org/projects/redmine/wiki/Rest_Issues</a></li>
<li><a href="http://qiita.com/mima_ita/items/1a939db423d8ee295c85" target="_blank" rel="noopener">http://qiita.com/mima_ita/items/1a939db423d8ee295c85</a></li>
</ul>
<h1 id="Jenkinsジョブの作成"><a href="#Jenkinsジョブの作成" class="headerlink" title="Jenkinsジョブの作成"></a>Jenkinsジョブの作成</h1><p>今回はRedmineから直接Kerasコンテナを呼ばずに、間にJenkinsを経由させるアーキテクチャになっています。<br>そのため、DockerコンテナをキックするJenkinsジョブを作成します。</p>
<p><img src="/images/20171006/photo_20171006_08.png"></p>
<figure class="highlight sh"><figcaption><span>サンプルコマンド</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># KerasコンテナがいるジョブサーバにSSHで接続して、コンテナにticket_idを渡します。</span></div><div class="line"><span class="comment"># issue_idはparameter付きビルドでredmineから受け取ります。</span></div><div class="line">$ ssh jobserver docker <span class="built_in">exec</span> -t keras_container python3 update_issue.py <span class="variable">$issue_id</span></div></pre></td></tr></table></figure>
<p>以下を参考にしました</p>
<ul>
<li><a href="https://wiki.jenkins.io/display/JENKINS/Parameterized+Build" target="_blank" rel="noopener">https://wiki.jenkins.io/display/JENKINS/Parameterized+Build</a></li>
</ul>
<h1 id="Redmine-Pluginの作成"><a href="#Redmine-Pluginの作成" class="headerlink" title="Redmine Pluginの作成"></a>Redmine Pluginの作成</h1><p>RedmineとJenkinsを連携させる部分を作ります。<br>Redmineにチケットが起票されたイベントをトリガーにしてJenkinsジョブを呼び出します。</p>
<p><img src="/images/20171006/photo_20171006_09.png"></p>
<figure class="highlight rb"><figcaption><span>サンプル実装.rb</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">import faraday</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">WebhookListener</span> &lt; Redmine::Hook::<span class="title">Listener</span></span></div><div class="line">    <span class="comment"># issueが新規に作成されると呼ばれます。</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">controller_issues_new_after_save</span><span class="params">(context = &#123;&#125;)</span></span></div><div class="line">      issue = context[<span class="symbol">:issue</span>]</div><div class="line">      controller = context[<span class="symbol">:controller</span>]</div><div class="line">      <span class="keyword">return</span> unless webhooks</div><div class="line">      post(issue)</div><div class="line">    <span class="keyword">end</span></div><div class="line"></div><div class="line">    <span class="comment"># Jenkinsジョブにissue.idをパラメータ付きビルドで渡します。</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">post</span></span></div><div class="line">        conn = Faraday.new(<span class="symbol">:url</span> =&gt; <span class="string">'http://jenkins'</span>) <span class="keyword">do</span> <span class="params">|builder|</span></div><div class="line">            builder.request  <span class="symbol">:url_encoded</span></div><div class="line">            builder.response <span class="symbol">:logger</span></div><div class="line">            builder.adapter  <span class="symbol">:net_http</span></div><div class="line">        <span class="keyword">end</span></div><div class="line">        res = conn.post <span class="string">"/JOB_NAME/buildWithParameters"</span>, &#123; <span class="symbol">issue_id:</span> issue.id&#125;</div><div class="line">    <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>以下を参考にしました</p>
<ul>
<li><a href="https://wiki.jenkins.io/display/JENKINS/Remote+access+API" target="_blank" rel="noopener">https://wiki.jenkins.io/display/JENKINS/Remote+access+API</a></li>
<li><a href="https://wiki.jenkins.io/display/JENKINS/Parameterized+Build" target="_blank" rel="noopener">https://wiki.jenkins.io/display/JENKINS/Parameterized+Build</a></li>
</ul>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>結果と所感について…</p>
<ul>
<li>目論見どおり大変だったチケットの再振り分けが減りました<ul>
<li>実は私もヘルプデスクに担当を持っていますが、チケット対応が以前より楽になったと実感しています</li>
</ul>
</li>
<li>意外だったのは、epoch数が10回程度でも思ったよりと正答率が高い（約８０％）ということ</li>
<li>Deep Learning の登場で機械学習の敷居は相当下がっていると感じます。みなさんも是非チャレンジしてみてください！</li>
</ul>
<p>あいちゃん は今後も大きく育てていきます！</p>
<ul>
<li>チケットの担当者振り分け<ul>
<li>カテゴリ毎にだいたい同じような担当者になるので、一緒に入れてしまえるのでは？</li>
</ul>
</li>
<li>あいちゃんと対話できるようにしたい<ul>
<li>チャット形式でチケット起票における質問にある程度答えてくれると助かるのでは？</li>
</ul>
</li>
</ul>
<hr>
<p>フューチャーアーキテクトでは、技術的視点だけでなく、ビジネス視点からも応用先を考え技術検証・現場への導入を行っています。<br>興味がある方、一緒に働きましょう！ぜひメッセージ下さい。</p>
<p><a href="http://www.future.co.jp/recruit/" target="_blank" rel="noopener">http://www.future.co.jp/recruit/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;社内ヘルプデスク（Redmine）における管理対応の業務を効率化し、サービスレベルを上げたい&lt;/li&gt;
&lt;li&gt;現状の課題&lt;ul
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="Redmine" scheme="https://future-architect.github.io/tags/Redmine/"/>
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>第12回NLP若手の会シンポジウム　参加報告</title>
    <link href="https://future-architect.github.io/articles/20170922/"/>
    <id>https://future-architect.github.io/articles/20170922/</id>
    <published>2017-09-22T07:20:00.000Z</published>
    <updated>2017-09-22T07:21:53.185Z</updated>
    
    <content type="html"><![CDATA[<p>はじめまして、7月からフューチャーアーキテクトで自然言語処理・人工知能分野の研究開発を担当している、Strategic R&amp;Dチームの貞光です。</p>
<p>2017/9/3(日) ～ 2017/9/5(火) に開催された第12回NLP若手の会シンポジウム (YANS)にスポンサーとして参加してきました。</p>
<p>今年は沖縄那覇市での開催。<br>亜熱帯の包み込むような熱気にも負けず、若い研究者間での議論が大いに白熱していました。<br>本記事では白熱していた議論の様子に加え、個人的に気になった発表について紹介いたします。</p>
<p><a href="http://yans.anlp.jp/entry/yans2017program" target="_blank" rel="noopener">http://yans.anlp.jp/entry/yans2017program</a></p>
<p><img src="/images/20170915/photo_20170915_01.jpeg"></p>
<h1 id="YANSならではの表彰"><a href="#YANSならではの表彰" class="headerlink" title="YANSならではの表彰"></a>YANSならではの表彰</h1><p>YANSでは若手研究者による一般ポスター発表と講師を招いた招待講演と、それに加えてスポンサー企業による口頭発表があります。</p>
<p>また、会議の最後に、参加者全員の投票によって奨励賞５件とデモ賞１件が表彰されます。<br>論文としての完成度を評価するのではなく、今後の発展性に期待する賞という位置づけとなっているのがYANSの特徴です。</p>
<p>表彰されたのは以下の発表です。おめでとうございます！</p>
<h3 id="奨励賞"><a href="#奨励賞" class="headerlink" title="奨励賞"></a>奨励賞</h3><ul>
<li><strong>活用情報を用いた日英ニューラル機械翻訳</strong><ul>
<li>黒澤道希, 山岸駿秀, 松村雪桜, 小町守（首都大）</li>
<li>日本語の活用情報によるスパースネスを解消するためのNMTの提案。活用情報をモデル内に取り入れることで、BLEUが向上したとのことです</li>
</ul>
</li>
<li><strong>オンライン環境下でのニューストピック検出への強化学習の応用</strong><ul>
<li>大倉俊平（ヤフー株式会社）</li>
<li>強化学習は、対話タスクにおいてMDP,POMDP等の手法が積極的に使われており、最近(2017/9/7)でも、Montreal大のBengio先生のグループが発表した、 Amazon Alexa Prize competition 向けのチャットボットに関する論文が注目を集めています[Serban+’17<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>]。<br>本研究はユーザへのニュースレコメンドに用いるためのトピック検出のために強化学習を用いた研究で、対話以外のNLPタスクで強化学習を用いたチャレンジ性が評価されたように思います</li>
</ul>
</li>
<li><strong>カーネル密度推定に基づく関係予測</strong><ul>
<li>横井祥, 乾健太郎（東北大）</li>
<li>embeddingを用いた新しい類似度の提案。PMIの一般化とみなすことができる、という主張も含まれていました。横井さんは、YANS初の試みであるYoutubeの生中継レポーターとしても大活躍でした</li>
</ul>
</li>
<li><strong>周辺文脈の集合による単語表現の獲得と関係抽出への応用</strong><ul>
<li>濱口拓男（NAIST）, 大岩秀和（RIT）, 新保仁, 松本裕治（NAIST）</li>
</ul>
</li>
<li><strong>発話スタイル空間の教師なし学習およびスタイル制御可能な対話システムの実現</strong><ul>
<li>赤間怜奈, 渡邉研斗, 横井祥, 乾健太郎（東北大）</li>
</ul>
</li>
</ul>
<p>※最後の2件は、弊社スポンサー展示時間帯と重なっており、残念ながら聴講できませんでした。</p>
<h3 id="デモ賞"><a href="#デモ賞" class="headerlink" title="デモ賞"></a>デモ賞</h3><ul>
<li><strong>deep-crf</strong><ul>
<li>佐藤元紀, 能地宏, 松本裕治（NAIST）</li>
<li>Bi-directional-LSTM等のDNNによる基本的な系列ラベリング手法を包含したツールで、attention情報等の分析用付加情報も出してくれるUIを含め、とても良くできていました。</li>
</ul>
</li>
</ul>
<p>上記発表では常に多くの聴衆を集めており、今後の研究の進め方についてのサジェスションを含め、YANSならではの活発な議論がかわされていたように思います。</p>
<h1 id="個人的に興味深かった発表"><a href="#個人的に興味深かった発表" class="headerlink" title="個人的に興味深かった発表"></a>個人的に興味深かった発表</h1><p>それ以外にも興味深い発表が多くありましたので、私の気になった発表について少しだけ紹介させていただきます。</p>
<ul>
<li><strong>教師なし学習による分野特有の固有表現認識</strong><ul>
<li>友利涼, 森信介（京大）</li>
<li>教師なし・半教師ありの固有表現（Named Entity:NE)認識に対し、教師なし単語分割や形態素解析(Mochihashi+’09<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>, Uchiumi+’15<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>)等で用いられる階層Pitman-Yor過程を応用した研究です。<br>IREXのような一般的なNEの体系ではなく、特定分野のNEの抽出、例えば「レシピ」や「将棋」といった特定ドメインに焦点をあてています。<br>提案法の教師なし学習では、特定分野のコーパスに加え、一般分野のコーパスを疑似教師データとして与えています。この時、一般分野コーパス中の単語は全て１単語で分割され、かつ”O”ラベル(非NE)を付与するようにします。そうして作られた学習データを元に学習すると、特定分野において高頻度で出現する単語列を、潜在クラス=”NE”として抽出できるようになるという主張で、ベースラインに比べ大幅に精度が改善されたとのことです。<br>事前にNEクラス数を与える必要があるという点や、個々のNEクラスで得意不得意があるようなので、その改善が課題とのことですが、応用範囲の広い技術だと思います。</li>
</ul>
</li>
<li><strong>エンコーダ・デコーダモデルを用いた画像の日本語キャプション生成のエラー分析</strong><ul>
<li>白井稔久, 尾形朋哉, 小町守（首都大）</li>
<li>NLPにおける近年の重要なマイルストンとして、2015年、幅広いNLPタスクに対して誤り分析を行ったプロジェクト<a href="https://sites.google.com/site/projectnextnlp/" target="_blank" rel="noopener">Project Next NLP</a>があります。<br>プロジェクトの結果、価値のある誤り分析の知見がコミュニティにもたらされ、その後もエラー分析に関する試みは続いています。<br>YANSでもいくつか、誤り分析の発表がありました。本発表は、エンコーダ・デコーダモデルを用いた画像からのキャプション生成のエラー分析に関するもので、誤りのタイプを定義した上でそれぞれの誤り割合を算出しています。<br>例として、長い生成文になればなるほど、前に一度出現した単語が再度出現しやすくなる、というエラーが確認されたそうです。</li>
</ul>
</li>
<li><strong>数量表現と比較に着目した意味解析に向けて</strong><ul>
<li>佐々木翔大, 田然, 乾健太郎（東北大）</li>
<li>スーパーのチラシで、「本日大根100円セール！」のところを「本日大根1000円セール！」と誤記すると、誰もお客さんは寄ってきませんよね。これは、数量の常識を理解する問題、と言えます。<br>この問題の一つに、個々のモノによって数量の常識的値のレンジが変化し、かつ多種多様なモノが世の中にスパースに存在する、という点が挙げられます。<br>本発表では、モノそれ自体を条件として常識的値を付与するのではなく、モノをembeddingしたものを条件として値を推定する、というアプローチを取っています。<br>残念ながら実験結果は良くなかったと報告されていましたが、その要因として、条件側となる文字列に対するembeddingを行う際、wikipediaから学習したword2vecを素直に用いた点が挙げられるかもしれません。<br>例えば、周辺文脈の単語を全て用いるword2vecの構成方法ではなく、特に数値に対して重みをかけつつ embedding 空間を構成できれば、この課題を解決できるかもしれず、その点について著者の佐々木さんと少し議論させていただきました。</li>
</ul>
</li>
</ul>
<h1 id="さいごに"><a href="#さいごに" class="headerlink" title="さいごに"></a>さいごに</h1><p>最後のクロージングセッションで報告されたアンケートは、運営委員渾身の力作で、様々な観点から参加者の意見をうかがい知ることができました。<br>特に興味深かったのは、学生の就職先の希望に関するアンケートです。<br>なんと、参加者の約半数がNLP関連を希望する一方、残る半数はNLP以外を希望しているとのこと。</p>
<p>私自身、産業発展の観点で、NLP以外の関連技術を自由に組み合わせたR&amp;Dにチャレンジしていきたいと思っています。</p>
<p>幸い、フューチャーアーキテクトには顧客毎の特徴的な、テキスト・画像・センサなど多数のデータが蓄積されており、チャレンジングな課題を持った人にとって、面白い環境だと思います。<br>もし、フューチャーアーキテクトについてご興味を持った方がいらっしゃいましたら、ぜひお気軽にご連絡ください！</p>
<p><img src="/images/20170915/photo_20170915_02.jpeg" width="50%"><br>弊社スポンサー発表の様子</p>
<p><img src="/images/20170915/photo_20170915_03.jpg" width="50%"><br>アメニティグッズ（清涼タブレット）</p>
<p><img src="/images/20170915/photo_20170915_04.jpg" width="50%"><br>海岸でのバーベキューの様子</p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">[Serban+’17] Iulian V. Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath Chandar, Nan Rosemary Ke, Sai Mudumba, Alexandre de Brebisson, Jose M. R. Sotelo, Dendi Suhubdy, Vincent Michalski, Alexandre Nguyen, Joelle Pineau and Yoshua Bengio, &quot;A Deep Reinforcement Learning Chatbot&quot;, arXiv:1709.02349, <a href="https://arxiv.org/abs/1709.02349" target="_blank" rel="noopener">https://arxiv.org/abs/1709.02349</a> (2017)</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">[Mochihashi+’09]Daichi Mochihashi, Takeshi Yamada and Naonori Ueda, &quot;Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling&quot;,  Proceeding of the the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 100-108 (2009)</span><a href="#fnref:2" rev="footnote"> ↩</a></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">3.</span><span style="display: inline-block; vertical-align: top;">[Uchiumi+’15]Kei Uchiumi, Hiroshi Tsukahara and Daichi Mochihashi, 2015, &quot;Inducing Word and Part-of-Speech with Pitman-Yor Hidden Semi-Markov Models&quot;, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1774–1782 (2015)</span><a href="#fnref:3" rev="footnote"> ↩</a></li></ol></div></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;はじめまして、7月からフューチャーアーキテクトで自然言語処理・人工知能分野の研究開発を担当している、Strategic R&amp;amp;Dチームの貞光です。&lt;/p&gt;
&lt;p&gt;2017/9/3(日) ～ 2017/9/5(火) に開催された第12回NLP若手の会シンポジウム (YA
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
      <category term="NLP" scheme="https://future-architect.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>uroboroSQL x Spring BootによるWebアプリケーション開発</title>
    <link href="https://future-architect.github.io/articles/20170828/"/>
    <id>https://future-architect.github.io/articles/20170828/</id>
    <published>2017-08-28T03:00:00.000Z</published>
    <updated>2017-09-13T07:18:14.720Z</updated>
    
    <content type="html"><![CDATA[<h1 id="uroboroSQLについて"><a href="#uroboroSQLについて" class="headerlink" title="uroboroSQLについて"></a>uroboroSQLについて</h1><p>こんにちは。星です。</p>
<p>今回は本技術ブログにもロゴが掲載されている弊社OSSプロダクトの一つ、「<a href="https://future-architect.github.io/uroborosql-doc/">uroboroSQL</a>」を利用してWebアプリケーションをどうやって開発すればよいのか？という話をさせて頂きたいと思います。</p>
<p>今年3月に実施されたオープンソースカンファレンス2017 Tokyo/Springを皮切りに、5月の名古屋、そして、8月の京都の開催においてもブース出展・セミナーでご紹介してきましたが、来場者の方々に興味をもって頂き、さまざまなフィードバックを頂き、OSSにした実感を得ています。</p>
<p>JavaでRDBにアクセスするアプリケーションを開発する場合、Java標準のJPA(Java Persistence API)や、Hiberate、MyBatis、EclipseLink、DBFlute、Domaなど、多くの選択肢が存在しています。</p>
<p>uroboroSQLもこういったJavaにおけるDB永続化ライブラリの一つであり、ORマッピングの機能も持っていますが、基本的にはJavaからSQLを生成することよりも、SQLに足りないところをJavaで補うアプローチを採用しているのが特徴です。</p>
<blockquote>
<p>uroboroSQLに興味を持たれた方は、公式サイトおよび私がオープンソースカンファレンス2017 Nagoyaにて、講演した資料をご覧ください。</p>
<p>uroboroSQL公式サイト<br><a href="https://future-architect.github.io/uroborosql-doc/">https://future-architect.github.io/uroborosql-doc/</a></p>
<p>uroboroSQLの紹介 (OSC2017 Nagoya) #oscnagoya<br><a href="https://www.slideshare.net/KenichiHoshi1/uroborosql-osc2017-nagoya-oscnagoya" target="_blank" rel="noopener">https://www.slideshare.net/KenichiHoshi1/uroborosql-osc2017-nagoya-oscnagoya</a></p>
</blockquote>
<h1 id="uroboroSQLを利用したSpring-BootによるWebアプリケーション"><a href="#uroboroSQLを利用したSpring-BootによるWebアプリケーション" class="headerlink" title="uroboroSQLを利用したSpring BootによるWebアプリケーション"></a>uroboroSQLを利用したSpring BootによるWebアプリケーション</h1><p>さて、JavaでWebアプリケーションを開発するとき、いわゆるWebアプリケーションフレームワークをどうするかという話がありますが、有償のWebアプリケーションサーバを利用する前提であれば、JavaEEは有力な選択肢でしょうし、そうでなければ、Spring Framework(Spring Boot)、Play Frameworkなどが有力かなと思います。特に最近はPaaSなどでコンテナ上で動かす場合は、TomcatやJettyなどを組み込んで、実行可能jarにしてデプロイするという方式もトレンドでしょうか。</p>
<p>uroboroSQLは、特定のWebアプリケーションフレームワークには依存しませんが、現在コマンドラインのサンプルぐらいしか用意できていないので、実際にWebアプリケーションを開発する場合のイメージがわかないという声も聞こえてくるようになりました。</p>
<p>そこで、現在最も人気のあるJavaのWebアプリケーションフレームワークの一つである「Spring Boot」を採用して、uroboroSQLのリファレンスとなるWebアプリケーションを開発してみました。</p>
<p>そして、今回採用するSpring Bootが「Spring PetClinic」というサンプルアプリケーションを公開していることもあり、それを見習って、「uroboroSQL PetClinic」を作ってみました。</p>
<p>なお、最近フロントエンドはJSフレームワーク利用のケースが多いかと思いますので、Thymeleafによるサーバサイドレンダリングではなく、Vue.jsを用いて実装しています。本サンプルのメイン部分ではありませんので、本記事では詳細には触れませんが、興味がある方はソースをご覧ください。</p>
<ul>
<li>uroboroSQL PetClinic<ul>
<li><a href="https://github.com/shout-star/uroborosql-springboot-demo" target="_blank" rel="noopener">https://github.com/shout-star/uroborosql-springboot-demo</a></li>
</ul>
</li>
</ul>
<h1 id="uroboroSQL-PetClinic"><a href="#uroboroSQL-PetClinic" class="headerlink" title="uroboroSQL PetClinic"></a>uroboroSQL PetClinic</h1><p><img src="/images/20170828/photo_20170828_01.jpg" class="img-middle-size"></p>
<p>トップページはSpring PetClinicをご存じの方だったら、ピンと来るかなと思います。<br>基本的に仕様はほぼ踏襲しています。</p>
<p>簡単にこのアプリケーションの説明をすると、動物病院のペット・飼い主の管理システムですね。「Find Owner」から飼い主を検索して、その飼い主のペットの登録やペットの来院履歴の登録ができるといったものです。</p>
<p><img src="/images/20170828/photo_20170828_02.png" class="img-middle-size"></p>
<p><img src="/images/20170828/photo_20170828_03.png" class="img-middle-size"></p>
<p>ログイン不要で利用できたり、DBの排他制御がなかったりと、あくまでサンプルという位置づけですね。</p>
<h1 id="uroboroSQLとSpring-Boot連携"><a href="#uroboroSQLとSpring-Boot連携" class="headerlink" title="uroboroSQLとSpring Boot連携"></a>uroboroSQLとSpring Boot連携</h1><p>Spring Bootだと、通常はSpring Data JPAを利用することが多いかと思いますが、今回は uroboroSQLと連携させるので、application.ymlで定義した<code>DataSource</code>を、uroboroSQLのSqlConfigに渡してやる必要があります。</p>
<p>今回はSpring BootのControllerの親クラス(BaseController)にて、その実装をしてみました。</p>
<p>なお、RDBはH2 Database、コネクションプールは、Tomcat JDBC Connection Poolを利用しています。</p>
<p><strong>application.yml（抜粋）</strong></p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="attr">spring:</span></div><div class="line"><span class="attr">  datasource:</span></div><div class="line"><span class="attr">    url:</span> jdbc:h2:file:./target/db/petclinic;AUTOCOMMIT=<span class="literal">FALSE</span></div><div class="line"><span class="attr">    username:</span> sa</div><div class="line"><span class="attr">    password:</span></div><div class="line"><span class="attr">    driver-class-name:</span> org.h2.Driver</div><div class="line"><span class="attr">    name:</span> jdbc/petclinic</div></pre></td></tr></table></figure>
<p><strong>BaseController.java（抜粋）</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseController</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">final</span> DataSource dataSource;</div><div class="line"></div><div class="line">    <span class="meta">@Autowired</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BaseController</span><span class="params">(DataSource dataSource)</span> </span>&#123;</div><div class="line">        <span class="keyword">this</span>.dataSource = dataSource;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * create &lt;code&gt;SqlAgent&lt;/code&gt; instance.</div><div class="line">     *</div><div class="line">     * <span class="doctag">@return</span> &lt;code&gt;SqlAgent&lt;/code&gt;</div><div class="line">     */</div><div class="line">    <span class="function">SqlAgent <span class="title">createAgent</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            SqlConfig config = DefaultSqlConfig.getConfig(dataSource.getConnection());</div><div class="line"></div><div class="line">            config.getSqlFilterManager().addSqlFilter(<span class="keyword">new</span> DebugSqlFilter());</div><div class="line">            config.getSqlFilterManager().initialize();</div><div class="line"></div><div class="line">            <span class="keyword">return</span> config.createAgent();</div><div class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/* 以下略 */</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>本来は<code>DefaultSqlConfig#getConfig</code>に直接<code>DataSource</code>を渡したいところだったんですが、uroboroSQL v0.2ではそのインタフェースがなくて、<code>getConnection</code>することにしました。ちなみに、v0.3では<code>DataSource</code>を直接渡せるようにする予定です。</p>
<h1 id="検索処理の実装"><a href="#検索処理の実装" class="headerlink" title="検索処理の実装"></a>検索処理の実装</h1><p>飼い主の検索画面(Find Owner)で、飼い主(Owner)の名字(LastName)で検索ボタンを押下したときに呼び出される実装は下記のようになります。</p>
<p><strong>OwnerController.java（抜粋）</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@RestController</span></div><div class="line"><span class="meta">@CrossOrigin</span></div><div class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/api/owners"</span>)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnerController</span> <span class="keyword">extends</span> <span class="title">BaseController</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OwnerController</span><span class="params">(DataSource dataSource)</span> </span>&#123;</div><div class="line">        <span class="keyword">super</span>(dataSource);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@RequestMapping</span>(method = RequestMethod.GET)</div><div class="line">    <span class="keyword">public</span> List&lt;Map&lt;String, Object&gt;&gt; find(<span class="meta">@RequestParam</span>(required = <span class="keyword">false</span>) String lastName) <span class="keyword">throws</span> SQLException &#123;</div><div class="line">        <span class="keyword">try</span> (SqlAgent agent = createAgent()) &#123;</div><div class="line">            <span class="keyword">return</span> agent.query(<span class="string">"owners-find"</span>)</div><div class="line">                .param(<span class="string">"lastName"</span>, lastName)</div><div class="line">                .collect(CaseFormat.CamelCase);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">/* 以下略 */</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>owner-find.sql</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> <span class="comment">/* _SQL_ID_ */</span></div><div class="line">  OWNERS.ID</div><div class="line">, OWNERS.FIRST_NAME</div><div class="line">, OWNERS.LAST_NAME</div><div class="line">, OWNERS.ADDRESS</div><div class="line">, OWNERS.CITY</div><div class="line">, OWNERS.TELEPHONE</div><div class="line">, <span class="keyword">GROUP_CONCAT</span>(PETS.NAME)   <span class="keyword">AS</span>  PETS_NAME</div><div class="line"><span class="keyword">FROM</span></div><div class="line">  OWNERS</div><div class="line"><span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span></div><div class="line">  PETS</div><div class="line"><span class="keyword">ON</span></div><div class="line">  OWNERS.ID = PETS.OWNER_ID</div><div class="line"><span class="comment">/*BEGIN*/</span></div><div class="line"><span class="keyword">WHERE</span></div><div class="line"><span class="comment">/*IF SF.isNotEmpty(lastName) */</span></div><div class="line">  LAST_NAME   <span class="keyword">LIKE</span>  <span class="string">'%'</span> ||  <span class="comment">/*lastName*/</span><span class="string">''</span>  ||  <span class="string">'%'</span></div><div class="line"><span class="comment">/*END*/</span></div><div class="line"><span class="comment">/*END*/</span></div><div class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> OWNERS.ID</div><div class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> OWNERS.ID</div></pre></td></tr></table></figure>
<p>uroboroSQLはSQLを実装する方式のライブラリなので、ご覧の通りControllerの実装は非常に簡潔で、パラメータを渡して呼び出すのみの実装です。</p>
<p>実際にSQL出力する際は、uroboroSQLの2-way SQLの機能により、名字(lastName)が未入力ならWHERE句自体がなくなり、全件検索するSQLになります。</p>
<p>なお、実際に自分で実装してみて、<code>SQLException</code>の検査例外が邪魔に感じたので、uroboroSQL v0.3からは実行時例外にする予定です。<br>やはり自身で実装してみると気づきがあるものですね。</p>
<h1 id="登録処理の実装"><a href="#登録処理の実装" class="headerlink" title="登録処理の実装"></a>登録処理の実装</h1><p><img src="/images/20170828/photo_20170828_04.png" class="img-middle-size"></p>
<p>次に飼い主の登録画面の実装を見てみます。</p>
<p><strong>OwnerController.java（抜粋）</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@RestController</span></div><div class="line"><span class="meta">@CrossOrigin</span></div><div class="line"><span class="meta">@RequestMapping</span>(<span class="string">"/api/owners"</span>)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnerController</span> <span class="keyword">extends</span> <span class="title">BaseController</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@RequestMapping</span>(value = <span class="string">"/new"</span>, method = RequestMethod.POST)</div><div class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Object&gt; <span class="title">create</span><span class="params">(@Validated @RequestBody Owner owner)</span> <span class="keyword">throws</span> SQLException </span>&#123;</div><div class="line">        <span class="keyword">return</span> handleCreate(owner);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/* 以下略 */</span></div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>BaseController.java（抜粋）</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseController</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * get generate keys.</div><div class="line">     *</div><div class="line">     * <span class="doctag">@param</span> agent SqlAgent</div><div class="line">     * <span class="doctag">@return</span> keys as &#123;<span class="doctag">@literal</span> Map&lt;String, Object&gt;&#125;</div><div class="line">     * <span class="doctag">@throws</span> SQLException SQLException</div><div class="line">     */</div><div class="line">    <span class="function"><span class="keyword">private</span> Map&lt;String, Object&gt; <span class="title">generatedKeys</span><span class="params">(SqlAgent agent)</span> <span class="keyword">throws</span> SQLException </span>&#123;</div><div class="line">        <span class="keyword">return</span> agent.queryWith(<span class="string">"SELECT SCOPE_IDENTITY() AS ID"</span>)</div><div class="line">            .collect(CaseFormat.CamelCase)</div><div class="line">            .get(<span class="number">0</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function">Map&lt;String, Object&gt; <span class="title">handleCreate</span><span class="params">(BaseModel model)</span> <span class="keyword">throws</span> SQLException </span>&#123;</div><div class="line">        <span class="keyword">try</span> (SqlAgent agent = createAgent()) &#123;</div><div class="line">            <span class="keyword">return</span> agent.required(() -&gt; &#123;</div><div class="line">                agent.insert(model);</div><div class="line">                <span class="keyword">return</span> generatedKeys(agent);</div><div class="line">            &#125;);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>Owner.java（抜粋）</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Table</span>(name = <span class="string">"OWNERS"</span>)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Owner</span> <span class="keyword">extends</span> <span class="title">BaseModel</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="meta">@NotEmpty</span></div><div class="line">    <span class="meta">@Size</span>(max = <span class="number">30</span>)</div><div class="line">    <span class="keyword">private</span> String firstName;</div><div class="line"></div><div class="line">    <span class="meta">@NotEmpty</span></div><div class="line">    <span class="meta">@Size</span>(max = <span class="number">30</span>)</div><div class="line">    <span class="keyword">private</span> String lastName;</div><div class="line"></div><div class="line">    <span class="meta">@NotEmpty</span></div><div class="line">    <span class="meta">@Size</span>(max = <span class="number">255</span>)</div><div class="line">    <span class="keyword">private</span> String address;</div><div class="line"></div><div class="line">    <span class="meta">@NotEmpty</span></div><div class="line">    <span class="meta">@Size</span>(max = <span class="number">80</span>)</div><div class="line">    <span class="keyword">private</span> String city;</div><div class="line"></div><div class="line">    <span class="meta">@NotEmpty</span></div><div class="line">    <span class="meta">@Digits</span>(fraction = <span class="number">0</span>, integer = <span class="number">10</span>)</div><div class="line">    <span class="keyword">private</span> String telephone;</div><div class="line"></div><div class="line">    <span class="comment">/* 以下getter/setter */</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>実際に呼ばれるControllerのメソッドは親に委譲しているだけにシンプルですね。<br>Spring Bootによって、フロントエンドから渡されたJSONがOwnerというエンティティクラスに自動的にマッピングされて、かつ、BeanValidationが実行され、問題なければDB登録処理が呼び出されます。</p>
<p>uroboroSQLもv0.2より、JPAライクなORマッピング機能を追加したことにより、INSERT/UPDATEといった処理はSQL不要でシンプルに実装することができました。</p>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>というわけで、uroboroSQLとSpring BootのWebアプリケーションの実装を見てきましたが、いかがでしたしょうか？</p>
<p>私自身、Spring Framework自体はこれまでも使ってきたものの、Spring Bootは初体験でしたが、うまい具合にSpring Frameworkの面倒だったところを隠していることもあり、非常にスマートですね。</p>
<p>RestControllerにおける検索処理では、uroboroSQLの<code>SqlAgent#query</code>の結果をそのまま返すだけでも事足りるケースも多く、Spring Bootとの相性は良いと感じました。<br>uroboroSQL自体がシンプルな仕様なこともあり、初見でも簡単に実装できることがわかって頂けたのではないかと思います。</p>
<p>むしろ、Vue.jsでのフロントエンドの実装ボリュームのほうが圧倒的に多いですね。。。</p>
<p>uroboroSQLは現在v0.3に向けて、鋭意開発を進めており、まだまだ進化していきますので、是非使ってみてください！！</p>
<p>uroboroSQL PetClinicも認証機能など、エンタープライズ用途の参考になるような機能をまだまだ追加していきたいと思っていますので、こちらもよろしくお願いします。</p>
<h2 id="番外編：SQLログ表示機能"><a href="#番外編：SQLログ表示機能" class="headerlink" title="番外編：SQLログ表示機能"></a>番外編：SQLログ表示機能</h2><p><img src="/images/20170828/photo_20170828_05.png" class="img-middle-size"></p>
<p>本家Spring Clinicには存在しない機能ですが、サーバサイドで実行されたuroboroSQLの出力するログを画面上で表示する機能を追加してみました。画面左下の目のアイコンをクリックするとログウィンドウが表示されます。</p>
<p>もちろん、実業務ではこのような機能はセキュリティホールになってしまいますが、これでどんな操作によってどんなSQLが実行されたのか、画面から見ることができますので、uroboroSQLのデモにも使えるかなと思っています。</p>
<p>そういうわけで、2017/9/9(土)、2017/9/10(日)に開催されるオープンソースカンファレンス2017 Tokyo/Fallにて、ブースでお見せしたいと思いますので、興味のある方は是非、当日フューチャーアーキテクト  ブースまで足を運んでください！！</p>
<ul>
<li>オープンソースカンファレンス2017 Tokyo/Fall - オープンソースの文化祭！<ul>
<li><a href="https://www.ospn.jp/osc2017-fall/" target="_blank" rel="noopener">https://www.ospn.jp/osc2017-fall/</a></li>
</ul>
</li>
</ul>
<h1 id="関連サイト"><a href="#関連サイト" class="headerlink" title="関連サイト"></a>関連サイト</h1><ul>
<li>uroboroSQL PetClinic<ul>
<li><a href="https://github.com/shout-star/uroborosql-springboot-demo" target="_blank" rel="noopener">https://github.com/shout-star/uroborosql-springboot-demo</a></li>
</ul>
</li>
<li>uroboroSQL Github Repository<ul>
<li><a href="https://github.com/future-architect/uroborosql" target="_blank" rel="noopener">https://github.com/future-architect/uroborosql</a></li>
</ul>
</li>
<li>uroboroSQL Document<ul>
<li><a href="https://future-architect.github.io/uroborosql-doc/">https://future-architect.github.io/uroborosql-doc/</a></li>
</ul>
</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li>Spring Boot<ul>
<li><a href="https://projects.spring.io/spring-boot/" target="_blank" rel="noopener">https://projects.spring.io/spring-boot/</a></li>
</ul>
</li>
<li>Spring PetClinic<ul>
<li><a href="https://github.com/spring-projects/spring-petclinic" target="_blank" rel="noopener">https://github.com/spring-projects/spring-petclinic</a>)</li>
</ul>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;uroboroSQLについて&quot;&gt;&lt;a href=&quot;#uroboroSQLについて&quot; class=&quot;headerlink&quot; title=&quot;uroboroSQLについて&quot;&gt;&lt;/a&gt;uroboroSQLについて&lt;/h1&gt;&lt;p&gt;こんにちは。星です。&lt;/p&gt;
&lt;p&gt;今回は本
    
    </summary>
    
      <category term="Programming" scheme="https://future-architect.github.io/categories/Programming/"/>
    
    
      <category term="Java" scheme="https://future-architect.github.io/tags/Java/"/>
    
      <category term="SQL" scheme="https://future-architect.github.io/tags/SQL/"/>
    
      <category term="uroboroSQL" scheme="https://future-architect.github.io/tags/uroboroSQL/"/>
    
  </entry>
  
  <entry>
    <title>古典的ゲームAIを用いたAlphaGo解説</title>
    <link href="https://future-architect.github.io/articles/20170804/"/>
    <id>https://future-architect.github.io/articles/20170804/</id>
    <published>2017-08-04T01:00:00.000Z</published>
    <updated>2017-08-03T23:50:21.932Z</updated>
    
    <content type="html"><![CDATA[<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>こんにちは。データ分析チームの李(碩)です。<br>この記事はAlphaGo解説の紹介です。本文を直接読みたい方は下記<a href="https://www.slideshare.net/suckgeunlee/aialphago/suckgeunlee/aialphago" target="_blank" rel="noopener">AlphaGo解説</a>にてスライドをご覧になってください。</p>
<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/MkHgW3PHRXZxOL" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/suckgeunlee/aialphago" title="古典的ゲームAIを用いたAlphaGo解説" target="_blank">古典的ゲームAIを用いたAlphaGo解説</a> </strong> from <strong><a target="_blank" href="https://www.slideshare.net/suckgeunlee">suckgeun lee</a></strong> </div></p>
<h1 id="AlphaGo調査のきっかけ"><a href="#AlphaGo調査のきっかけ" class="headerlink" title="AlphaGo調査のきっかけ"></a>AlphaGo調査のきっかけ</h1><p>私がAlphaGoについて初めて聞いたのは2016年3月、囲碁の伝説的な棋士、イ・セドルとの対局の時でした。AlphaGoの勝利が確定した時に「人間はもはや機械に勝てない!」とか、「AIが支配する世界」など海外のメディアも非常に炎上してたことを今でも覚えています。囲碁のプレイ経験もないし、ゲームAIなんて興味もなかったのですが、さすがにこれだけ大騒ぎになると調べたくなるものですね(笑)。</p>
<h1 id="AlphaGoは背景知識がない人には難しい"><a href="#AlphaGoは背景知識がない人には難しい" class="headerlink" title="AlphaGoは背景知識がない人には難しい"></a>AlphaGoは背景知識がない人には難しい</h1><p>AlphaGoの中身はどうなってるのかを調べるためにいろいろとブログや記事などを読んでいたのですが、当時は難しいアルゴリズム説明がドンとくる不親切な情報しか見あたらなかったです。何を説明しているのか全く分からなかったので、結局<a href="https://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" target="_blank" rel="noopener">AlphaGoの論文</a>を直接読むことになりました。そこで分かったのが、これはゲームAIの背景知識がないと理解しにくいことでした。AlphaGoがモンテカルロ木検索とDLを使って強くなったとは理解しましたが、これって何がすごいのかが全く伝わらないのです。</p>
<h1 id="AlphaGo中身と、そのすごさを理解するには、古典的ゲームAIも触れる必要がある"><a href="#AlphaGo中身と、そのすごさを理解するには、古典的ゲームAIも触れる必要がある" class="headerlink" title="AlphaGo中身と、そのすごさを理解するには、古典的ゲームAIも触れる必要がある"></a>AlphaGo中身と、そのすごさを理解するには、古典的ゲームAIも触れる必要がある</h1><p>突然AlphaGoのアルゴリズムをドンと説明されても困ってしまいます。そもそもなぜAlphaGoが今の形になったのか、従来のアルゴリズムと何が違うのか、性能はどれくらい上がったのかが全く見えなかったです。<br>結局私はゲームAIの歴史を含め、他の木探索系のアルゴリズムを調査しその違いを比較することになりました。<br>その背景知識を得た後に、なぜAlphaGoが今の形になったのか、どこがすごいのかがやっと理解できたのです。</p>
<h1 id="本資料の狙い"><a href="#本資料の狙い" class="headerlink" title="本資料の狙い"></a>本資料の狙い</h1><p>この資料は私のように、ゲームAIに関する背景知識はないがAlphaGoの中身が知りたい人のための資料です。単純にアルゴリズムを説明するのではなく、古典的なゲームAIに比べAlphaGoはどう発展してきたのか、何がすごいのかを説明していきます。<br>この資料に書いている古典的ゲームAIは全てのゲームAIではないですが、AlphaGoの発展の方向性を理解するには十分だと思っています。本資料ではディープラーニングや強化学習の詳細については触れませんが、その詳細が知りたい人は<a href="https://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" target="_blank" rel="noopener">AlphaGoの論文</a>を直接読んだ方が絶対に分かりやすいと思いますので、論文をおすすめします。</p>
<h1 id="本資料の目次"><a href="#本資料の目次" class="headerlink" title="本資料の目次"></a>本資料の目次</h1><ol>
<li>AlphaGo簡略紹介</li>
<li>ゲームAIの基層知識紹介<ul>
<li>ゲーム木</li>
<li>完全ゲーム木</li>
<li>部分ゲーム木</li>
</ul>
</li>
<li>効果的にゲーム木を計算するためのポイント</li>
<li>ミニマックス法 (1949年)</li>
<li>アルファ・ベータ法 (1958年)</li>
<li>古典的アルゴリズムの成功と失敗<ul>
<li>チェスでの成功：Deep Blue、1997年</li>
<li>囲碁での失敗</li>
</ul>
</li>
<li>モンテカルロ木検索 (1993年)<ul>
<li>囲碁での進歩</li>
<li>AlphaGoのベースになるアルゴリズム</li>
</ul>
</li>
<li>Deep Learningの登場 (2012年)</li>
<li>AlphaGoの登場 (2016年)<ul>
<li>モンテカルロ木検索の改良</li>
<li>Policy Network</li>
<li>Value Network</li>
<li>Rollout Policy</li>
<li>強化学習</li>
<li>AlphaGoのアーキテクチャ</li>
<li>性能比較</li>
</ul>
</li>
</ol>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>本資料は2016年の論文をベースにしていますが、現在のAlphaGoは仕組みがかなり変わったらしいですね。2017年末新しい論文が出るとのことですが、非常に楽しみです。その論文が出る前に、この資料がたくさんの人の役に立てればと思っています。</p>
<p>ちなみに、技術職ではない私のかわいい妻に見せたら、何の問題もなく理解してくれたので、背景知識が無くても読めると思います。</p>
<p>では、また新しい論文がでるまで、みなさんお元気で！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h1&gt;&lt;p&gt;こんにちは。データ分析チームの李(碩)です。&lt;br&gt;この記事はAlphaGo解説の紹介です。本文を直接読みたい方は下記&lt;a 
    
    </summary>
    
      <category term="DataScience" scheme="https://future-architect.github.io/categories/DataScience/"/>
    
    
      <category term="MachineLearning" scheme="https://future-architect.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>インフラ入門vol.2(ネットワーク)</title>
    <link href="https://future-architect.github.io/articles/20170704/"/>
    <id>https://future-architect.github.io/articles/20170704/</id>
    <published>2017-07-04T06:04:04.000Z</published>
    <updated>2017-07-03T02:32:15.556Z</updated>
    
    <content type="html"><![CDATA[<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>みなさま、こんにちは。洞内です。</p>
<p>今回はインフラ入門のvol.2と題して、インフラエンジニアの若手がネットワーク設計の現場で向き合っていく事柄（ここでは手段ではなく、設計の検討で外してはいけないポイント）について、説明します。</p>
<ul>
<li>前記事は<a href="https://future-architect.github.io/articles/20170109/">こちら</a>です</li>
</ul>
<h1 id="ネットワークって？"><a href="#ネットワークって？" class="headerlink" title="ネットワークって？"></a>ネットワークって？</h1><p>ネットワークとは「網状に作られた何か」（by Wikipedia）ですが、ITインフラにおけるネットワークとは、<strong>コンピューターAからコンピューターBへとデータを送るための通信経路</strong> の事を指します。<br><img src="/images/20170704/photo_20170704_01.png"></p>
<p>身近なものでイメージをするのであれば、電話、メール、SNS等はネットワークが存在して、その上をデータがやり取りされている事が大前提となる機能です。<br>電話であれば「音声データ」、メールやSNSであれば「文字データや画像データ」等を、相手のスマートフォンに届けるために、通信経路となる「ネットワーク」が使われている訳ですね。<br><img src="/images/20170704/photo_20170704_02.png"></p>
<p>身近なものを例に挙げましたが、業務システム（業務アプリケーション）でも同じことが言えます。<br>例えば、お店で不足した商品を補充する発注業務等でもネットワークを通じて、商品を注文していますし、遠く離れた相手とTV会議をするにもネットワークが使われています。</p>
<p>では、様々な場面で利用されているネットワークですが、どのようなポイントを意識して設計するのかを考えていきたいと思います。</p>
<h1 id="設計の要件"><a href="#設計の要件" class="headerlink" title="設計の要件"></a>設計の要件</h1><p>ネットワークを設計する際のポイントとなる要件は以下です。</p>
<ol>
<li>だれがどのように使うのか</li>
<li>どの位のデータが流れるのか</li>
<li>機械の故障等による停止はどの程度まで許容できるのか</li>
</ol>
<p>上から順番に見てみましょう。</p>
<h2 id="1-だれがどのように使うのか"><a href="#1-だれがどのように使うのか" class="headerlink" title="1. だれがどのように使うのか"></a>1. だれがどのように使うのか</h2><p>例えば、以下のように状況によって用意すべきネットワークは異なります。</p>
<p><strong>a. 会社員の方が、必ず会社のオフィスの自席で使う</strong><br>→この場合、ネットワークは有線で用意すれば良いでしょう</p>
<p><strong>b. 会社員の方が、会社のオフィス内の色々な場所で使う</strong><br>→この場合、ネットワークは無線の方が良いですね？</p>
<p><strong>c. 会社員の方が、客先でお客様と商談をしながら使う</strong><br>→こうなってくると、モバイルWi-Fiを使う事も検討する必要がでてきます</p>
<p><img src="/images/20170704/photo_20170704_03.png"></p>
<p>利用者がどのようなシチュエーションで利用するのかは、ネットワークを設計する上での非常に重要なポイントですね。</p>
<p>他にも、考慮すべき点は色々考えられると思います。</p>
<p><strong>d. インターネット上にWebサイトを公開するので、不特定多数の人がアクセスする</strong><br>→この場合は、使い勝手の話だけではなく、不正アクセスの対策等も必要になってきます。<br>（もちろん、アプリケーションに不具合を作らない事も重要ですが、ネットワークの設計や設定としても不要な通信は通さないといった事を考える必要があります）</p>
<h2 id="2-どの位のデータが流れるのか"><a href="#2-どの位のデータが流れるのか" class="headerlink" title="2. どの位のデータが流れるのか"></a>2. どの位のデータが流れるのか</h2><p>例えば、家庭のインターネットサービスでも、10Mbps、100Mbps、1Gbps等の通信速度の違うサービスがありますね。<br>これは用途にあったサービスプランを選ぶと思います。</p>
<ul>
<li>メールや、ちょっと調べ物をしたいだけの人は10Mbps</li>
<li>動画をたくさん見たい人は100Mbps～1Gbps</li>
</ul>
<p>動画をたくさん見たい人が、10Mbpsの回線を契約した場合は通信速度が遅くて動画がカクカクしてしまうかもしれませんね。また、<strong>複数人で同じ10Mbpsの回線を使っている場合、他の人もなかなかメールが送れない</strong>といった状況になるかもしれません。</p>
<p><img src="/images/20170704/photo_20170704_04.png"></p>
<p>ネットワークの設計をする上では、<strong>どの位のデータが流れるのか</strong> や <strong>今後、どの位のデータが流れるのか</strong> を考慮する必要があります。</p>
<p>家庭用のインターネットサービスでは、契約を変更して、機械を取り換えれば大体の場合は増速ができますが、会社の中のネットワークで増速しようとすると、大規模な工事が必要になる場合がほとんどです。</p>
<p>そのため、<strong>目的や使用用途を明確にして、どの位のデータがネットワークを流れるのか</strong>を見極めて設計をする必要があります。</p>
<h2 id="3-機械の故障等による停止はどの程度まで許容できるのか"><a href="#3-機械の故障等による停止はどの程度まで許容できるのか" class="headerlink" title="3. 機械の故障等による停止はどの程度まで許容できるのか"></a>3. 機械の故障等による停止はどの程度まで許容できるのか</h2><p>ネットワークを構成している機械もモノなので、いつかは故障しますし、それは突然訪れる事がままあります。</p>
<p>そんな突然故障した時に、ネットワークはどの程度の時間、使えないままになるのでしょうか？<br>5分？1時間？1日？</p>
<p>企業のネットワークがまるまる1日止まってしまったら大損失ですね。<br>そんな時のために、予備の機械を用意したり、壊れたときにすぐに自動で切り替えができる機械を用意したりすることを考えます。</p>
<p>当然、予備の機械の用意や、自動切り替えの機能がついた機械は値段も高くなりますので、<strong>全体の中で通信が止まってはいけないポイントはどこなのか</strong> を整理して、重要な箇所から優先的に投資を行うことが必要となってきます。</p>
<p><img src="/images/20170704/photo_20170704_05.png"></p>
<h1 id="設計の構成"><a href="#設計の構成" class="headerlink" title="設計の構成"></a>設計の構成</h1><p>前述の要件に加え、企業のネットワーク構成の設計には、以下の3つのポイントがあります。</p>
<ol>
<li>ネットワークのコア</li>
<li>LANとWANの利用</li>
<li>クラウドの利用</li>
</ol>
<p>こちらも順番にみていきましょう。</p>
<h2 id="1-ネットワークのコア"><a href="#1-ネットワークのコア" class="headerlink" title="1. ネットワークのコア"></a>1. ネットワークのコア</h2><p>ネットワークを構築する際、ほとんどの場合がコアルーターやコアスイッチと呼ばれるような、ネットワークの中心となる機械を用意します。</p>
<p>コアには沢山のデータが集まるため末端の機械よりも高い処理能力が必要になります。</p>
<p>また、通信が集中するため、不要な通信をブロックするファイアウォール機能や、通信の流量をコントロール（トラフィックシェーピング）して、セキュリティ性を高めたり、皆が快適に使えたりするといった制御を担うのがコアになります。</p>
<p>そのため、コアとなる機械が故障すると、全ネットワークが停止する恐れがあるため、先ほど挙げた<strong>機械の故障等による停止はどの程度まで許容できるのか</strong>という考え方の中で、非常に重要度の高いポイントと言えますね。</p>
<p> </p>
<h2 id="2-LANとWANの利用"><a href="#2-LANとWANの利用" class="headerlink" title="2. LANとWANの利用"></a>2. LANとWANの利用</h2><p>そもそも、LANとWANとはなんぞや？という方もいらっしゃると思いますので、簡単に説明します。</p>
<blockquote>
<ul>
<li>LAN(Local Area Network) <ul>
<li>1つの建物や、1つの組織の中で運用されるネットワークを指す言葉</li>
</ul>
</li>
<li>WAN(Wide Area Network):<ul>
<li>LANに対して、もっと広範囲に広がるネットワークを指す言葉。一般的にはインターネットサービスプロバイダが持つネットワークをWANと呼ぶ。</li>
</ul>
</li>
</ul>
</blockquote>
<p>LANは、何となくイメージが出来るかもしれません。<br>でも、WANはどうでしょうか？ちょっとぼんやりしたイメージかもしれませんね。<br>もう少し具体的にしましょう。</p>
<p>私たちが家庭でインターネットを利用する際、ほとんどの人が、インターネットサービスプロバイダと契約をして、インターネットを利用していると思います。<br>この時、契約内容にもよりますが、多くの場合はルーターと呼ばれる機械を用意して、このルーターに複数のPCやTV、ゲーム機等を接続して、インターネットを利用していると思います。<br>実はこのルーターの下に繋いだPC、TV、ゲーム機は1つのネットワークに所属しており、このネットワークの事をLANと言います。</p>
<p>また、ルーターを挟んで反対側(インターネットサービスプロバイダ側)をWANと言います。<br>このWANと呼ばれる、インターネットサービスプロバイダ側のネットワークがたくさん集まり、繋がっている物がインターネットと呼ばれるものです。言い換えると、インターネットとは様々なWANの集合体ということですね。</p>
<p>さて、話を戻して、当社の顧客には企業規模の大きなお客様が多く、オフィスも1つではなく、たくさんの拠点を構えていらっしゃいます。そのため1つの建物内で構成されたLANでは、業務遂行にあたり不便な部分が出てきてしまいます。</p>
<p>例えば、会計処理をするにあたり東京、名古屋、大阪それぞれのオフィスで処理したものを集めて、また合算するよりも、1ヶ所で処理をした方が早いですよね？</p>
<p>他にも同じ文書ファイルを各オフィスに配布するときに、それぞれの拠点の担当者にメールで配布をするよりも、ファイルサーバ上に置いて、1ヶ所から皆が参照をする方が効率的ですね。そのため、各拠点のLANをWANで繋いで、1つのLANとして扱うような設計を行う事が多くあります。</p>
<p>このように複数の拠点をまたぐLANを設計する場合、先ほど挙げたようなポイントを検討する必要があります。</p>
<ul>
<li>通信内容をどのように保護(第三者に傍受されないように)するのか。</li>
<li>ネットワークのコアをどこに配置するのか</li>
<li>だれがどのように使うのか。</li>
<li>どの位のデータが流れるのか。</li>
<li>機械の故障等による停止はどの程度まで許容できるのか。</li>
</ul>
<p>他にも、</p>
<ul>
<li>ネットワーク上の住所となるIPアドレスをどのように割り当てるのか。</li>
<li>将来拠点が増えたら、どのように拡張するのか。</li>
<li>重要なデータのある場所へのアクセス制御の有無。</li>
<li>通信のレイテンシ(遅延)はどうか。</li>
</ul>
<p>などなど考えることは様々です。</p>
<h2 id="3-クラウドの利用"><a href="#3-クラウドの利用" class="headerlink" title="3. クラウドの利用"></a>3. クラウドの利用</h2><p>昨今、このクラウドという単語は、IT業界だけではなく一般的にも浸透してきたのではないでしょうか？<br>一言でクラウドといっても、多種多様なサービスがありますが、共通していることがいくつかあります。</p>
<ul>
<li>必要な時に必要な分だけ機能やサービスを利用する</li>
<li>ネットワーク経由での利用が前提</li>
</ul>
<p>クラウドサービスはネットワーク経由での利用が前提になっているのですね。<br>クラウドへの接続の方法は、多くはインターネット経由ですが、クラウドサービスの提供側が対応していれば、WAN経由での接続も可能です。</p>
<p>先ほど、WANを使って各拠点のLANを1つにまとめるといった話をしましたが、クラウドサービスも、そんなLANの1つとして扱う事ができる訳ですね。</p>
<p>ただし、当然ですが何でもかんでも繋げば良いという事ではなく、限られた予算の中でどのようにするのが一番良いのかを常に考えて、不要なのであれば切り捨てるといった判断をする事も必要です。</p>
<h1 id="プロジェクトを進める上でのネットワーク"><a href="#プロジェクトを進める上でのネットワーク" class="headerlink" title="プロジェクトを進める上でのネットワーク"></a>プロジェクトを進める上でのネットワーク</h1><p>実際にプロジェクトの中では、ほとんどの企業が既にネットワークを持っている場合が多いのですが、新しく作るシステムを作る上で、ネットワークを拡張するための設計や対応が必要な場合があります。</p>
<p>そんな時、インフラの構築の中でも特にネットワークはプロジェクト全体の進行に対して大きく先行して進めなければなりません。</p>
<p>ネットワークを作るためには、機械の発注や購入だけではなく、建物工事や、通信が正しく行えるか疎通試験等も行う必要があるからです。</p>
<p>その他にも、お客様自身が自社のネットワークを網羅しきれておらず、現行のネットワーク調査を行う必要が有る場合等もあります。</p>
<p>こういった作業に対するスケジュールや、見積り、設計を誤ってしまうと、プロジェクトの進行が大きく遅れるだけではなく、お客様に想定外の出費をさせてしまう可能性があります。</p>
<p>実際に、当社の中でも建物工事を行う業者の繁忙期閑散期を見誤り、実際に工事が行えるタイミングが当初想定より遅れてしまいギリギリでの対応となったことや、いざ使い始めてみると、当初の想定見積よりも多くの通信が流れており、通信の圧縮処理や、通信処理が集中する時間を分散させて影響を最小限に留めるように業務処理の見直しを行った事などもあります。</p>
<h1 id="おわりに"><a href="#おわりに" class="headerlink" title="おわりに"></a>おわりに</h1><p>いかがでしたでしょうか？<br>普段、多くの方々はあまりネットワークについて意識をしていないかもしれません。</p>
<p>その理由は<a href="https://future-architect.github.io/articles/20170109/">インフラ入門vol1</a>にもありましたが、ITインフラが<strong>動いて当たり前</strong>だからです。ただ、実際に壊れない機器は無いですし、<strong>メールだけ出来れば良い</strong> としていた環境でだれかが勝手に<strong>動画も配信！</strong> となれば、その元々の要件下での <strong>動いて当たり前</strong> は崩れてしまいます。</p>
<p>この <strong>動いて当たり前</strong> を崩れないようにしっかりと作りつつ、今後の拡張性や、セキュリティ、予算等のバランスを取って作り上げる事がネットワークの設計に求められることです。</p>
<p>では、また次回をお楽しみに。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;はじめに&quot;&gt;&lt;a href=&quot;#はじめに&quot; class=&quot;headerlink&quot; title=&quot;はじめに&quot;&gt;&lt;/a&gt;はじめに&lt;/h1&gt;&lt;p&gt;みなさま、こんにちは。洞内です。&lt;/p&gt;
&lt;p&gt;今回はインフラ入門のvol.2と題して、インフラエンジニアの若手がネットワー
    
    </summary>
    
      <category term="Infrastructure" scheme="https://future-architect.github.io/categories/Infrastructure/"/>
    
    
      <category term="Infrastructure" scheme="https://future-architect.github.io/tags/Infrastructure/"/>
    
      <category term="Network" scheme="https://future-architect.github.io/tags/Network/"/>
    
  </entry>
  
</feed>
