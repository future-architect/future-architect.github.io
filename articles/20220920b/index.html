<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <!--
    ███████╗██╗░░░██╗████████╗██╗░░░██╗██████╗░███████╗
    ██╔════╝██║░░░██║╚══██╔══╝██║░░░██║██╔══██╗██╔════╝
    █████╗░░██║░░░██║░░░██║░░░██║░░░██║██████╔╝█████╗░░
    ██╔══╝░░██║░░░██║░░░██║░░░██║░░░██║██╔══██╗██╔══╝░░
    ██║░░░░░╚██████╔╝░░░██║░░░╚██████╔╝██║░░██║███████╗
    ╚═╝░░░░░░╚═════╝░░░░╚═╝░░░░╚═════╝░╚═╝░░╚═╝╚══════╝
    ████████╗███████╗░█████╗░██╗░░██╗
    ╚══██╔══╝██╔════╝██╔══██╗██║░░██║
    ░░░██║░░░█████╗░░██║░░╚═╝███████║
    ░░░██║░░░██╔══╝░░██║░░██╗██╔══██║
    ░░░██║░░░███████╗╚█████╔╝██║░░██║
    ░░░╚═╝░░░╚══════╝░╚════╝░╚═╝░░╚═╝
    ██████╗░██╗░░░░░░█████╗░░██████╗░
    ██╔══██╗██║░░░░░██╔══██╗██╔════╝░
    ██████╦╝██║░░░░░██║░░██║██║░░██╗░
    ██╔══██╗██║░░░░░██║░░██║██║░░╚██╗
    ██████╦╝███████╗╚█████╔╝╚██████╔╝
    ╚═════╝░╚══════╝░╚════╝░░╚═════╝░
    Welcome engineer.
    https://www.future.co.jp/recruit/
  -->
  
  <title>Dataflow後編（Dataflowの事前準備からPub/Sub・BigQueryとの連携例まで） | フューチャー技術ブログ</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  
  <meta name="description" content="はじめにはじめまして、フューチャーのインターン”Engineer Camp”に参加した平野と申します。今回のインターンでは、Google Cloud Platform (GCP)のサービスとして提供されているDataflowについて調査し、その仕組みや使い方についてこの技術ブログにまとめることに取り組みました。 フューチャーのインターンについてはこちらをご覧ください！ 今回の記事は前編・後編に">
<meta property="og:type" content="article">
<meta property="og:title" content="Dataflow後編（Dataflowの事前準備からPub&#x2F;Sub・BigQueryとの連携例まで） | フューチャー技術ブログ">
<meta property="og:url" content="https://future-architect.github.io/articles/20220920b/index.html">
<meta property="og:site_name" content="フューチャー技術ブログ">
<meta property="og:description" content="はじめにはじめまして、フューチャーのインターン”Engineer Camp”に参加した平野と申します。今回のインターンでは、Google Cloud Platform (GCP)のサービスとして提供されているDataflowについて調査し、その仕組みや使い方についてこの技術ブログにまとめることに取り組みました。 フューチャーのインターンについてはこちらをご覧ください！ 今回の記事は前編・後編に">
<meta property="og:locale" content="ja_JP">
<meta property="og:image" content="https://future-architect.github.io/images/20220920b/dataflow_top2.png">
<meta property="og:image" content="https://future-architect.github.io/images/20220920b/IAM_setting.png">
<meta property="og:image" content="https://future-architect.github.io/images/20220920b/make_bucket_new.gif">
<meta property="og:image" content="https://future-architect.github.io/images/20220920b/make_topic.png">
<meta property="og:image" content="https://future-architect.github.io/images/20220920b/make_dataset.gif">
<meta property="og:image" content="https://future-architect.github.io/images/20220920b/make_query.png">
<meta property="og:image" content="https://future-architect.github.io/images/20220920b/pubsub2bq_result.png">
<meta property="article:published_time" content="2022-09-19T15:00:01.000Z">
<meta property="article:modified_time" content="2022-09-20T03:52:33.400Z">
<meta property="article:tag" content="インターン">
<meta property="article:tag" content="GCP">
<meta property="article:tag" content="BigQuery">
<meta property="article:tag" content="インターン2022">
<meta property="article:tag" content="Dataflow">
<meta property="article:tag" content="ApacheBeam">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://future-architect.github.io/images/20220920b/dataflow_top2.png">
  
  <link rel="alternate" href="/atom.xml" title="フューチャー技術ブログ" type="application/atom+xml">
  
  <link rel="icon" href="/favicon.ico">
  <link rel="apple-touch-icon" sizes='180x180' href="/apple-touch-icon.png">
  <link rel="apple-touch-icon" sizes='57x57' href="/apple-touch-icon-57x57.png">
  <link rel="canonical" href="https://future-architect.github.io/articles/20220920b/">
  <meta content="インターン,GCP,BigQuery,インターン2022,Dataflow,ApacheBeam" name="keywords">
  <meta content="平野甫" name="author">
  <link rel="preload" as="image" href="/banner.jpg" />
  <link rel='manifest' href='/manifest.webmanifest'/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css" integrity="sha384-F3w7mX95PdgyTmZZMECAngseQB83DfGTowi0iMjiWaeVhAn4FJkqJByhZMI3AhiU" crossorigin="anonymous">
  <link rel="stylesheet" href="/metronic/assets/style.css">
  <link rel="stylesheet" href="/css/theme-styles.css">
<meta name="generator" content="Hexo 5.4.2"></head>

<body class="corporate">
  <div class="wrap" itemscope itemtype="https://schema.org/TechArticle">
  <!-- BEGIN HEADER -->
<header class="header">
	<div class="header-overlay">
		<div class="header-menu"></div>
		<div class="header-title"><a href="/">Future Tech Blog</a></div>
		<div class="header-title-sub">フューチャー技術ブログ</div>
	</div>
</header>
<!-- Header END -->

  <div class="container">
  <ul class="breadcrumb">
    <li><a href="/">Home</a></li>
    <li><a href="/articles/">Blog</a></li>
    <li class="active">Post</li>
  </ul>
  <section id="main" class="margin-top-30">
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Programming/">Programmingカテゴリ</a>
  </div>


    <h2 itemprop="name" class="article-title">Dataflow後編（Dataflowの事前準備からPub/Sub・BigQueryとの連携例まで）
  
  <a target="_blank" rel="noopener" href="https://github.com/future-architect/tech-blog/edit/master/source/_posts/20220920b_Dataflow後編（Dataflowの事前準備からPub／Sub・BigQueryとの連携例まで）.md" title="Suggest Edits" class="github-edit"><i class="github-edit-icon"></i></a>
  
</h2>

    <div class="row">
  <main class="col-md-9 blog-posts">
    <article id="post-20220920b_Dataflow後編（Dataflowの事前準備からPub／Sub・BigQueryとの連携例まで）" class="article article-type-post blog-item" itemscope itemprop="blogPost">
      <div class="article-inner">
        
        <header class="article-header">
          <ul class="blog-info">
            <li class="blog-info-item"><a href="/articles/2022/" class="publish-date"><time datetime="2022-09-19T15:00:01.000Z" itemprop="datePublished">2022.09.20</time></a>
</li>
            <li class="blog-info-item"><li><a href="/authors/%E5%B9%B3%E9%87%8E%E7%94%AB" title="平野甫さんの記事一覧へ" class="post-author">平野甫</a></li></li>
            <li class="blog-info-item">
  
    
    <a href="/tags/インターン/" title="インターンタグの記事へ" class="tag-list-link">インターン</a>
  
    
    <a href="/tags/GCP/" title="GCPタグの記事へ" class="tag-list-link">GCP</a>
  
    
    <a href="/tags/BigQuery/" title="BigQueryタグの記事へ" class="tag-list-link">BigQuery</a>
  
    
    <a href="/tags/インターン2022/" title="インターン2022タグの記事へ" class="tag-list-link">インターン2022</a>
  
    
    <a href="/tags/Dataflow/" title="Dataflowタグの記事へ" class="tag-list-link">Dataflow</a>
  
    
    <a href="/tags/ApacheBeam/" title="ApacheBeamタグの記事へ" class="tag-list-link">ApacheBeam</a>
  

</li>
          </ul>
          </header>
        
        <div class="article-entry" itemprop="articleBody">
          
            <img src="/images/20220920b/dataflow_top2.png" alt="" width="1000" height="663">

<h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>はじめまして、フューチャーのインターン”Engineer Camp”に参加した平野と申します。<br>今回のインターンでは、Google Cloud Platform (GCP)のサービスとして提供されているDataflowについて調査し、その仕組みや使い方についてこの技術ブログにまとめることに取り組みました。</p>
<p>フューチャーのインターンについては<a href="/tags/%E3%82%A4%E3%83%B3%E3%82%BF%E3%83%BC%E3%83%B3/">こちら</a>をご覧ください！</p>
<p>今回の記事は前編・後編に分かれており</p>
<ul>
<li>前編:<ul>
<li>Dataflowの概要</li>
<li>Apache Beamの概要・内部的な仕組み</li>
<li>Apache Beamのコードの書き方</li>
</ul>
</li>
<li>後編：<ul>
<li>Dataflowを使う上での事前準備と基本的な使い方</li>
<li>GPUを使う上での事前準備と基本的な使い方</li>
<li>Pub&#x2F;Sub・BigQueryとの連携例</li>
</ul>
</li>
</ul>
<p>という構成になっています。前編は<a href="/articles/20220920a/">こちら</a>。</p>
<h1 id="Datflowの事前準備と基本的な使い方"><a href="#Datflowの事前準備と基本的な使い方" class="headerlink" title="Datflowの事前準備と基本的な使い方"></a>Datflowの事前準備と基本的な使い方</h1><p>Dataflowを使うための事前準備からパイプライン実行までの一連の流れについて説明します。以下の手順で進めます。</p>
<ol>
<li>APIの有効化</li>
<li>IAMの設定</li>
<li>Apache Beam SDKのインストール</li>
<li>Cloud Storageバケットの作成</li>
<li>Dataflow上でパイプラインを実行</li>
</ol>
<p>なお、以降の</p>
<ul>
<li><a href="#dataflow%E3%81%AE%E4%BD%BF%E7%94%A8%E4%BE%8Bgpu%E3%81%AA%E3%81%97ver">Dataflowの使用例（GPUなしver.）</a></li>
<li><a href="#dataflow%E3%81%A7gpu%E3%82%92%E4%BD%BF%E3%81%86%E9%9A%9B%E3%81%AE%E4%BA%8B%E5%89%8D%E6%BA%96%E5%82%99%E3%81%A8%E5%9F%BA%E6%9C%AC%E7%9A%84%E3%81%AA%E4%BD%BF%E3%81%84%E6%96%B9">DataflowでGPUを使う際の事前準備と基本的な使い方</a></li>
<li><a href="#dataflow%E3%81%AE%E4%BD%BF%E7%94%A8%E4%BE%8Bgpu%E3%81%82%E3%82%8Aver">Dataflowの使用例（GPUありver.）</a></li>
<li><a href="#%E4%BB%96%E3%81%AEgcp%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9%E3%81%A8%E3%81%AE%E9%80%A3%E6%90%BA%E3%81%A8%E3%82%B9%E3%83%88%E3%83%AA%E3%83%BC%E3%83%9F%E3%83%B3%E3%82%B0%E5%87%A6%E7%90%86">他のGCPサービスとの連携とストリーミング処理</a></li>
</ul>
<p>では、ここで説明する<a href="#api%E3%81%AE%E6%9C%89%E5%8A%B9%E5%8C%96">APIの有効化</a>、<a href="#iam%E3%81%AE%E8%A8%AD%E5%AE%9A">IAMの設定</a>、<a href="#cloud-storage%E3%83%90%E3%82%B1%E3%83%83%E3%83%88%E3%81%AE%E4%BD%9C%E6%88%90">Cloud Storageバケットの作成</a>ができている前提で話を進めています。</p>
<h2 id="APIの有効化"><a href="#APIの有効化" class="headerlink" title="APIの有効化"></a>APIの有効化</h2><p>Compute Engine API, Dataflow API, Cloud Storage APIとその他必要な（連携させたい）APIを有効化します。APIの有効化はコンソール画面上部にある検索窓から有効化したいAPIを検索すれば簡単に有効化できます。</p>
<h2 id="IAMの設定"><a href="#IAMの設定" class="headerlink" title="IAMの設定"></a>IAMの設定</h2><p>APIを有効化するとIAMに<strong>Compute Engine default service account</strong>という名前のアカウントが追加されているはずです。<br>Dataflowを利用するにはそのサービスアカウントに<strong>Dataflowワーカー</strong>、<strong>Dataflow管理者</strong>、<strong>Storageオブジェクト管理者</strong>のロールを追加して保存します。以下の画像のようになっていればOKです。<br><img src="/images/20220920b/IAM_setting.png" alt="IAM_setting.png" width="1200" height="164" loading="lazy"><br>なお、ロールを付与するには、<strong>resourcemanager.projects.setIamPolicy</strong>の権限を持っている必要があります。持っていない場合はプロジェクトの管理者に権限を付与してもらうか、サービスアカウントへのロールの付与を代わりにやってもらってください。</p>
<h2 id="Apache-Beam-SDKのインストール"><a href="#Apache-Beam-SDKのインストール" class="headerlink" title="Apache Beam SDKのインストール"></a>Apache Beam SDKのインストール</h2><p>続いて、ローカル環境（今回はCloud Shell）にApache Beam SDKをインストールします。2022&#x2F;08&#x2F;30現在、Apache Beam SDKでサポートされているPythonのバージョンは3.8までです。一方、Cloud ShellにデフォルトでインストールされているPythonのバージョンは3.9ですので、pyenv等を用いてPython3.8を実行する仮想環境を作成してください。その後、作成した仮想環境にApache Beamをインストールします。Dataflow(GCP)上で実行するには追加パッケージをインストールする必要があるので、以下のコマンドでインストールしてください。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install apache-beam[gcp]</span><br></pre></td></tr></table></figure>

<h2 id="Cloud-Storageバケットの作成"><a href="#Cloud-Storageバケットの作成" class="headerlink" title="Cloud Storageバケットの作成"></a>Cloud Storageバケットの作成</h2><p>Dataflowでパイプライン処理を行う場合、一時ファイルや出力ファイルを保存するためにCloud Storageのバケットを作成する必要があります。<br>バケットの作成はコンソール画面から作成する方法とpythonから作成する方法があります。<br>コンソール画面からは以下のように作成できます。<br><img src="/images/20220920b/make_bucket_new.gif" alt="make_bucket_new.gif" width="1200" height="665" loading="lazy"></p>
<p>pythonからバケットを作成する際は以下のコードを参考にしてください（<code>pip3 install google-cloud-storage</code>が必要です）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.cloud <span class="keyword">import</span> storage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_bucket</span>(<span class="params">project_name, bucket_name, region</span>):</span><br><span class="line">    client = storage.Client(project_name)</span><br><span class="line">    bucket = storage.Bucket(client)</span><br><span class="line">    bucket.name = bucket_name</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> bucket.exists():</span><br><span class="line">        client.create_bucket(bucket, location=region)</span><br></pre></td></tr></table></figure>

<h2 id="Dataflow上でパイプラインを実行"><a href="#Dataflow上でパイプラインを実行" class="headerlink" title="Dataflow上でパイプラインを実行"></a>Dataflow上でパイプラインを実行</h2><p>続いて、Dataflow上でパイプラインを実行していきます。Dataflow上でパイプラインを実行するにはいくつかのオプションを指定する必要があります（主にGCP関連）。ここでは、それらのオプションの説明とオプションの渡し方について説明します。<br>Dataflowでパイプラインを実行するためには以下のようなオプションを指定する必要があります。</p>
<div class="scroll"><table>
<thead>
<tr>
<th align="center">オプション名</th>
<th align="left">説明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">runner</td>
<td align="left">Dataflowで動かす場合には<code>DataflowRunner</code>を指定。ローカルで動かす場合には<code>DirectRunner</code>。</td>
</tr>
<tr>
<td align="center">project</td>
<td align="left">プロジェクトID。指定しないとエラーが返ってくる。</td>
</tr>
<tr>
<td align="center">job_name</td>
<td align="left">実行するジョブの名前。Dataflowのジョブのところにジョブの一覧が表示されるが、その際にどのジョブかを見分ける際に使える。指定しなければ勝手に名前をつけてくれるが、パッと見で判断しづらい。</td>
</tr>
<tr>
<td align="center">temp_location</td>
<td align="left">一時ファイルを保存するためのGCSのパス（<code>gs://</code>からスタートするパス）。指定しなければstaging_locationのパスが使用される。</td>
</tr>
<tr>
<td align="center">staging_location</td>
<td align="left">ローカルファイルをステージングするためのGCSのパス。指定しなければtemp_locationのパスが使用される。temp_locationかstaging_locationのどちらかは指定しなければならない。</td>
</tr>
<tr>
<td align="center">region</td>
<td align="left">Dataflowジョブをデプロイするリージョンエンドポイント。デフォルトでは<code>us-central1</code>。</td>
</tr>
</tbody></table></div>
<p>ここでは動かすのに必要な（とりあえずこのへんを渡しておけば動く）オプションを紹介していますので、その他のオプションについては<a target="_blank" rel="noopener" href="https://cloud.google.com/dataflow/docs/guides/setting-pipeline-options#setting-other-cloud-dataflow-pipeline-options">公式ドキュメント</a>を参照してください。</p>
<p>実行する際には以下のように<code>--&lt;オプション名&gt; 値</code>の形式で指定することでオプションを渡すことができます。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python &#123;ソースコードまでのpath&#125; \</span><br><span class="line">--runner <span class="string">&quot;DataflowRunner&quot;</span> \</span><br><span class="line">--project <span class="string">&quot;&#123;プロジェクトID&#125;&quot;</span> \</span><br><span class="line">--job_name <span class="string">&quot;&#123;ジョブの名前&#125;&quot;</span> \</span><br><span class="line">--temp_location <span class="string">&quot;gs://&#123;バケットの名前&#125;/temp&quot;</span> \</span><br><span class="line">--region <span class="string">&quot;asia-northeast1&quot;</span></span><br></pre></td></tr></table></figure>

<h1 id="Dataflowの使用例（GPUなしver-）"><a href="#Dataflowの使用例（GPUなしver-）" class="headerlink" title="Dataflowの使用例（GPUなしver.）"></a>Dataflowの使用例（GPUなしver.）</h1><p>ここでは、scikit-learnのモデルの推論をDataflow上で行う例を扱っていきます。今回はIrisデータセットで学習したモデルの重みパラメータ(<code>SVC_iris.pkl2</code>)が既に手元にあるという想定で、そのモデルの推論（学習時と同じIrisデータセットを使用）をDataflow上で行っていきます。以下のような手順で進めていきます。</p>
<ol>
<li>ソースコードの準備</li>
<li>Cloud ShellでPythonの環境構築</li>
<li>パイプラインの実行</li>
</ol>
<p>なお、<a href="#api%E3%81%AE%E6%9C%89%E5%8A%B9%E5%8C%96">APIの有効化</a>、<a href="#iam%E3%81%AE%E8%A8%AD%E5%AE%9A">IAMの設定</a>、<a href="#cloud-storage%E3%83%90%E3%82%B1%E3%83%83%E3%83%88%E3%81%AE%E4%BD%9C%E6%88%90">Cloud Storageバケットの作成</a>がお済みでない方はまずそちらから始めてください。</p>
<h2 id="ソースコードの準備"><a href="#ソースコードの準備" class="headerlink" title="ソースコードの準備"></a>ソースコードの準備</h2><p>今回実行したいソースコード(ファイル名:<code>runinference_sklearn.py</code>)です。モデルの重みパラメータまでのpathは<code>&#123;ソースコードがあるディレクトリ&#125;/models/sklearn_models/SVC_iris.pkl2</code>です。</p>
<figure class="highlight python"><figcaption><span>runinference_sklearn.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> apache_beam <span class="keyword">as</span> beam</span><br><span class="line"><span class="keyword">from</span> apache_beam.ml.inference <span class="keyword">import</span> RunInference</span><br><span class="line"><span class="keyword">from</span> apache_beam.ml.inference.sklearn_inference <span class="keyword">import</span> ModelFileType, SklearnModelHandlerNumpy</span><br><span class="line"><span class="keyword">from</span> apache_beam.options.pipeline_options <span class="keyword">import</span> PipelineOptions</span><br><span class="line"><span class="keyword">from</span> google.cloud <span class="keyword">import</span> storage</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">upload_model_to_gcs</span>(<span class="params">local_model_path, gcs_model_path, project_name, bucket_name</span>):</span><br><span class="line">    client = storage.Client(project_name)</span><br><span class="line">    bucket = storage.Bucket(client)</span><br><span class="line">    bucket.name = bucket_name</span><br><span class="line">    blob = bucket.blob(gcs_model_path)</span><br><span class="line">    blob.upload_from_filename(local_model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># パイプラインオプションの設定</span></span><br><span class="line">    pipeline_options = PipelineOptions()</span><br><span class="line">    options_dict = pipeline_options.display_data()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Irisデータの準備</span></span><br><span class="line">    data = load_iris()</span><br><span class="line">    numpy_data = data.data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># モデルのアップロード</span></span><br><span class="line">    upload_model_to_gcs(</span><br><span class="line">        local_model_path=<span class="string">&quot;./models/sklearn_models/SVC_iris.pkl2&quot;</span>,</span><br><span class="line">        gcs_model_path=<span class="string">&quot;models/sklearn_models/SVC_iris.pkl2&quot;</span>,</span><br><span class="line">        project_name=options_dict[<span class="string">&quot;project&quot;</span>],</span><br><span class="line">        bucket_name=options_dict[<span class="string">&quot;bucket_name&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ハンドラーの設定</span></span><br><span class="line">    model_uri = <span class="string">&quot;gs://&#123;&#125;/models/sklearn_models/SVC_iris.pkl2&quot;</span>.<span class="built_in">format</span>(options_dict[<span class="string">&quot;bucket_name&quot;</span>])</span><br><span class="line">    model_file_type = ModelFileType.JOBLIB</span><br><span class="line">    model_handler = SklearnModelHandlerNumpy(model_uri=model_uri, model_file_type=model_file_type)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># パイプライン実行</span></span><br><span class="line">    logging.getLogger().setLevel(logging.INFO)</span><br><span class="line">    <span class="keyword">with</span> beam.Pipeline(options=pipeline_options) <span class="keyword">as</span> p:</span><br><span class="line">        <span class="built_in">input</span> = p | <span class="string">&quot;read&quot;</span> &gt;&gt; beam.Create(numpy_data)</span><br><span class="line"></span><br><span class="line">        prediction = (</span><br><span class="line">            <span class="built_in">input</span></span><br><span class="line">            | RunInference(model_handler)</span><br><span class="line">            | beam.io.WriteToText(options_dict[<span class="string">&quot;output_executable_path&quot;</span>], shard_name_template=<span class="string">&quot;&quot;</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<h2 id="Cloud-ShellでPythonの環境構築"><a href="#Cloud-ShellでPythonの環境構築" class="headerlink" title="Cloud ShellでPythonの環境構築"></a>Cloud ShellでPythonの環境構築</h2><p>次にCloud ShellのPython環境を構築していきます。<br>まず、Python 3.8の環境を準備します。ターミナル上で</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyenv install 3.8.13</span><br></pre></td></tr></table></figure>

<p>を実行し、Python 3.8をインストールします。その後、</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pyenv virtualenv 3.8.13 dataflow</span><br><span class="line">pyenv activate dataflow</span><br></pre></td></tr></table></figure>

<p>を実行してPython 3.8.13がインストールされた仮想環境（ここでは<code>dataflow</code>）をアクティベートします。<br>続いて、必要なパッケージをインストールしていきます。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip3 install apache-beam[gcp] google-gcloud-storage</span><br><span class="line">pip3 install scikit-learn</span><br></pre></td></tr></table></figure>

<h2 id="パイプラインの実行"><a href="#パイプラインの実行" class="headerlink" title="パイプラインの実行"></a>パイプラインの実行</h2><p>必要なパッケージのインストールが終わったら、最後にパイプラインを実行していきます。以下のコマンドを実行するとDataflow上でパイプライン処理が動き始めます。<code>&#123;プロジェクトID&#125;</code>、<code>&#123;ジョブの名前&#125;</code>、<code>&#123;バケットの名前&#125;</code>は適宜変更してください。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python runinference_sklearn.py \</span><br><span class="line">--runner <span class="string">&quot;DataflowRunner&quot;</span> \</span><br><span class="line">--project <span class="string">&quot;&#123;プロジェクトID&#125;&quot;</span> \</span><br><span class="line">--job_name <span class="string">&quot;&#123;ジョブの名前&#125;&quot;</span> \</span><br><span class="line">--temp_location <span class="string">&quot;gs://&#123;バケットの名前&#125;/temp/&quot;</span> \</span><br><span class="line">--staging_location <span class="string">&quot;gs://&#123;バケットの名前&#125;/stage/&quot;</span> \</span><br><span class="line">--region <span class="string">&quot;asia-northeast1&quot;</span> \</span><br><span class="line">--bucket_name <span class="string">&quot;&#123;バケットの名前&#125;&quot;</span> \</span><br><span class="line">--output <span class="string">&quot;gs://&#123;バケットの名前&#125;/output.txt&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h2><p>推論結果はCloud Storageのバケットの<code>output.txt</code>に出力されます。今回の例では以下のような結果が得られました。</p>
<figure class="highlight text"><figcaption><span>output.txt</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">PredictionResult(example=array([5.1, 3.4, 1.5, 0.2]), inference=0)</span><br><span class="line">PredictionResult(example=array([5. , 3.4, 1.6, 0.4]), inference=0)</span><br><span class="line">PredictionResult(example=array([7.6, 3. , 6.6, 2.1]), inference=2)</span><br><span class="line">PredictionResult(example=array([5.9, 3. , 4.2, 1.5]), inference=1)</span><br><span class="line">PredictionResult(example=array([5.7, 3.8, 1.7, 0.3]), inference=0)</span><br><span class="line">PredictionResult(example=array([5.7, 4.4, 1.5, 0.4]), inference=0)</span><br><span class="line">PredictionResult(example=array([6.9, 3.1, 5.4, 2.1]), inference=2)</span><br><span class="line">PredictionResult(example=array([6.2, 2.2, 4.5, 1.5]), inference=1)</span><br><span class="line">PredictionResult(example=array([5.2, 4.1, 1.5, 0.1]), inference=0)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h1 id="DataflowでGPUを使う際の事前準備と基本的な使い方"><a href="#DataflowでGPUを使う際の事前準備と基本的な使い方" class="headerlink" title="DataflowでGPUを使う際の事前準備と基本的な使い方"></a>DataflowでGPUを使う際の事前準備と基本的な使い方</h1><p>DataflowでGPUを使用したい場合（例えば機械学習モデルの推論など）には、Dockerと組み合わせることでGPUを使用できます。<br>基本的な流れは<a href="#datflow%E3%81%AE%E4%BA%8B%E5%89%8D%E6%BA%96%E5%82%99%E3%81%A8%E5%9F%BA%E6%9C%AC%E7%9A%84%E3%81%AA%E4%BD%BF%E3%81%84%E6%96%B9">Datflowの事前準備と基本的な使い方</a>と同じです。違いはDockerイメージの準備とパイプラインに追加で渡すオプションが増えることくらいです。ここでは</p>
<ol>
<li>Dockerイメージの準備</li>
<li>GPU使用時のオプション</li>
</ol>
<p>について説明します。なお、<a href="#api%E3%81%AE%E6%9C%89%E5%8A%B9%E5%8C%96">APIの有効化</a>、<a href="#iam%E3%81%AE%E8%A8%AD%E5%AE%9A">IAMの設定</a>、<a href="#cloud-storage%E3%83%90%E3%82%B1%E3%83%83%E3%83%88%E3%81%AE%E4%BD%9C%E6%88%90">Cloud Storageバケットの作成</a>がお済みでない方はまずそちらから始めてください。</p>
<h2 id="Dockerイメージの準備"><a href="#Dockerイメージの準備" class="headerlink" title="Dockerイメージの準備"></a>Dockerイメージの準備</h2><p>DataflowでGPUを使用するには、Apache Beamが扱える、かつ、必要なGPUライブラリが入ったDockerイメージを用意する必要があります。ありがたいことに<a target="_blank" rel="noopener" href="https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/gpu-examples/pytorch-minimal">PyTorch用の最小イメージ</a>や<a target="_blank" rel="noopener" href="https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/gpu-examples/tensorflow-minimal">TensorFlow用の最小イメージ</a>のためのsampleが既に用意されているので、特に理由がなければこちらを利用するのが楽かと思います。</p>
<p>PyTorchを使用する場合には<a target="_blank" rel="noopener" href="https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/gpu-examples/pytorch-minimal">PyTorch用の最小イメージ</a>からファイルをダウンロード後、</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud builds submit --config build.yaml</span><br></pre></td></tr></table></figure>

<p>で、DockerイメージをContainer Registryに保存します（デフォルトでのイメージ名は<code>samples/dataflow/pytorch-gpu:latest</code>）。</p>
<p>なお、私の環境では、Pythonのバージョンが3.8ではパイプライン実行の際にエラー（<code>TypeError: code() takes at most 15 arguments (16 given)</code>）が発生してしまっていたため、Pythonのバージョンを3.7に落としました。具体的には以下のように変更することでエラーは発生しなくなりました。</p>
<ul>
<li><p>pyenvでPython 3.7の環境を用意<br>  ターミナル上で</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyenv install 3.7.13</span><br></pre></td></tr></table></figure>

<p>  を実行し、Python 3.7をインストールします。その後、</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pyenv virtualenv 3.7.13 dataflow_gpu</span><br><span class="line">pyenv activate dataflow_gpu</span><br></pre></td></tr></table></figure>

<p>  を実行してPython 3.7.13がインストールされた仮想環境（ここでは<code>dataflow_gpu</code>）をアクティベートします。<br>  続いて、Apache Beamをインストールしていきます。</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install apache-beam[gcp]</span><br></pre></td></tr></table></figure></li>
<li><p>Dockerfileを以下のように変更</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">FROM pytorch/pytorch:1.9.1-cuda11.1-cudnn8-runtime</span><br><span class="line"></span><br><span class="line">WORKDIR /pipeline</span><br><span class="line"></span><br><span class="line">COPY requirements.txt .</span><br><span class="line">COPY *.py ./</span><br><span class="line"></span><br><span class="line">RUN apt-get update \</span><br><span class="line">    &amp;&amp; apt-get install -y --no-install-recommends g++ \</span><br><span class="line">    &amp;&amp; apt-get install -y curl \  # この行を追加</span><br><span class="line">        python3.7 \  # この行を追加</span><br><span class="line">        python3-distutils \  # この行を追加</span><br><span class="line">    &amp;&amp; rm -rf /var/lib/apt/lists/* \</span><br><span class="line">    # Install the pipeline requirements and check that there are no conflicts.</span><br><span class="line">    # Since the image already has all the dependencies installed,</span><br><span class="line">    # there&#x27;s no need to run with the --requirements_file option.</span><br><span class="line">    &amp;&amp; pip install --no-cache-dir --upgrade pip \</span><br><span class="line">    &amp;&amp; pip install --no-cache-dir -r requirements.txt \</span><br><span class="line">    &amp;&amp; pip check</span><br><span class="line"></span><br><span class="line"># Set the entrypoint to Apache Beam SDK worker launcher.</span><br><span class="line">COPY --from=apache/beam_python3.8_sdk:2.38.0 /opt/apache/beam /opt/apache/beam</span><br><span class="line">ENTRYPOINT [ &quot;/opt/apache/beam/boot&quot; ]</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="GPU使用時のオプション"><a href="#GPU使用時のオプション" class="headerlink" title="GPU使用時のオプション"></a>GPU使用時のオプション</h2><p>DataflowでGPUを使用する際には、実行時に以下のようなオプションを追加で指定する必要があります。</p>
<div class="scroll"><table>
<thead>
<tr>
<th align="left">オプション名</th>
<th align="left">説明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">sdk_container_image</td>
<td align="left">使用するコンテナイメージの名前。</td>
</tr>
<tr>
<td align="left">disk_size_gb</td>
<td align="left">各ワーカー VM のブートディスクのサイズ</td>
</tr>
<tr>
<td align="left">experiments</td>
<td align="left">Dataflow Runner v2を使用するかやGPUのタイプ・個数、Nvidiaドライバをインストールするかを指定する際に使用。具体的な使い方は下の例を参照。</td>
</tr>
</tbody></table></div>
<p><code>experiments</code>オプションに関しては次のように指定します。下の例のように複数個に分けて指定してもOKです。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--experiments <span class="string">&quot;worker_accelerator=type:nvidia-tesla-t4;count:1;install-nvidia-driver&quot;</span> \</span><br><span class="line">--experiments <span class="string">&quot;use_runner_v2&quot;</span></span><br></pre></td></tr></table></figure>

<h1 id="Dataflowの使用例（GPUありver-）"><a href="#Dataflowの使用例（GPUありver-）" class="headerlink" title="Dataflowの使用例（GPUありver.）"></a>Dataflowの使用例（GPUありver.）</h1><p>ここでは、PyTorchのモデルの推論をDataflow上で行う例を扱っていきます。今回はMNISTデータセットで学習したモデルの重みパラメータ(<code>mnist_epoch_10.pth</code>)が既に手元にあるという想定で、そのモデルの推論（MNISTのテスト用データセットを使用）をDataflow上で行っていきます。以下のような手順で進めていきます。</p>
<ol>
<li>ソースコードの準備</li>
<li>Dockerコンテナイメージの作成</li>
<li>Cloud ShellでPythonの環境構築</li>
<li>パイプラインの実行</li>
</ol>
<p>なお、<a href="#api%E3%81%AE%E6%9C%89%E5%8A%B9%E5%8C%96">APIの有効化</a>、<a href="#iam%E3%81%AE%E8%A8%AD%E5%AE%9A">IAMの設定</a>、<a href="#cloud-storage%E3%83%90%E3%82%B1%E3%83%83%E3%83%88%E3%81%AE%E4%BD%9C%E6%88%90">Cloud Storageバケットの作成</a>がお済みでない方はまずそちらから始めてください。</p>
<h2 id="ソースコードの準備-1"><a href="#ソースコードの準備-1" class="headerlink" title="ソースコードの準備"></a>ソースコードの準備</h2><p>今回実行したいソースコード(ファイル名:<code>runinference_pytorch.py</code>)です。</p>
<figure class="highlight python"><figcaption><span>runinference_pytorch.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> apache_beam <span class="keyword">as</span> beam</span><br><span class="line"><span class="keyword">from</span> apache_beam.ml.inference.base <span class="keyword">import</span> RunInference</span><br><span class="line"><span class="keyword">from</span> apache_beam.ml.inference.pytorch_inference <span class="keyword">import</span> PytorchModelHandlerTensor</span><br><span class="line"><span class="keyword">from</span> apache_beam.options.pipeline_options <span class="keyword">import</span> PipelineOptions</span><br><span class="line"><span class="keyword">from</span> google.cloud <span class="keyword">import</span> storage</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pytorch_MNIST <span class="keyword">import</span> MNIST_Model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">upload_model_to_gcs</span>(<span class="params">local_model_path, gcs_model_path, project_name, bucket_name</span>):</span><br><span class="line">    client = storage.Client(project_name)</span><br><span class="line">    bucket = storage.Bucket(client)</span><br><span class="line">    bucket.name = bucket_name</span><br><span class="line">    blob = bucket.blob(gcs_model_path)</span><br><span class="line">    blob.upload_from_filename(local_model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># パイプラインオプションの設定</span></span><br><span class="line">    pipeline_options = PipelineOptions()</span><br><span class="line">    options_dict = pipeline_options.display_data()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># データセットの準備</span></span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.1307</span>), (<span class="number">0.3081</span>), inplace=<span class="literal">True</span>)</span><br><span class="line">    ])</span><br><span class="line">    test_dataset = datasets.MNIST(</span><br><span class="line">        root=<span class="string">&quot;./data/&quot;</span>,</span><br><span class="line">        train=<span class="literal">False</span>,</span><br><span class="line">        transform=transform,</span><br><span class="line">        download=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># モデルのアップロード</span></span><br><span class="line">    upload_model_to_gcs(</span><br><span class="line">        local_model_path=<span class="string">&quot;./models/pytorch_models/mnist_epoch_10.pth&quot;</span>,</span><br><span class="line">        gcs_model_path=<span class="string">&quot;models/pytorch_models/mnist_epoch_10.pth&quot;</span>,</span><br><span class="line">        project_name=options_dict[<span class="string">&quot;project&quot;</span>],</span><br><span class="line">        bucket_name=options_dict[<span class="string">&quot;bucket_name&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ハンドラーの設定</span></span><br><span class="line">    model_handler = PytorchModelHandlerTensor(</span><br><span class="line">        state_dict_path=<span class="string">&quot;gs://&#123;&#125;/models/pytorch_models/mnist_epoch_10.pth&quot;</span>.<span class="built_in">format</span>(options_dict[<span class="string">&quot;bucket_name&quot;</span>]),</span><br><span class="line">        model_class=MNIST_Model,</span><br><span class="line">        model_params=&#123;&#125;,</span><br><span class="line">        device=<span class="string">&quot;GPU&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># パイプライン実行</span></span><br><span class="line">    logging.getLogger().setLevel(logging.INFO)</span><br><span class="line">    <span class="keyword">with</span> beam.Pipeline(options=pipeline_options) <span class="keyword">as</span> p:</span><br><span class="line">        data = p | <span class="string">&quot;read&quot;</span> &gt;&gt; beam.Create(test_dataset)</span><br><span class="line">        test = (</span><br><span class="line">            data</span><br><span class="line">            | <span class="string">&quot;extract image&quot;</span> &gt;&gt; beam.Map(<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">            | <span class="string">&quot;inference&quot;</span> &gt;&gt; RunInference(model_handler)</span><br><span class="line">            | beam.io.WriteToText(options_dict[<span class="string">&quot;output_executable_path&quot;</span>], shard_name_template=<span class="string">&quot;&quot;</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>モデルの構造を定義したコード(ファイル名:<code>pytorch_MNIST.py</code>)です。</p>
<figure class="highlight python"><figcaption><span>pytorch_MNIST.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MNIST_Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.feature = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">147</span>, <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.feature(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>これらのソースコードはCloud Shellの同一のディレクトリに置いてください。また、モデルの重みパラメータまでのpathは<code>&#123;ソースコードがあるディレクトリ&#125;/models/pytorch_models/mnist_epoch_10.pth</code>です。</p>
<h2 id="Dockerコンテナイメージの作成"><a href="#Dockerコンテナイメージの作成" class="headerlink" title="Dockerコンテナイメージの作成"></a>Dockerコンテナイメージの作成</h2><p>続いて、Dockerイメージを準備していきます。<a target="_blank" rel="noopener" href="https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/gpu-examples/pytorch-minimal">PyTorch用の最小イメージ</a>からファイルをダウンロード後、それらのファイルをソースコードと同一のディレクトリに置きます。続いてDockerfileを以下のように変更します。</p>
<figure class="highlight dockerfile"><figcaption><span>Dockerfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> pytorch/pytorch:<span class="number">1.9</span>.<span class="number">1</span>-cuda11.<span class="number">1</span>-cudnn8-runtime</span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /pipeline</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> requirements.txt .</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> *.py ./</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> apt-get update \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; apt-get install -y --no-install-recommends g++ \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; apt-get install -y curl \  <span class="comment"># この行を追加</span></span></span><br><span class="line">        python3.<span class="number">7</span> \  <span class="comment"># この行を追加</span></span><br><span class="line">        python3-distutils \  <span class="comment"># この行を追加</span></span><br><span class="line">    &amp;&amp; rm -rf /var/lib/apt/lists/* \</span><br><span class="line">    <span class="comment"># Install the pipeline requirements and check that there are no conflicts.</span></span><br><span class="line">    <span class="comment"># Since the image already has all the dependencies installed,</span></span><br><span class="line">    <span class="comment"># there&#x27;s no need to run with the --requirements_file option.</span></span><br><span class="line">    &amp;&amp; pip install --no-cache-dir --upgrade pip \</span><br><span class="line">    &amp;&amp; pip install --no-cache-dir -r requirements.txt \</span><br><span class="line">    &amp;&amp; pip check</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the entrypoint to Apache Beam SDK worker launcher.</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> --from=apache/beam_python3.8_sdk:2.38.0 /opt/apache/beam /opt/apache/beam</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> [ <span class="string">&quot;/opt/apache/beam/boot&quot;</span> ]</span></span><br></pre></td></tr></table></figure>

<p>その後、コンテナイメージをContainer Registryに保存するために以下のコマンドを実行します。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud builds submit --config build.yaml</span><br></pre></td></tr></table></figure>

<p>コンテナイメージ名は<code>samples/dataflow/pytorch-gpu:latest</code>で保存されます。</p>
<h2 id="Cloud-ShellでPythonの環境構築-1"><a href="#Cloud-ShellでPythonの環境構築-1" class="headerlink" title="Cloud ShellでPythonの環境構築"></a>Cloud ShellでPythonの環境構築</h2><p>次にCloud ShellのPython環境を構築していきます。<br>まず、Python 3.7の環境を準備します。ターミナル上で</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyenv install 3.7.13</span><br></pre></td></tr></table></figure>

<p>を実行し、Python 3.7をインストールします。その後、</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pyenv virtualenv 3.7.13 dataflow_gpu</span><br><span class="line">pyenv activate dataflow_gpu</span><br></pre></td></tr></table></figure>

<p>を実行してPython 3.7.13がインストールされた仮想環境（ここでは<code>dataflow_gpu</code>）をアクティベートします。<br>続いて、必要なパッケージをインストールしていきます。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip3 install apache-beam[gcp] google-gcloud-storage</span><br><span class="line">pip3 install torch torchvision</span><br></pre></td></tr></table></figure>

<h2 id="パイプラインの実行-1"><a href="#パイプラインの実行-1" class="headerlink" title="パイプラインの実行"></a>パイプラインの実行</h2><p>必要なパッケージのインストールが終わったら、最後にパイプラインを実行していきます。<br>以下のコマンドを実行するとDataflow上でパイプライン処理が動き始めます。<code>&#123;プロジェクトID&#125;</code>、<code>&#123;ジョブの名前&#125;</code>、<code>&#123;バケットの名前&#125;</code>は適宜変更してください。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">python runinference_pytorch.py \</span><br><span class="line">--runner <span class="string">&quot;DataflowRunner&quot;</span> \</span><br><span class="line">--project <span class="string">&quot;&#123;プロジェクトID&#125;&quot;</span> \</span><br><span class="line">--job_name <span class="string">&quot;&#123;ジョブの名前&#125;&quot;</span> \</span><br><span class="line">--temp_location <span class="string">&quot;gs://&#123;バケットの名前&#125;/temp/&quot;</span> \</span><br><span class="line">--staging_location <span class="string">&quot;gs://&#123;バケットの名前&#125;/stage/&quot;</span> \</span><br><span class="line">--region <span class="string">&quot;asia-northeast1&quot;</span> \</span><br><span class="line">--bucket_name <span class="string">&quot;&#123;バケットの名前&#125;&quot;</span> \</span><br><span class="line">--output <span class="string">&quot;gs://&#123;バケットの名前&#125;/output.txt&quot;</span> \</span><br><span class="line">--sdk_container_image <span class="string">&quot;gcr.io/&#123;プロジェクトID&#125;/samples/dataflow/pytorch-gpu:latest&quot;</span> \</span><br><span class="line">--disk_size_gb 50 \</span><br><span class="line">--experiments <span class="string">&quot;worker_accelerator=type:nvidia-tesla-t4;count:1;install-nvidia-driver&quot;</span> \</span><br><span class="line">--experiments <span class="string">&quot;use_runner_v2&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="結果-1"><a href="#結果-1" class="headerlink" title="結果"></a>結果</h2><p>推論結果はCloud Storageのバケットの<code>output.txt</code>に出力されます。今回の例では以下のような結果が得られました。</p>
<figure class="highlight text"><figcaption><span>output.txt</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tensor([ -8.2468,  -2.1803,  -9.8459,   1.3747,   2.4845,  -5.6996, -18.9429,</span><br><span class="line">          3.0085,  -5.7692,  12.0357], requires_grad=True)</span><br><span class="line">tensor([ -5.9876, -14.5651,  -7.3873,   8.2820,  -6.1497,   1.6121, -18.5136,</span><br><span class="line">         -9.5785,   1.7698,  12.8093], requires_grad=True)</span><br><span class="line">tensor([  9.2505,  -1.7219,  -2.7147,  -3.9045, -10.8319,  -1.9610,   2.5355,</span><br><span class="line">         -8.6489,  -3.3169,  -6.9540], requires_grad=True)</span><br><span class="line">tensor([-8.1391, -0.9647, -6.3984,  2.4964, -0.9498,  1.4407, -8.2989, -3.1957,</span><br><span class="line">         2.5867,  2.6507], requires_grad=True)</span><br><span class="line">tensor([-7.6571, -2.4950, -5.2014, -1.6730, 10.1947, -7.5948, -9.2541,  0.5039,</span><br><span class="line">        -2.6531,  7.1487], requires_grad=True)</span><br><span class="line">tensor([ -5.8362,  12.8431,  -4.1835,  -8.8176,  -6.0804, -10.7981,  -6.2982,</span><br><span class="line">         -0.1830,  -1.4379,  -4.4298], requires_grad=True)</span><br><span class="line">tensor([-4.6527, -7.1966, -8.8277, -7.4921,  6.7380, -4.9899, -0.2908, -4.7030,</span><br><span class="line">         2.0198,  2.2414], requires_grad=True)</span><br><span class="line">tensor([-9.9818, -9.7239, -4.4335, -2.8926,  7.8835,  1.4599, -1.7376, -6.2337,</span><br><span class="line">        -0.9638, -0.7414], requires_grad=True)</span><br><span class="line">tensor([ -3.8291,  -2.5081,  16.6454,   6.6208,  -7.5311, -10.9999, -13.9144,</span><br><span class="line">         -5.1685,   2.5498,  -7.2168], requires_grad=True)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h1 id="他のGCPサービスとの連携とストリーミング処理"><a href="#他のGCPサービスとの連携とストリーミング処理" class="headerlink" title="他のGCPサービスとの連携とストリーミング処理"></a>他のGCPサービスとの連携とストリーミング処理</h1><p>最後に、Pub&#x2F;Subからリアルタイムにデータを取得→Dataflowでデータ処理→結果をBigQueryに書き出す例を紹介します。<br>今回はIrisデータセットの各サンプルを10秒間隔でPub&#x2F;SubにPublishし、<a href="#dataflow%E3%81%AE%E4%BD%BF%E7%94%A8%E4%BE%8Bgpu%E3%81%AA%E3%81%97ver">Dataflowの使用例（GPUなしver.）</a>で行ったscikit-learnモデルを用いた推論をストリーミング処理でDataflow上で行い、その結果をBigQueryに書き出します。今回もIrisデータセットで学習したモデルの重みパラメータ(<code>SVC_iris.pkl2</code>)が既に手元にあるという想定で、以下のような手順で進めていきます。</p>
<ol>
<li>ソースコードの準備</li>
<li>Pub&#x2F;Sub・BigQueryの準備</li>
<li>パイプラインの実行</li>
</ol>
<p>なお、<a href="#api%E3%81%AE%E6%9C%89%E5%8A%B9%E5%8C%96">APIの有効化</a>、<a href="#iam%E3%81%AE%E8%A8%AD%E5%AE%9A">IAMの設定</a>、<a href="#cloud-storage%E3%83%90%E3%82%B1%E3%83%83%E3%83%88%E3%81%AE%E4%BD%9C%E6%88%90">Cloud Storageバケットの作成</a>がお済みでない方はまずそちらから始めてください。</p>
<h2 id="ソースコードの準備-2"><a href="#ソースコードの準備-2" class="headerlink" title="ソースコードの準備"></a>ソースコードの準備</h2><p>今回実行したいソースコード(ファイル名:<code>predict_iris_dataflow_pubsub2bq.py</code>)です。<br>モデルの重みパラメータまでのpathは<code>&#123;ソースコードがあるディレクトリ&#125;/models/sklearn_models/SVC_iris.pkl2</code>です。</p>
<figure class="highlight python"><figcaption><span>predict_iris_dataflow_pubsub2bq.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> apache_beam <span class="keyword">as</span> beam</span><br><span class="line"><span class="keyword">from</span> apache_beam.ml.inference <span class="keyword">import</span> RunInference</span><br><span class="line"><span class="keyword">from</span> apache_beam.ml.inference.sklearn_inference <span class="keyword">import</span> ModelFileType, SklearnModelHandlerNumpy</span><br><span class="line"><span class="keyword">from</span> apache_beam.options.pipeline_options <span class="keyword">import</span> PipelineOptions, StandardOptions</span><br><span class="line"><span class="keyword">from</span> google.cloud <span class="keyword">import</span> storage</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">upload_model_to_gcs</span>(<span class="params">local_model_path, gcs_model_path, project_name, bucket_name</span>):</span><br><span class="line">    client = storage.Client(project_name)</span><br><span class="line">    bucket = storage.Bucket(client)</span><br><span class="line">    bucket.name = bucket_name</span><br><span class="line">    blob = bucket.blob(gcs_model_path)</span><br><span class="line">    blob.upload_from_filename(local_model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># パイプラインオプションの設定</span></span><br><span class="line">    options = PipelineOptions()</span><br><span class="line">    options_dict = options.display_data()</span><br><span class="line">    options.view_as(StandardOptions).runner = <span class="string">&quot;DataflowRunner&quot;</span></span><br><span class="line">    options.view_as(StandardOptions).streaming = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># モデルのアップロード</span></span><br><span class="line">    upload_model_to_gcs(</span><br><span class="line">        local_model_path=<span class="string">&quot;./models/sklearn_models/SVC_iris.pkl2&quot;</span>,</span><br><span class="line">        gcs_model_path=<span class="string">&quot;models/sklearn_models/SVC_iris.pkl2&quot;</span>,</span><br><span class="line">        project_name=options_dict[<span class="string">&quot;project&quot;</span>],</span><br><span class="line">        bucket_name=options_dict[<span class="string">&quot;bucket_name&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ハンドラーの設定</span></span><br><span class="line">    model_uri = <span class="string">&quot;gs://&#123;&#125;/models/sklearn_models/SVC_iris.pkl2&quot;</span>.<span class="built_in">format</span>(options_dict[<span class="string">&quot;bucket_name&quot;</span>])</span><br><span class="line">    model_file_type = ModelFileType.JOBLIB</span><br><span class="line">    model_handler = SklearnModelHandlerNumpy(model_uri=model_uri, model_file_type=model_file_type)</span><br><span class="line"></span><br><span class="line">    topic = <span class="string">&quot;projects/&#123;&#125;/topics/&#123;&#125;&quot;</span>.<span class="built_in">format</span>(options_dict[<span class="string">&quot;project&quot;</span>], options_dict[<span class="string">&quot;topic_name&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># パイプライン実行</span></span><br><span class="line">    logging.getLogger().setLevel(logging.INFO)</span><br><span class="line">    <span class="keyword">with</span> beam.Pipeline(options=options) <span class="keyword">as</span> p:</span><br><span class="line">        raw_data = (</span><br><span class="line">            p</span><br><span class="line">            | <span class="string">&quot;ReadFromPub/Sub&quot;</span> &gt;&gt; beam.io.ReadFromPubSub(topic)</span><br><span class="line">            | <span class="string">&quot;Decode&quot;</span> &gt;&gt; beam.Map(<span class="keyword">lambda</span> x: x.decode())</span><br><span class="line">            | <span class="string">&quot;StrToDict&quot;</span> &gt;&gt; beam.Map(json.loads)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        inference = (</span><br><span class="line">            raw_data</span><br><span class="line">            | <span class="string">&quot;ExtractFeature&quot;</span> &gt;&gt; beam.Map(<span class="keyword">lambda</span> x: x[<span class="string">&quot;feature&quot;</span>])</span><br><span class="line">            | <span class="string">&quot;RunInference&quot;</span> &gt;&gt; RunInference(model_handler)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        write2bq = (</span><br><span class="line">            inference</span><br><span class="line">            | <span class="string">&quot;ConvertToBigQueryFormat&quot;</span> &gt;&gt; beam.Map(<span class="keyword">lambda</span> x: &#123;</span><br><span class="line">                <span class="string">&quot;input&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;sepal_length&quot;</span>: x[<span class="number">0</span>][<span class="number">0</span>],</span><br><span class="line">                    <span class="string">&quot;sepal_width&quot;</span>: x[<span class="number">0</span>][<span class="number">1</span>],</span><br><span class="line">                    <span class="string">&quot;petal_length&quot;</span>: x[<span class="number">0</span>][<span class="number">2</span>],</span><br><span class="line">                    <span class="string">&quot;petal_width&quot;</span>: x[<span class="number">0</span>][<span class="number">3</span>]</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;predict&quot;</span>: x[<span class="number">1</span>].item()</span><br><span class="line">            &#125;)</span><br><span class="line">            | <span class="string">&quot;WriteToBigQuery&quot;</span> &gt;&gt; beam.io.WriteToBigQuery(table=options_dict[<span class="string">&quot;table_name&quot;</span>], dataset=options_dict[<span class="string">&quot;dataset_name&quot;</span>])</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>

<p>また、Irisデータセットの各サンプルを10秒間隔でPub&#x2F;SubにPublishにするためのコード（ファイル名:<code>publish_iris_local2pubsub.py</code>）です。</p>
<figure class="highlight python"><figcaption><span>publish_iris_local2pubsub.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> google.cloud <span class="keyword">import</span> pubsub</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--project&quot;</span>, required=<span class="literal">True</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--topic_name&quot;</span>, required=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    data = load_iris()</span><br><span class="line">    feature = data.data</span><br><span class="line">    target = data.target</span><br><span class="line"></span><br><span class="line">    publisher = pubsub.PublisherClient()</span><br><span class="line">    topic_path = publisher.topic_path(args.project, args.topic_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, (f, t) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(feature, target)):</span><br><span class="line">        f_t_dict = &#123;<span class="string">&quot;id&quot;</span>: i, <span class="string">&quot;feature&quot;</span>: f.tolist(), <span class="string">&quot;target&quot;</span>: t.item()&#125;</span><br><span class="line">        message = json.dumps(f_t_dict)</span><br><span class="line">        <span class="built_in">print</span>(message)</span><br><span class="line">        b_message = message.encode()</span><br><span class="line">        publisher.publish(topic_path, b_message)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Pub-x2F-Sub・BigQueryの準備"><a href="#Pub-x2F-Sub・BigQueryの準備" class="headerlink" title="Pub&#x2F;Sub・BigQueryの準備"></a>Pub&#x2F;Sub・BigQueryの準備</h2><p>まず、Pub&#x2F;Subのトピック作成から始めていきます。Pub&#x2F;Subのページ上部にある「トピックを作成」から、トピックIDを設定してトピックを作成します。そのほかの設定に関しては今回はデフォルトのままで大丈夫です。<br><img src="/images/20220920b/make_topic.png" alt="make_topic.png" width="1200" height="691" loading="lazy"></p>
<p>続いて、BigQueryのデータセット・テーブルの作成に入ります。BigQueryのデータセット・テーブルは以下のようにして作成できます。<br><img src="/images/20220920b/make_dataset.gif" alt="make_dataset.gif" width="1200" height="675" loading="lazy"></p>
<p>なお、今回使用しているスキーマは以下の通りです。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;input&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;RECORD&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;NULLABLE&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sepal_length&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;FLOAT&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;NULLABLE&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sepal_width&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;FLOAT&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;NULLABLE&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;petal_length&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;FLOAT&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;NULLABLE&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;petal_width&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;FLOAT&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;NULLABLE&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;predict&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;INTEGER&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;NULLABLE&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>

<h2 id="パイプラインの実行-2"><a href="#パイプラインの実行-2" class="headerlink" title="パイプラインの実行"></a>パイプラインの実行</h2><p>続いて、パイプラインの実行に移ります。以下のコマンドを実行するとパイプラインが動き始めます。<code>&#123;プロジェクトID&#125;</code>、<code>&#123;ジョブの名前&#125;</code>、<code>&#123;バケットの名前&#125;</code>、<code>&#123;テーブルの名前&#125;</code>、<code>&#123;データセットの名前&#125;</code>、<code>&#123;トピックの名前&#125;</code>は適宜変更してください。今回はRunnerおよびストリーミング処理のオプションはコード内で記述しているためコマンドライン引数から渡す必要はありません。ストリーミング処理をコマンドラインから有効化したい場合は、<code>--streaming</code>を加えるとできます。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">python predict_iris_dataflow_pubsub2bq.py \</span><br><span class="line">--project <span class="string">&quot;&#123;プロジェクトID&#125;&quot;</span> \</span><br><span class="line">--job_name <span class="string">&quot;&#123;ジョブの名前&#125;&quot;</span> \</span><br><span class="line">--temp_location <span class="string">&quot;gs://&#123;バケットの名前&#125;/temp/&quot;</span> \</span><br><span class="line">--staging_location <span class="string">&quot;gs://&#123;バケットの名前&#125;/stage/&quot;</span> \</span><br><span class="line">--region <span class="string">&quot;asia-northeast1&quot;</span> \</span><br><span class="line">--bucket_name <span class="string">&quot;&#123;バケットの名前&#125;&quot;</span> \</span><br><span class="line">--table_name <span class="string">&quot;&#123;テーブルの名前&#125;&quot;</span> \</span><br><span class="line">--dataset_name <span class="string">&quot;&#123;データセットの名前&#125;&quot;</span> \</span><br><span class="line">--topic_name <span class="string">&quot;&#123;トピックの名前&#125;&quot;</span></span><br></pre></td></tr></table></figure>

<p>これでパイプラインが実行されます。</p>
<p>パイプラインのジョブが動き始めたら、以下のコマンドで、Irisデータセットの各サンプルをPublishしていきます。なお、PythonファイルからPub&#x2F;SubにPublishする際にはサービスアカウントキー作成する必要があります。<code>IAMと管理→サービスアカウント</code>からサービスアカウントキーを含むjsonファイルを作成し</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> GOOGLE_APPLICATION_CREDENTIALS=<span class="string">&quot;&#123;jsonファイルまでのpath&#125;&quot;</span></span><br></pre></td></tr></table></figure>

<p>で、PythonファイルからPub&#x2F;SubにPublishできるようになります。それが終わったら</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python publish_iris_local2pubsub.py \</span><br><span class="line">--project <span class="string">&quot;&#123;プロジェクトID&#125;&quot;</span> \</span><br><span class="line">--topic_name <span class="string">&quot;&#123;トピックの名前&#125;&quot;</span></span><br></pre></td></tr></table></figure>

<p>を実行して、Pub&#x2F;Subに10秒間隔でデータを送ります。</p>
<h2 id="結果-2"><a href="#結果-2" class="headerlink" title="結果"></a>結果</h2><p>BigQueryの画面からクエリを実行して結果を確認します。クエリは下図の赤枠の部分を順にクリックして<br><img src="/images/20220920b/make_query.png" alt="make_query.png" width="702" height="486" loading="lazy"></p>
<p>開いたエディタに</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> `&#123;プロジェクトID&#125;.&#123;データセットの名前&#125;.&#123;テーブルの名前&#125;` LIMIT <span class="number">1000</span></span><br></pre></td></tr></table></figure>

<p>を入力して実行します。</p>
<p>今回の例では以下のような結果が得られました。<br><img src="/images/20220920b/pubsub2bq_result.png" alt="pubsub2bq_result" width="1164" height="822" loading="lazy"></p>
<h1 id="最後に"><a href="#最後に" class="headerlink" title="最後に"></a>最後に</h1><p>今回のインターンで扱わせていただいたDataflowは、なかなか個人で扱う機会がない一方で、ビジネスの場面ではとても需要のあるサービスです。そのようなものを扱う機会を頂けたことは今回のインターンに参加してよかったと思えることの１つです。また、私は今まで技術ブログを書いた経験がなかったため、今回のインターンで、学んだことを言語化しまとめることの難しさを知ることができました。</p>
<p>そのほかにも、インターンではSAIG（フューチャーのAIチーム）の進捗報告会に参加させていただき、さまざまなプロジェクトの存在、各プロジェクトの進め方、各プロジェクトの難しさなど実際の仕事の現場を体験することができました。また、インターンのイベントの一環である社員の方にインタビューをさせていただき、そこでは専門分野の勉強の進め方、AIのトレンドのキャッチアップのやり方を教えていただきました。</p>
<p>今回のインターンでは本当に多くのことを学ばせていただきました。受け入れ先プロジェクトの方々やフューチャーHRの皆さん、本当にありがとうございました！</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a target="_blank" rel="noopener" href="https://qiita.com/esakik/items/3c5c18d4a645db7a8634">Apache Beam (Dataflow) 実践入門【Python】</a></li>
<li><a target="_blank" rel="noopener" href="https://beam.apache.org/documentation/runtime/model/">How Beam executes a pipeline (公式ドキュメント)</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/dataflow/docs/quickstarts/create-pipeline-python">Python を使用して Dataflow パイプラインを作成する</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/dataflow/docs/guides/using-gpus">GPUの使用</a></li>
</ul>
<p>アイキャッチは<a target="_blank" rel="noopener" href="https://pixabay.com/ja/users/paulbr75-2938186/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2471293">Paul Brennan</a>による<a target="_blank" rel="noopener" href="https://pixabay.com/ja//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2471293">Pixabay</a>からの画像です。</p>

          
        </div>
        <footer>
          <section class="social-area">
          <!-- シェアボタン START -->
  <ul class="social-button">
    
    <!-- Twitter -->
    <li>
      <a class="social-btn twitter-btn" target="_blank" href="https://twitter.com/share?url=https://future-architect.github.io/articles/20220920b/&related=twitterapi%2Ctwitter&text=Dataflow%E5%BE%8C%E7%B7%A8%EF%BC%88Dataflow%E3%81%AE%E4%BA%8B%E5%89%8D%E6%BA%96%E5%82%99%E3%81%8B%E3%82%89Pub/Sub%E3%83%BBBigQuery%E3%81%A8%E3%81%AE%E9%80%A3%E6%90%BA%E4%BE%8B%E3%81%BE%E3%81%A7%EF%BC%89%20%7C%20%E3%83%95%E3%83%A5%E3%83%BC%E3%83%81%E3%83%A3%E3%83%BC%E6%8A%80%E8%A1%93%E3%83%96%E3%83%AD%E3%82%B0" rel="nofollow noopener">
        <i></i><span class="social-btn-label">5</span>
      </a>
    </li>
    <!-- Facebook -->
    <li>
      <a class="social-btn fb-btn" target="_blank" href="http://www.facebook.com/share.php?u=https://future-architect.github.io/articles/20220920b/&t=Dataflow%E5%BE%8C%E7%B7%A8%EF%BC%88Dataflow%E3%81%AE%E4%BA%8B%E5%89%8D%E6%BA%96%E5%82%99%E3%81%8B%E3%82%89Pub/Sub%E3%83%BBBigQuery%E3%81%A8%E3%81%AE%E9%80%A3%E6%90%BA%E4%BE%8B%E3%81%BE%E3%81%A7%EF%BC%89" rel="nofollow noopener">
        <i></i><span class="social-btn-label">シェア</span>
      </a>
    </li>
    <!-- hatebu -->
    <li>
      <a class="social-btn hatebu-btn" target="_blank" href="https://b.hatena.ne.jp/entry/s/future-architect.github.io/articles/20220920b/" rel="nofollow noopener">
        <i></i><span class="social-btn-label">はてな</span>
      </a>
    </li>
    <!-- pocket -->
    <li>
      <a class="social-btn pocket-btn" target="_blank" href="https://getpocket.com/save?url=https://future-architect.github.io/articles/20220920b/" rel="nofollow noopener">
        <i></i><span class="social-btn-label">7</span>
      </a>
    </li>
    
  </ul>
<!-- シェアボタン END -->

          </section>
          <aside>
            <section class="related-post margin-bottom-40 nav">
              <h2 id="related"><a href="#related" class="headerlink" title="関連記事"></a>関連記事</h2>
              
  <div class="widget">
    <ul class="nav related-post-link"><li class="related-posts-item"><span>2022.09.20</span><span class="snscount">&#9825;10</span><a href=/articles/20220920a/ title="フューチャーのインターンEngineer Campに参加した平野と申します。今回のインターンでは、Google Cloud Platform (GCP)のサービスとして提供されているDataflowについて調査し、その仕組みや使い方についてこの技術ブログにまとめることに取り組みました。">Dataflow前編（Dataflowの概要からApache Beamの使い方まで）</a></li><li class="related-posts-item"><span>2023.02.13</span><span class="snscount">&#9825;11</span><a href=/articles/20230213a/ title="Vertex AI Pipelinesを利用してみて分かったTipsについて、いくつかピックアップしてまとめました。なお、コードは全てPython・Kubeflowを用いた場合を記載しています。Vertex AI Pipelinesとは、GCP上でMLパイプライン機能を提供するサービスです。サーバーレス方式でMLワークフローをオーケストレートします。">Vertex AI PipelinesのTips</a></li><li class="related-posts-item"><span>2022.11.17</span><span class="snscount">&#9825;7</span><a href=/articles/20221117a/ title="フューチャーのサマーインターン2022 Engineer Campに参加いたしました。サブスクサービスのWebアプリ開発のインターンで学んだことをまとめていきます！ ">初めての長期インターンでWebアプリ開発を経験しました！</a></li><li class="related-posts-item"><span>2022.09.16</span><span class="snscount">&#9825;9</span><a href=/articles/20220916c/ title="作成したフォーマッタの実装について説明します。前編でも示しましたが、今回作成したフォーマッタの処理の流れを再度示します。">Engineer Camp2022 RustでSQLフォーマッタ作成（後編）</a></li><li class="related-posts-item"><span>2022.09.16</span><span class="snscount">&#9825;60</span><a href=/articles/20220916b/ title="みなさん、こんにちは！Future Engineer Camp 2022に参加した川渕と齋藤です。今回のインターンではSQLフォーマッタをRustで作成しました。私達が取り組んだ内容を紹介します。SQLフォーマッタとはSQLを統一された体裁にフォーマットしてくれるツールです。体裁を統一することで他人が見ても読みやすいコードになり、生産性が向上します。">Engineer Camp2022 RustでSQLフォーマッタ作成（前編）</a></li><li class="related-posts-item"><span>2022.09.16</span><span class="snscount">&#9825;14</span><a href=/articles/20220916a/ title="フューチャーのインターンシッププログラムの、Enginner Summer Camp 2022にてHealthCare Innovation Group（以下HIG）のインターンシップに参加した永田遊希です。インターン参加理由は...">Enginner Camp 2022に参加しました（HIG編）</a></li></ul>
  </div>
            </section>
            <section class="reference-post margin-bottom-40 nav">
              
  <div class="card">
    <div id="reference" class="reference-lede"><a href="#reference" class="headerlink" title="参照されている記事"></a>この記事を参照している記事</div>
    <ul class="reference-post-link"><li class="reference-posts-item"><a href=/articles/20220920a/ title="フューチャーのインターンEngineer Campに参加した平野と申します。今回のインターンでは、Google Cloud Platform (GCP)のサービスとして提供されているDataflowについて調査し、その仕組みや使い方についてこの技術ブログにまとめることに取り組みました。">Dataflow前編（Dataflowの概要からApache Beamの使い方まで）</a></li><li class="reference-posts-item"><a href=/articles/20220606b/ title="フューチャーのサマーインターン2022 Summer Engineer Camp🌞⛺🏃の募集が始まりました。フューチャーでは夏のインターンシップは2つのコースがあります。">フューチャー夏のインターンシップ2022（Engineer Camp）の募集を開始しました！</a></li></ul>
  </div>
            </section>
          </aside>
        </footer>
      </div>
    </article>
  </main>
  <aside class="col-md-3 blog-sidebar">
    <!-- START SIDEBAR  -->


<section class="toc-section">
  <h2 class="margin-top-30">目次</h2>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"><span class="toc-text">はじめに</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Datflow%E3%81%AE%E4%BA%8B%E5%89%8D%E6%BA%96%E5%82%99%E3%81%A8%E5%9F%BA%E6%9C%AC%E7%9A%84%E3%81%AA%E4%BD%BF%E3%81%84%E6%96%B9"><span class="toc-text">Datflowの事前準備と基本的な使い方</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#API%E3%81%AE%E6%9C%89%E5%8A%B9%E5%8C%96"><span class="toc-text">APIの有効化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IAM%E3%81%AE%E8%A8%AD%E5%AE%9A"><span class="toc-text">IAMの設定</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Apache-Beam-SDK%E3%81%AE%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB"><span class="toc-text">Apache Beam SDKのインストール</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud-Storage%E3%83%90%E3%82%B1%E3%83%83%E3%83%88%E3%81%AE%E4%BD%9C%E6%88%90"><span class="toc-text">Cloud Storageバケットの作成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataflow%E4%B8%8A%E3%81%A7%E3%83%91%E3%82%A4%E3%83%97%E3%83%A9%E3%82%A4%E3%83%B3%E3%82%92%E5%AE%9F%E8%A1%8C"><span class="toc-text">Dataflow上でパイプラインを実行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dataflow%E3%81%AE%E4%BD%BF%E7%94%A8%E4%BE%8B%EF%BC%88GPU%E3%81%AA%E3%81%97ver-%EF%BC%89"><span class="toc-text">Dataflowの使用例（GPUなしver.）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%B3%E3%83%BC%E3%83%89%E3%81%AE%E6%BA%96%E5%82%99"><span class="toc-text">ソースコードの準備</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud-Shell%E3%81%A7Python%E3%81%AE%E7%92%B0%E5%A2%83%E6%A7%8B%E7%AF%89"><span class="toc-text">Cloud ShellでPythonの環境構築</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%83%91%E3%82%A4%E3%83%97%E3%83%A9%E3%82%A4%E3%83%B3%E3%81%AE%E5%AE%9F%E8%A1%8C"><span class="toc-text">パイプラインの実行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B5%90%E6%9E%9C"><span class="toc-text">結果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dataflow%E3%81%A7GPU%E3%82%92%E4%BD%BF%E3%81%86%E9%9A%9B%E3%81%AE%E4%BA%8B%E5%89%8D%E6%BA%96%E5%82%99%E3%81%A8%E5%9F%BA%E6%9C%AC%E7%9A%84%E3%81%AA%E4%BD%BF%E3%81%84%E6%96%B9"><span class="toc-text">DataflowでGPUを使う際の事前準備と基本的な使い方</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Docker%E3%82%A4%E3%83%A1%E3%83%BC%E3%82%B8%E3%81%AE%E6%BA%96%E5%82%99"><span class="toc-text">Dockerイメージの準備</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPU%E4%BD%BF%E7%94%A8%E6%99%82%E3%81%AE%E3%82%AA%E3%83%97%E3%82%B7%E3%83%A7%E3%83%B3"><span class="toc-text">GPU使用時のオプション</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dataflow%E3%81%AE%E4%BD%BF%E7%94%A8%E4%BE%8B%EF%BC%88GPU%E3%81%82%E3%82%8Aver-%EF%BC%89"><span class="toc-text">Dataflowの使用例（GPUありver.）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%B3%E3%83%BC%E3%83%89%E3%81%AE%E6%BA%96%E5%82%99-1"><span class="toc-text">ソースコードの準備</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Docker%E3%82%B3%E3%83%B3%E3%83%86%E3%83%8A%E3%82%A4%E3%83%A1%E3%83%BC%E3%82%B8%E3%81%AE%E4%BD%9C%E6%88%90"><span class="toc-text">Dockerコンテナイメージの作成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud-Shell%E3%81%A7Python%E3%81%AE%E7%92%B0%E5%A2%83%E6%A7%8B%E7%AF%89-1"><span class="toc-text">Cloud ShellでPythonの環境構築</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%83%91%E3%82%A4%E3%83%97%E3%83%A9%E3%82%A4%E3%83%B3%E3%81%AE%E5%AE%9F%E8%A1%8C-1"><span class="toc-text">パイプラインの実行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B5%90%E6%9E%9C-1"><span class="toc-text">結果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%96%E3%81%AEGCP%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9%E3%81%A8%E3%81%AE%E9%80%A3%E6%90%BA%E3%81%A8%E3%82%B9%E3%83%88%E3%83%AA%E3%83%BC%E3%83%9F%E3%83%B3%E3%82%B0%E5%87%A6%E7%90%86"><span class="toc-text">他のGCPサービスとの連携とストリーミング処理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%B3%E3%83%BC%E3%83%89%E3%81%AE%E6%BA%96%E5%82%99-2"><span class="toc-text">ソースコードの準備</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pub-x2F-Sub%E3%83%BBBigQuery%E3%81%AE%E6%BA%96%E5%82%99"><span class="toc-text">Pub&#x2F;Sub・BigQueryの準備</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%83%91%E3%82%A4%E3%83%97%E3%83%A9%E3%82%A4%E3%83%B3%E3%81%AE%E5%AE%9F%E8%A1%8C-2"><span class="toc-text">パイプラインの実行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B5%90%E6%9E%9C-2"><span class="toc-text">結果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E5%BE%8C%E3%81%AB"><span class="toc-text">最後に</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-text">参考</span></a></li></ol>
</section>

<section class="category">
<h2 class="margin-top-30">カテゴリー</h2>
<div class="widget">
  <ul class="nav sidebar-categories margin-bottom-40">
  
  <li class=""><a href="/categories/Programming/">Programming (375)</a></li>
<li class=""><a href="/categories/Infrastructure/">Infrastructure (231)</a></li>
<li class=""><a href="/categories/Culture/">Culture (88)</a></li>
<li class=""><a href="/categories/DataScience/">DataScience (52)</a></li>
<li class=""><a href="/categories/IoT/">IoT (32)</a></li>
<li class=""><a href="/categories/DB/">DB (23)</a></li>
<li class=""><a href="/categories/Business/">Business (21)</a></li>
<li class=""><a href="/categories/%E8%AA%8D%E8%A8%BC%E8%AA%8D%E5%8F%AF/">認証認可 (20)</a></li>
<li class=""><a href="/categories/DevOps/">DevOps (19)</a></li>
<li class=""><a href="/categories/Management/">Management (15)</a></li>
<li class=""><a href="/categories/VR/">VR (12)</a></li>
<li class=""><a href="/categories/Security/">Security (12)</a></li>
<li class=""><a href="/categories/Design/">Design (11)</a></li>

  </ul>
</div>

</section>
<section class="podcast-link">
<h2 class="margin-top-30">Tech Cast</h2>

  <div class="class="widget-wrap">
  <div class="widget">
    <ul class="nav techcast">
      <li><a href="https://podcasters.spotify.com/pod/show/futuretechcast/episodes/38-AIAI-e22h1v0" title="フューチャーがお届けするポッドキャストです。#38 AIグループリーダー加藤さんに聞く「AIチームのミッションと展望」" target="_blank" rel="noopener"> #38 AIグループリーダー加藤さんに聞く「AIチームのミッションと展望」</a></li>
<li><a href="https://podcasters.spotify.com/pod/show/futuretechcast/episodes/37-e227p84" title="フューチャーがお届けするポッドキャストです。#37 自然言語処理を使った文書検索エンジンシステム開発と新規サービス検討（後編）" target="_blank" rel="noopener"> #37 自然言語処理を使った文書検索エンジンシステム開発と新規サービス検討（後編）</a></li>
<li><a href="https://podcasters.spotify.com/pod/show/futuretechcast/episodes/36-e1rdbcu" title="フューチャーがお届けするポッドキャストです。#36 自然言語処理を使った文書検索エンジンシステム開発と新規サービス検討（前編）" target="_blank" rel="noopener"> #36 自然言語処理を使った文書検索エンジンシステム開発と新規サービス検討（前編）</a></li>
    </ul>
  </div>
  </div>
  
</section>
<section class="advent-calendar">
<h2 class="margin-top-30">アドベントカレンダー</h2>
<div class="widget">
  <ul class="nav-flex">
    <li><a href="http://qiita.com/advent-calendar/2022/future" title="フューチャー Advent Calendar 2022 #Qiita" target="_blank" rel="noopener">2022年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2021/future" title="フューチャー Advent Calendar 2021 #Qiita" target="_blank" rel="noopener">2021年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2020/future" title="フューチャー Advent Calendar 2020 #Qiita" target="_blank" rel="noopener">2020年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2019/future" title="フューチャー Advent Calendar 2019 #Qiita" target="_blank" rel="noopener">2019年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2018/future" title="フューチャー Advent Calendar 2018 #Qiita" target="_blank" rel="noopener">2018年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2017/future" title="フューチャー Advent Calendar 2017 #Qiita" target="_blank" rel="noopener">2017年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2016/future" title="フューチャー Advent Calendar 2016 #Qiita" target="_blank" rel="noopener">2016年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2015/future" title="フューチャー Advent Calendar 2015 #Qiita" target="_blank" rel="noopener">2015年</a></li>
  </ul>
</div>

</section>
<!-- END SIDEBAR -->

  </aside>
</div>

  </section>
</div>

      <!-- BEGIN PRE-FOOTER -->
    <footer>
      <div class="pre-footer">
        <div class="container">
          <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-6 col-6 pre-footer-col">
              <h2>About Us</h2>
              <p>経営とITをデザインする、フューチャーの技術ブログです。業務で利用している幅広い技術について紹介します。<br /><br /><a target="_blank" rel="noopener" href="http://www.future.co.jp/">http://www.future.co.jp/</a></p>
              <div class="social-btn twitter-btn twitter-follow-btn">
                <a href="https://twitter.com/intent/follow?screen_name=future_techblog " target="_blank" rel="nofollow noopener">
                  <i></i><span class="tw-btn-label">フューチャー技術ブログをフォロー</span>
                </a>
              </div>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-4 col-4 pre-footer-col">
              <h2>Contact</h2>
              <address class="margin-bottom-40">
                <a href="https://www.future.co.jp/recruit/recruit/rec-fresh/" title="新卒採用" target="_blank" rel="noopener">新卒採用</a><br>
                <a href="https://www.future.co.jp/recruit/recruit/rec-career/" title="キャリア採用" target="_blank" rel="noopener">キャリア採用</a><br>
                <a href="https://www.future.co.jp/contact_us/" title="お問い合わせページ" target="_blank" rel="noopener">お問い合わせ</a><br>
                <a href="https://www.future.co.jp/architect/socialmediapolicy/" title="ソーシャルメディアポリシー" target="_blank" rel="noopener">メディアポリシー</a><br><br>
                <a href="mailto:techblog@future.co.jp">techblog@future.co.jp</a>
              </address>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6 col-6 pre-footer-col">
              <h2>Contents</h2>
              <a href="https://future-architect.github.io/coding-standards/" title="Future Enterprise Coding Standards" target="_blank" rel="noopener">コーディング規約</a><br>
              <a href="https://future-architect.github.io/typescript-guide/" title="仕事ですぐに使えるTypeScript" target="_blank" rel="noopener">仕事ですぐに使えるTypeScript</a><br>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-3 col-3 pre-footer-col">
              <h2>Event</h2>
              <a href="https://future.connpass.com/" title="経営とITをデザインするフューチャーの勉強会です" target="_blank" rel="noopener">connpass</a><br>
              <a href="https://www.future.co.jp/futureinsightseminar/" title="フューチャーインサイトセミナー" target="_blank" rel="noopener">Webセミナー</a><br>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-3 col-3 pre-footer-col">
              <h2>SNS</h2>
              <a href="https://github.com/future-architect" title="Future's official open source repositories" target="_blank" rel="noopener">GitHub</a><br>
              <a href="https://qiita.com/organizations/future" title="フューチャーのQiita Organizationです" target="_blank" rel="noopener">Qiita</a><br>
              <a href="https://note.future.co.jp/" title="フューチャーの公式note" target="_blank" rel="noopener">未来報</a><br>
              <a href="https://www.youtube.com/channel/UCJUSwYYd0CkGgmEKAW7QVpw" title="フューチャーYoutubeチャネル" target="_blank" rel="noopener">Youtube</a>
            </div>
          </div>
        </div>
      </div>
      <div class="footer">
        <div class="container">
          <div class="row">
            <div class="col-md-6 col-sm-6 padding-top-10">
              &copy; 2023 フューチャー技術ブログ<br>
            </div>
          </div>
        </div>
      </div>
    </footer>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-X1C28R8H0M"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-X1C28R8H0M');
  gtag('config', 'UA-74047147-1'); // 過渡期対応
</script>

  </div>
</body>
</html>
