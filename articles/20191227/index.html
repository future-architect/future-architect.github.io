<!DOCTYPE html>
<!--[if IE 8]> <html lang="ja" class="ie8 no-js"> <![endif]-->
<!--[if IE 9]> <html lang="ja" class="ie9 no-js"> <![endif]-->
<!--[if !IE]><!-->
<html lang="ja">
<!--<![endif]-->
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <!--
    ███████╗██╗░░░██╗████████╗██╗░░░██╗██████╗░███████╗
    ██╔════╝██║░░░██║╚══██╔══╝██║░░░██║██╔══██╗██╔════╝
    █████╗░░██║░░░██║░░░██║░░░██║░░░██║██████╔╝█████╗░░
    ██╔══╝░░██║░░░██║░░░██║░░░██║░░░██║██╔══██╗██╔══╝░░
    ██║░░░░░╚██████╔╝░░░██║░░░╚██████╔╝██║░░██║███████╗
    ╚═╝░░░░░░╚═════╝░░░░╚═╝░░░░╚═════╝░╚═╝░░╚═╝╚══════╝
    ████████╗███████╗░█████╗░██╗░░██╗
    ╚══██╔══╝██╔════╝██╔══██╗██║░░██║
    ░░░██║░░░█████╗░░██║░░╚═╝███████║
    ░░░██║░░░██╔══╝░░██║░░██╗██╔══██║
    ░░░██║░░░███████╗╚█████╔╝██║░░██║
    ░░░╚═╝░░░╚══════╝░╚════╝░╚═╝░░╚═╝
    ██████╗░██╗░░░░░░█████╗░░██████╗░
    ██╔══██╗██║░░░░░██╔══██╗██╔════╝░
    ██████╦╝██║░░░░░██║░░██║██║░░██╗░
    ██╔══██╗██║░░░░░██║░░██║██║░░╚██╗
    ██████╦╝███████╗╚█████╔╝╚██████╔╝
    ╚═════╝░╚══════╝░╚════╝░░╚═════╝░
    Welcome engineer.
    https://www.future.co.jp/recruit/
  -->
  
  <title>NeurIPS 2019 論文紹介 | フューチャー技術ブログ</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  
  <meta name="description" content="こんにちは、Strategic AI Group(SAIG)の田中、上野です。少し前にNeurIPSという学会に参加して来たことをご報告しましたが、今回はNeurIPSで気になった論文をいくつか紹介したいと思います。 画像認識・生成まずは、上野からは画像認識・生成に関する下記2つの研究を取り上げます。 This Looks Like That: Deep Learning for Interpre">
<meta property="og:type" content="article">
<meta property="og:title" content="NeurIPS 2019 論文紹介 | フューチャー技術ブログ">
<meta property="og:url" content="https://future-architect.github.io/articles/20191227/index.html">
<meta property="og:site_name" content="フューチャー技術ブログ">
<meta property="og:description" content="こんにちは、Strategic AI Group(SAIG)の田中、上野です。少し前にNeurIPSという学会に参加して来たことをご報告しましたが、今回はNeurIPSで気になった論文をいくつか紹介したいと思います。 画像認識・生成まずは、上野からは画像認識・生成に関する下記2つの研究を取り上げます。 This Looks Like That: Deep Learning for Interpre">
<meta property="og:locale" content="ja_JP">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/CAM.png">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/ProtoPNet.png">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/HR-CAM.png">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/IS.png">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/FID.png">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/HYPE.png">
<meta property="article:published_time" content="2019-12-27T07:22:02.000Z">
<meta property="article:modified_time" content="2021-04-11T08:20:20.652Z">
<meta property="article:tag" content="MachineLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://future-architect.github.io/images/20191227/CAM.png">
  
  <link rel="alternate" href="/atom.xml" title="フューチャー技術ブログ" type="application/atom+xml">
  
  
  <link rel="icon" href="/favicon.ico">
  
  <meta content="MachineLearning" name="keywords">
  <meta content="上野貴史" name="author">

  <link rel="stylesheet" href="/metronic/assets/plugins/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="/metronic/assets/corporate/css/style.css">
  <link rel="stylesheet" href="/css/theme-styles.css">
<meta name="generator" content="Hexo 5.4.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body class="corporate">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="wrap" itemscope itemtype="https://schema.org/TechArticle">
  <!-- BEGIN HEADER -->
<div class="header">
	<div class="header-overlay">
		<div class="header-menu"></div>
		<div class="header-title"><a href="/">Future Tech Blog</a></div>
		<div class="header-title-sub">フューチャー技術ブログ</div>
	</div>
</div>
<!-- Header END -->

  <div class="container">
  <ul class="breadcrumb">
    <li><a href="/">Home</a></li>
    <li><a href="/articles/">Blog</a></li>
    <li class="active">Post</li>
  </ul>
  <section id="main">
    
    <h2 itemprop="name">
      <a class="article-title" href="/articles/20191227/">NeurIPS 2019 論文紹介</a>
    </h2>


    <div class="row">
<div class="col-md-9 col-sm-9 blog-posts">
<article id="post-20191227-neurips" class="article article-type-post blog-item" itemscope itemprop="blogPost">
  <div class="article-meta">
  </div>
  <div class="article-inner">
    
    <header class="article-header">
    <ul class="blog-info">
    <li><a href="/authors/%E4%B8%8A%E9%87%8E%E8%B2%B4%E5%8F%B2">上野貴史</a></li>
    <li><time datetime="2019-12-27T07:22:02.000Z" itemprop="datePublished">2019/12/27</time>
</li>
    <li>
  
    <a href="/tags/MachineLearning/" title="MachineLearning">MachineLearning</a>
  

</li>
    </ul>
    
  <div class="article-category">
    
    Category: 
    
    <a class="article-category-link" href="/categories/DataScience/">DataScience</a>
  </div>
  <br>


    </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>こんにちは、Strategic AI Group(SAIG)の田中、上野です。<br>少し前に<a href="https://future-architect.github.io/articles/20191210/">NeurIPSという学会に参加して来たことをご報告</a>しましたが、今回はNeurIPSで気になった論文をいくつか紹介したいと思います。</p>
<h1 id="画像認識・生成"><a href="#画像認識・生成" class="headerlink" title="画像認識・生成"></a>画像認識・生成</h1><p>まずは、上野からは画像認識・生成に関する下記2つの研究を取り上げます。</p>
<h2 id="This-Looks-Like-That-Deep-Learning-for-Interpretable-Image-Recognition"><a href="#This-Looks-Like-That-Deep-Learning-for-Interpretable-Image-Recognition" class="headerlink" title="This Looks Like That: Deep Learning for Interpretable Image Recognition"></a>This Looks Like That: Deep Learning for Interpretable Image Recognition</h2><ul>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/9095-this-looks-like-that-deep-learning-for-interpretable-image-recognition">https://papers.nips.cc/paper/9095-this-looks-like-that-deep-learning-for-interpretable-image-recognition</a></li>
</ul>
<p>Chaofan Chen(※1), Oscar Li(※1), Daniel Tao(※1), Alina Barnett(※1), Cynthia Rudin(※1), Jonathan K. Su(※2)</p>
<p>※1: Duku University<br>※2: MIT Lincoln Laboratory</p>
<h2 id="HYPE-A-Benchmark-for-Human-eYe-Perceptual-Evaluation-of-Generative-Models"><a href="#HYPE-A-Benchmark-for-Human-eYe-Perceptual-Evaluation-of-Generative-Models" class="headerlink" title="HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models"></a>HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models</h2><ul>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/8605-hype-a-benchmark-for-human-eye-perceptual-evaluation-of-generative-models">https://papers.nips.cc/paper/8605-hype-a-benchmark-for-human-eye-perceptual-evaluation-of-generative-models</a></li>
</ul>
<p>Sharon Zhou(※1), Mitchell Gordon(※1), Ranjay Krishna(※1), Austin Narcomey(※1), Li F. Fei-Fei(※1), Michael Bernstein(※1)</p>
<p>※1: Stanford University</p>
<p>一つ目はCNNの解釈性に関する研究で、二つ目は生成モデルの評価方法に関する研究です。<br>それぞれ、関連する研究を取り上げながら紹介します。</p>
<h2 id="CNNの解釈性に関する研究"><a href="#CNNの解釈性に関する研究" class="headerlink" title="CNNの解釈性に関する研究"></a>CNNの解釈性に関する研究</h2><p>Deep Learningの威力を世に知らしめた出来事の一つは、2012年のILSVRCという画像認識に関するコンペティションでした。2012年以降、画像認識の精度がどんどん向上し、ついには人間のレベルに匹敵するまでになりました。</p>
<p>画像認識では、Convolutional Neural Network(CNN)と呼ばれる技術が用いられますが、非線形な演算を何層にも渡って繰り返すため、CNNが画像のどこに着目して分類をしているかといった解釈が非常に難しくなってしまいます。</p>
<p>CNNの着目領域を可視化した初期の研究が、Class Activation Mapping(CAM) [1]です。</p>
<p>CAMでは、下図のようにGlobal Average Poolingをする直前のfeature mapに分類層の結合重みを使った線形和によって、CNNの判断根拠を可視化します。</p>
<img src="/images/20191227/CAM.png">

<p>[1]のFigure2より引用</p>
<p>CAMは、conv feature maps → global average pooling → softmax layer という構成である必要がありましたが、Grad-CAM [2]では、勾配を用いてfeature mapの重み付けをすることで、ネットワーク構成の制約がなくなりました。</p>
<p>また、Attention Branch Network [3]では、Activation mapをAttentionに使う方法が提案されています。<br>Attention Branch Networkは、中部大学の研究グループが提案したこともあり、今夏に参加した日本の学会MIRUではよく見かけました。</p>
<p>NeurIPSでは、porototypical part network(ProtoPNet) [4]という手法が提案されました。ProtoPNetでは、Prototype layerによって、入力画像中のどの領域が、学習データのどの部分と類似しているかまでを判断することができます。</p>
<p>その結果、単に着目領域が可視化されるだけではなく、画像の部分ごとの判断根拠を組み合わせた、より詳細な推論の解釈を可能にします。</p>
<img src="/images/20191227/ProtoPNet.png">

<p>[4]のFigure2より引用</p>
<p>NeurIPS最終日に行われたMedical Imaging meets NeurIPSというワークショップでは、CAMを医療画像へ適用した事例がポスター発表でありました。</p>
<p>HR-CAM [5]では、最後のfeature mapだけではなく、中間層のfeature mapも用いることで、より鮮明に判断根拠を可視化します。</p>
<img src="/images/20191227/HR-CAM.png">
[5]のFigure5より引用

<p>また、初日のEXPOでは、Googleが”Interpretability - Now What?”というタイトルで解釈性に関する発表をしていました。<br>そこでは、Testing with Concept Activation Vectors(TCAV) [6]という手法が紹介されました。<br>TCAVは上記までの手法の流れとは少し異なり、概念的な重要度を抽出する方法を取っています。<br>画像認識の分野に限らず、解釈性に関する研究は近年、注目を高めている分野の一つです。</p>
<ul>
<li>[1] B.Zhou, et al., Learning Deep Features for Discriminative Localization, 2016.</li>
<li>[2] R.R.Selvaraju, et al., Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, 2017.</li>
<li>[3] H.Fukui, et al., Attention Branch Network: Learning of Attention Mechanism for Visual Explanation, 2018.</li>
<li>[4] C.Chen, et al., This Looks Like That: Deep Learning for Interpretable Image Recognition, 2019.</li>
<li>[5] S.Shinde, et al., HR-CAM: Precise Localization of pathology using multi-level learning in CNNs, 2019.</li>
<li>[6] B.Kim, et al., Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV), 2018.</li>
</ul>
<h2 id="生成モデルの評価方法に関する研究"><a href="#生成モデルの評価方法に関する研究" class="headerlink" title="生成モデルの評価方法に関する研究"></a>生成モデルの評価方法に関する研究</h2><p>近年、Generative Adversarial Network(GAN)をはじめとした深層生成モデルは目覚ましい発展を遂げ、本物と見間違える程きれいな画像を生成できるようになってきました。<br>しかし、生成された画像のクオリティを適切に評価することは、それほど簡単なことではありません。<br>よく使われる指標は、Inception Score [7]とFréchet Inception Distance [8] です。</p>
<p>Inception Scoreは、次式で計算されます。<br>p(y|x)は、ImageNetで学習済みのInception Modelで生成された画像を予測したときのラベルの分布、p(y)は予測ラベルの周辺分布であり、それらの分布間の距離をKullback–Leibler divergenceで測っています。</p>
<p>生成される画像が、識別が容易で、かつ、バリエーションが豊富であるほど、スコアが高くなるように設計されています。</p>
<img src="/images/20191227/IS.png" class="img-middle-size">

<p>もう一つのFréchet Inception Distanceでは、実画像と生成画像でのInception Modelから得られる特徴ベクトルの距離を次式で測ります。</p>
<p>m_w, C_wは実画像から得られる特徴ベクトルの平均と共分散行列、m, Cは生成画像から得られる特徴ベクトルの平均と共分散行列であり、それぞれ多変量正規分布に従うと仮定し、Fréchet距離で分布間の距離を測ります。</p>
<img src="/images/20191227/FID.png" class="img-middle-size">

<p>どちらの手法も、画像の「本物らしさ」をどのようにスコアするかや、ImageNetでの学習済みモデルに依存してしまっていることなどが課題としてあげられます。</p>
<p>NeurIPSでは、HYPE [9]というクラウドソーシングを利用して人の目で評価する手法が提案されました。<br>Amazon Mechanical Turkを利用したクラウドソーシングにより、実画像と生成画像の分類を人の目で行います。</p>
<p>論文では、心理物理学に基づいて評価者への画像の提示時間を制御する方法と、コストを抑えるために時間の制限を設けない方法の2つの手法が提案されています。</p>
<p>次図のように、HYPEのスコアによってモデルの善し悪しが判断できるような結果が得られています。</p>
<img src="/images/20191227/HYPE.png">

<p>[9]のFigure1,Figure2より引用</p>
<p>HYPEを試すためには、<a target="_blank" rel="noopener" href="https://hype.stanford.edu/">https://hype.stanford.edu/</a> からAWSのS3の情報を送ると、<code>$60 ~ $100</code> 程度の値段でスコアが得られるようです。<br>NeurIPSは理論よりの研究が多いなかで、少し変わり種の発表に感じました。</p>
<ul>
<li>[7] T.salimans, et al., Improved Techniques for Training GANs, 2016.</li>
<li>[8] M.Heusel, et al., GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium, 2017.</li>
<li>[9] S.Zhou, et al., HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models, 2019.</li>
</ul>
<h1 id="言語・認知理解"><a href="#言語・認知理解" class="headerlink" title="言語・認知理解"></a>言語・認知理解</h1><p>続いて田中から言語理解や認知機能に関する研究を紹介します。</p>
<h2 id="SuperGLUE-A-Stickier-Benchmark-for-General-Purpose-Language-Understanding-Systems"><a href="#SuperGLUE-A-Stickier-Benchmark-for-General-Purpose-Language-Understanding-Systems" class="headerlink" title="SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"></a>SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</h2><ul>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems">https://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems</a></li>
</ul>
<p>Alex Wang(※1), Yada Pruksachatkun(※1), Nikita Nangia(※1), Amanpreet Singh(※2), Julian Michael(※3), Felix Hill(※4), Omer Levy(※2), Samuel R. Bowman(※1)</p>
<p>※1: New York University<br>※2: Facebook AI Research<br>※3: University of Washington<br>※4: DeepMind</p>
<p>GLUEを置き換える、言語理解タスク・転移学習のベンチマークに関する研究です。<br>GLUEベンチマークでは、システムの評価結果がヒトの評価結果を超えましたが、依然としてシステムの評価を行うために頑健な、単一の評価基準が必要です。<br>そこで、多くの学習データ/ジャンル/難易度をカバーした8つの言語理解タスク用ベンチマーク、SuperGLUEを提案しました。<br>新たな評価タスクとして、coreference resolutionとQAを追加し、トレーニングデータが比較的少ないタスクに重点を置いた設計になっています。リーダーボードや、詳細な分析を行うためのデータセットはGLUE同様に提供されています。</p>
<p>SuperGLUEは<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems">こちら</a>のリンクから使用できます。</p>
<h2 id="From-voxels-to-pixels-and-back-Self-supervision-in-natural-image-reconstruction-from-fMRI"><a href="#From-voxels-to-pixels-and-back-Self-supervision-in-natural-image-reconstruction-from-fMRI" class="headerlink" title="From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI"></a>From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI</h2><ul>
<li><a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/8879-from-voxels-to-pixels-and-back-self-supervision-in-natural-image-reconstruction-from-fmri">http://papers.nips.cc/paper/8879-from-voxels-to-pixels-and-back-self-supervision-in-natural-image-reconstruction-from-fmri</a></li>
</ul>
<p>Roman Beliy(※1), Guy Gaziv(※1), Assaf Hoogi(※1), Francesca Strappini(※1), Tal Golan(※2), Michal Irani(※1)</p>
<p>※1: The Weizmann Institute of Science<br>※2: Columbia University</p>
<p>fMRIからのNatural Image Reconstructionタスク(ヒトが何かしらの画像を思い浮かべている/見ている際にMRIで脳の活動を記録、脳のMRIデータから、思い浮かべていた/見ていた画像を再構築するタスク)において、fMRIデータと正解の画像ペアのデータ数が少なく、学習が十分にできない問題があり、単純に教師あり学習を行っても十分な精度がでない課題があります。<br>そこで、コーパス外の50000件の画像データと、正解ラベルの付いていないテスト用fMRIデータをそれぞれ用いて、事前にencoder-decoderを学習する方法を提案しました。結果として、state-of-the-art、もしくはそれに匹敵する精度を達成しました。</p>
<h1 id="最後に"><a href="#最後に" class="headerlink" title="最後に"></a>最後に</h1><p>NeurIPSは、EXPOからWorkshopまで入れると1週間ほどありました。<br>そこでは、多くの研究発表があり、ここでは取り上げきれないほど、おもしろい研究がたくさんありました。</p>
<p>みなさんもぜひ、興味のある分野を調べてみてください。</p>

      
    </div>
    <div class="social-area">
    <!-- シェアボタン START -->
  <ul class="social-button">
    
    <!-- Twitter -->
    <li class="twitter-btn">
      <a target="_blank" href="https://twitter.com/share?url=https://future-architect.github.io/articles/20191227/&via=future_techblog&related=twitterapi%2Ctwitter&text=NeurIPS%202019%20%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B"
        onclick="window.open(this.href, 'tweetwindow', 'width=650, height=470, personalbar=0, toolbar=0, scrollbars=1, sizable=1'); return false;" rel="nofollow noopener">
        <i></i><span class="tw-btn-label">ツイート</span>
      </a>
    </li>
    <!-- Facebook -->
    <li class="social-button-fb" style="width:120px; height:20px">
     <div class="fb-like" data-href="https://future-architect.github.io/articles/20191227/" data-layout="button_count" data-action="like" data-size="small" data-show-faces="true" data-share="true"></div>
    </li>
    <!-- はてなブックマーク -->
    <li class="social-button-hatebu" style="width:155px; height:20px">
       <a target="_blank" rel="noopener" href="http://b.hatena.ne.jp/entry/https://future-architect.github.io/articles/20191227/" class="hatena-bookmark-button" data-hatena-bookmark-layout="standard-balloon" data-hatena-bookmark-lang="ja" title="はてなブックマークに追加">はてブする</a>
    </li>
    <!-- pocket -->
    <li class="social-button-pocket">
      <a data-pocket-label="pocket" data-pocket-count="horizontal" class="pocket-btn" data-lang="en" data-save-url="https://future-architect.github.io/articles/20191227/"></a>
    </li>
  </ul>
<!-- シェアボタン END -->

    </div>
  </div>
  </article>
</div>
<div class="col-md-3 col-sm-3 blog-sidebar">
  <!-- BEGIN FEATURED POSTS -->
<h3>注目の記事</h3>
<div class="widget-wrap">
  
  <div class="widget">
    <ul class="nav">
      
    <li>2020.03.11<a href="/articles/20200311/" title="Java to Go in-depth tutorialの日本語訳です。原文の著者に許諾を得て翻訳・公開いたします。このチュートリアルは、JavaプログラマーがすばやくGo言語にキャッチアップできるようにすることを目的としています。">JavaプログラマーのためのGo言語入門</a></li>

    <li>2021.04.22<a href="/articles/20210422a/" title="私は新卒でフューチャーへ入社しITの世界でのキャリアが始まりました。その後、一度フューチャーを離れIoTプラットフォーマーのソリューションアーキテクトとして多くのお客様へ自社サービスの導入支援やIoTシステムの設計や構築の支援をしておりました。そうしたソリューションアーキテクトとしてのロールを通じて、多くの経験や学びを得ることができたのですが、その中でも特にTechnical Credibilityというキーワードについて自分の経験を交えて本ブログにて書いてみたいと思います。">Technical Credibilityを築くということ</a></li>

    <li>2021.04.10<a href="/articles/20210410/" title="参加しているプロジェクトで、OpenAPI定義ファイルからモックサーバを建てることができるOSSツール「Prism」を導入することになりました。この記事では、Prism導入の手順や、躓いた点などを紹介します。">OpenAPIからモックサーバを建てられるPrismを実際のプロジェクトに導入してみた</a></li>

    <li>2021.04.06<a href="/articles/20210406/" title="シェルスクリプトで前提とするシェルは、大抵のコンピューターにインストールされていることが多いbashを選択することが多いと思います。当記事ではそのbashを対象に、意外と色々あるシェルの展開処理の概要をまとめました。">Bashのシェル展開</a></li>

    <li>2021.03.24<a href="/articles/20210324/" title="TIGの伊藤真彦です。業務で行っている開発がいよいよリリースを視野に入れたフェーズに入り、E2Eテストや各種性能試験を行いました。">k6の使い方 シンプル&軽快な負荷試験ツールを試す</a></li>

    <li>2020.11.09<a href="/articles/20201109/" title="1年弱ほどGo言語でWebAPIアプリケーション開発を行っていますが、かなり割り切った構成・テスト方針を採用しました。そろそろ1年弱になり機能開発も比較的落ち着き、保守運用フェーズの割合も徐々に増えてきた頃合いなので、やったこと・学び・反省といった振り返りを共有します。">GoのWebアプリ開発でフラットパッケージにした話</a></li>

    <li>2020.11.07<a href="/articles/20201107/" title="フューチャーOSS推進タスクフォースを開始しました。実は7月に発足してから8月には活動の骨子が固まり、タスクフォース自体は始動していましたが、中身を作ってから告知した方が具体性があってブレも無くなり良いのでは？という話があり、少しタイミングをずらしてのご報告です。">フューチャーOSS推進タスクフォース始めます</a></li>

    <li>2020.08.07<a href="/articles/20200807/" title="Go は標準ライブラリが充実しているとよく言われます。標準ライブラリだけで、HTTP サーバを作れたり、暗号化処理や、JSON や CSV といったデータ形式を扱うことができます">GoとSuffixArray</a></li>

    <li>2020.05.12<a href="/articles/20200512/" title="TIG DXチーム　アルバイターの三村です．今回はHeadlessCMSを利用したタスクをやることになりました．私自身HeadlessCMSどころかCMSを聞いたことすら無かったので，初めてCMS使ってみるにあたって感じたことを共有出来たらと思い本記事を作成させていただいております．">日本製HeadlessCMSのmicroCMSを触ってみた</a></li>

    <li>2020.02.07<a href="/articles/20200207/" title="Go + Vue + Cloud Runでかんたんな管理画面を作ろうと思います。ストレージ側にもサーバーレスがあります。MySQLやPostgreSQLのクラウドサービス（Cloud SQLとかRDS）は、サーバーマシンを可動させて、その上にDBMSが稼働しますので、起動している時間だけお金がかかってしまします。一方、FirestoreやDynamoDBの場合は容量と通信（と、キャパシティユニット）にしかお金がかからないモデルになっており、サーバーレスです。今回はかんたん化のためにストレージは扱いません。">GCP連載#3 Goでサーバーレスな管理画面アプリを作る</a></li>
    </ul>
  </div>
  
</div>
<!-- END FEATURED POSTS -->

<!-- CATEGORIES START -->
<h3 class="margin-top-30">カテゴリー</h3>

<div class="widget-wrap">
  <div class="widget">
    <ul class="nav sidebar-categories margin-bottom-40">
    
    <li><a href="/categories/Culture/">Culture (52)</a></li>
    
    <li><a href="/categories/IoT/">IoT (14)</a></li>
    
    <li><a href="/categories/VR/">VR (10)</a></li>
    
    <li><a href="/categories/Security/">Security (2)</a></li>
    
    <li><a href="/categories/Design/">Design (7)</a></li>
    
    <li><a href="/categories/Programming/">Programming (166)</a></li>
    
    <li><a href="/categories/Management/">Management (9)</a></li>
    
    <li><a href="/categories/Infrastructure/">Infrastructure (131)</a></li>
    
    <li><a href="/categories/DB/">DB (14)</a></li>
    
    <li><a href="/categories/DataScience/">DataScience (30)</a></li>
    
    <li><a href="/categories/CI-CD/">CI/CD (4)</a></li>
    
    <li><a href="/categories/%E8%AA%8D%E8%A8%BC%E8%AA%8D%E5%8F%AF/">認証認可 (6)</a></li>
    
    </ul>
  </div>
</div>


<!-- CATEGORIES END -->

<!-- START Blog -->
<div class="margin-bottom-20">
  <h3>リンク</h3>
  <div class="widget-wrap">
  <div class="widget">
    <ul class="nav">
      <li><a href="https://note.future.co.jp/" title="フューチャーの公式note" target="_blank" rel="noopener">未来報</a></li>
      <li><a href="https://future-fintech.github.io/" title="Future Fintech EyE - 金融の未来を語るブログ" target="_blank" rel="noopener">Future Fintech EYE</a></li>
      <li><a href="https://future.connpass.com/" title="経営とITをデザインするフューチャーの勉強会です" target="_blank" rel="noopener">勉強会Connpass</a></li>
    </ul>
  </div>
</div>

</div>
<!-- END Blog -->


<!-- START ADVENT CALENDAR -->
<div class="margin-bottom-20">
  <h3>アドベントカレンダー</h3>
  <div class="widget-wrap">
  <div class="widget">
    <ul class="nav">
      <li><a href="http://qiita.com/advent-calendar/2020/future" title="フューチャー Advent Calendar 2020 #Qiita" target="_blank" rel="noopener">2020年</a></li>
      <li><a href="http://qiita.com/advent-calendar/2019/future" title="フューチャー Advent Calendar 2019 #Qiita" target="_blank" rel="noopener">2019年</a></li>
      <li><a href="http://qiita.com/advent-calendar/2018/future" title="フューチャー Advent Calendar 2018 #Qiita" target="_blank" rel="noopener">2018年</a></li>
      <li><a href="http://qiita.com/advent-calendar/2017/future" title="フューチャー Advent Calendar 2017 #Qiita" target="_blank" rel="noopener">2017年</a></li>
      <li><a href="http://qiita.com/advent-calendar/2016/future" title="フューチャー Advent Calendar 2016 #Qiita" target="_blank" rel="noopener">2016年</a></li>
      <li><a href="http://qiita.com/advent-calendar/2015/future" title="フューチャー Advent Calendar 2015 #Qiita" target="_blank" rel="noopener">2015年</a></li>
    </ul>
  </div>
</div>

</div>
<!-- END ADVENT CALENDAR -->

</div>
</div>

  </section>
</div>

    <!-- BEGIN PRE-FOOTER -->
    <div class="pre-footer">
      <div class="container">
        <div class="row">
          <!-- BEGIN BOTTOM ABOUT BLOCK -->
          <div class="col-md-4 col-sm-6 pre-footer-col">
            <h2>About Us</h2>
            <p>経営とITをデザインする、フューチャーの技術ブログです。業務で利用している幅広い技術について紹介します。記事についてのお問い合わせはTwitterのDMで連絡いただけると幸いです。<br /><br /><a target="_blank" rel="noopener" href="http://www.future.co.jp/">http://www.future.co.jp/</a></p>
          </div>
          <!-- END BOTTOM ABOUT BLOCK -->

          <!-- BEGIN BOTTOM CONTACTS -->
          <div class="col-md-4 col-sm-6 pre-footer-col">
            <h2>Contact</h2>
            <address class="margin-bottom-40">
              東京都品川区大崎1-2-2<br>
              アートヴィレッジ大崎セントラルタワー<br><br>
              Email: <a href="mailto:techblog@future.co.jp">techblog@future.co.jp</a><br>
            </address>
          </div>
          <!-- END BOTTOM CONTACTS -->

  <!-- FacebookのShareボタン用. ページに1度初期化できればよいのでフッターに配備 -->
  <div id="fb-root"></div>
  <script>(function(d, s, id) {
      var js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return;
      js = d.createElement(s); js.id = id;
      js.src = "https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.0";
      fjs.parentNode.insertBefore(js, fjs);
    }(document, 'script', 'facebook-jssdk'));</script>

  <!-- Pocket Shareボタン. ページ毎に1度初期化できれば良いのでフッターに配備 -->
  <script>!function(d,i){if(!d.getElementById(i)){var j=d.createElement("script");j.id=i;j.src="https://widgets.getpocket.com/v1/j/btn.js?v=1";var w=d.getElementById(i);d.body.appendChild(j);}}(document,"pocket-btn-js");</script>
  <!-- Twitter Shareボタン. ページ毎に1度初期化できれば良いのでフッターに配備 -->
  <script>window.twttr = (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0], t = window.twttr || {};
    if (d.getElementById(id)) return t;
    js = d.createElement(s);
    js.id = id;
    js.src = "https://platform.twitter.com/widgets.js";
    fjs.parentNode.insertBefore(js, fjs);

    t._e = [];
    t.ready = function(f) {
      t._e.push(f);
    };
    return t;
  }(document, "script", "twitter-wjs"));</script>

  <!-- はてぶ Shareボタン. ページ毎に1度初期化できれば良いのでフッターに配備 -->
  <script src="https://b.st-hatena.com/js/bookmark_button.js" async="async"></script>

	
    <!-- BEGIN TWITTER BLOCK -->
    <div class="col-md-4 col-sm-6 pre-footer-col">
      <!-- Twitterフォローボタン -->
      <a target="_blank" rel="noopener" href="https://twitter.com/future_techblog" class="twitter-follow-button" data-show-count="false" data-lang="ja">/future_techblogさんをフォロー</a>
      <!-- 組み込みTwitterウィジェット -->
      <a data-tweet-limit="1" class="twitter-timeline" target="_blank" rel="noopener" href="https://twitter.com/future_techblog?ref_src=twsrc%5Etfw" data-lang="ja" data-dnt="true">Tweets by @future_techblog</a>
    </div>
    <!-- END TWITTER BLOCK -->
	
        </div>
      </div>
    </div>
    <!-- END PRE-FOOTER -->

    <!-- BEGIN FOOTER -->
    <div class="footer">
      <div class="container">
        <div class="row">
          <!-- BEGIN COPYRIGHT -->
          <div class="col-md-6 col-sm-6 padding-top-10">
                  &copy; 2021 フューチャー技術ブログ<br>
 <!-- <a href="javascript:;">Privacy Policy</a> | <a href="javascript:;">Terms of Service</a> -->
          </div>
          <!-- END COPYRIGHT -->
	  <!-- BEGIN SOCIAL -->
<div class="col-md-6 col-sm-6">
  <ul class="social-footer list-unstyled list-inline pull-right">
    
      <li><a target="_blank" rel="noopener" href="https://github.com/future-architect"><i class="fa fa-github"></i></a></li>
  
      <li><a target="_blank" rel="noopener" href="https://twitter.com/future_techblog"><i class="fa fa-twitter"></i></a></li>
  
      <li><a href="/atom.xml"><i class="fa fa-rss"></i></a></li>
  
  </ul>
</div>
<!-- END SOCIAL -->

        </div>
      </div>
    </div>
    <!-- END FOOTER -->

  <script src="/metronic/assets/plugins/jquery.min.js"></script>
<script src="/metronic/assets/plugins/bootstrap/js/bootstrap.min.js"></script>
<!-- START INTEGRATIONS -->

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-74047147-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


<!-- END INTEGRATIONS -->

</div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
