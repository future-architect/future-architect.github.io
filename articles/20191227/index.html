<!DOCTYPE html>
<!--[if IE 8]> <html lang="ja" class="ie8 no-js"> <![endif]-->
<!--[if IE 9]> <html lang="ja" class="ie9 no-js"> <![endif]-->
<!--[if !IE]><!-->
<html lang="ja">
<!--<![endif]-->
<head>
  <meta charset="utf-8">
  <!--
    ███████╗██╗░░░██╗████████╗██╗░░░██╗██████╗░███████╗
    ██╔════╝██║░░░██║╚══██╔══╝██║░░░██║██╔══██╗██╔════╝
    █████╗░░██║░░░██║░░░██║░░░██║░░░██║██████╔╝█████╗░░
    ██╔══╝░░██║░░░██║░░░██║░░░██║░░░██║██╔══██╗██╔══╝░░
    ██║░░░░░╚██████╔╝░░░██║░░░╚██████╔╝██║░░██║███████╗
    ╚═╝░░░░░░╚═════╝░░░░╚═╝░░░░╚═════╝░╚═╝░░╚═╝╚══════╝
    ████████╗███████╗░█████╗░██╗░░██╗
    ╚══██╔══╝██╔════╝██╔══██╗██║░░██║
    ░░░██║░░░█████╗░░██║░░╚═╝███████║
    ░░░██║░░░██╔══╝░░██║░░██╗██╔══██║
    ░░░██║░░░███████╗╚█████╔╝██║░░██║
    ░░░╚═╝░░░╚══════╝░╚════╝░╚═╝░░╚═╝
    ██████╗░██╗░░░░░░█████╗░░██████╗░
    ██╔══██╗██║░░░░░██╔══██╗██╔════╝░
    ██████╦╝██║░░░░░██║░░██║██║░░██╗░
    ██╔══██╗██║░░░░░██║░░██║██║░░╚██╗
    ██████╦╝███████╗╚█████╔╝╚██████╔╝
    ╚═════╝░╚══════╝░╚════╝░░╚═════╝░
    Welcome engineer.
    https://www.future.co.jp/recruit/
  -->
  
  <title>NeurIPS 2019 論文紹介 | フューチャー技術ブログ</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  
  <meta name="description" content="こんにちは、Strategic AI Group(SAIG)の田中、上野です。 少し前にNeurIPSという学会に参加して来たことをご報告しましたが、今回はNeurIPSで気になった論文をいくつか紹介したいと思います。 画像認識・生成まずは、上野からは画像認識・生成に関する下記2つの研究を取り上げます。 This Looks Like That: Deep Learning for Interpr">
<meta property="og:type" content="article">
<meta property="og:title" content="NeurIPS 2019 論文紹介 | フューチャー技術ブログ">
<meta property="og:url" content="https://future-architect.github.io/articles/20191227/index.html">
<meta property="og:site_name" content="フューチャー技術ブログ">
<meta property="og:description" content="こんにちは、Strategic AI Group(SAIG)の田中、上野です。 少し前にNeurIPSという学会に参加して来たことをご報告しましたが、今回はNeurIPSで気になった論文をいくつか紹介したいと思います。 画像認識・生成まずは、上野からは画像認識・生成に関する下記2つの研究を取り上げます。 This Looks Like That: Deep Learning for Interpr">
<meta property="og:locale" content="ja_JP">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/CAM.png">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/ProtoPNet.png">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/HR-CAM.png">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/IS.png">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/FID.png">
<meta property="og:image" content="https://future-architect.github.io/images/20191227/HYPE.png">
<meta property="article:published_time" content="2019-12-27T07:22:02.000Z">
<meta property="article:modified_time" content="2021-05-25T12:53:10.811Z">
<meta property="article:tag" content="機械学習">
<meta property="article:tag" content="論文紹介">
<meta property="article:tag" content="NeurIPS">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://future-architect.github.io/images/20191227/CAM.png">
  
  <link rel="alternate" href="/atom.xml" title="フューチャー技術ブログ" type="application/atom+xml">
  
  
  <link rel="icon" href="/favicon.ico">
  
  <link rel="canonical" href="https://future-architect.github.io/articles/20191227/">
  <meta content="機械学習,論文紹介,NeurIPS" name="keywords">
  <meta content="上野貴史" name="author">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" integrity="sha384-pdapHxIh7EYuwy6K7iE41uXVxGCXY0sAjBzaElYGJUrzwodck3Lx6IE2lA0rFREo" crossorigin="anonymous">
  <link rel="stylesheet" href="/metronic/assets/style.css">
  <link rel="stylesheet" href="/css/theme-styles.css">
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="corporate">
  <div class="wrap" itemscope itemtype="https://schema.org/TechArticle">
  <!-- BEGIN HEADER -->
<header class="header">
	<div class="header-overlay">
		<div class="header-menu"></div>
		<div class="header-title"><a href="/">Future Tech Blog</a></div>
		<div class="header-title-sub">フューチャー技術ブログ</div>
	</div>
</header>
<!-- Header END -->

  <div class="container">
  <ul class="breadcrumb">
    <li><a href="/">Home</a></li>
    <li><a href="/articles/">Blog</a></li>
    <li class="active">Post</li>
  </ul>
  <section id="main">
    
  <h2 itemprop="name">
    <a class="article-title" href="/articles/20191227/">NeurIPS 2019 論文紹介</a>
  </h2>


    <div class="row">
  <main class="col-md-9 col-sm-9 blog-posts">
    <article id="post-20191227-neurips" class="article article-type-post blog-item" itemscope itemprop="blogPost">
      <div class="article-inner">
        
        <header class="article-header">
          <ul class="blog-info">
            <li class="blog-info-item"><li><a href="/authors/%E4%B8%8A%E9%87%8E%E8%B2%B4%E5%8F%B2">上野貴史</a></li></li>
            <li class="blog-info-item"><a href="/articles/2019/"><time datetime="2019-12-27T07:22:02.000Z" itemprop="datePublished">2019.12.27</time></a>
</li>
            <li class="blog-info-item">
  <ul>
  
    <li><a href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/" title="機械学習">機械学習</a></li> 
  
    <li><a href="/tags/%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/" title="論文紹介">論文紹介</a></li> 
  
    <li><a href="/tags/NeurIPS/" title="NeurIPS">NeurIPS</a>
  
  </ul>

</li>
          </ul>
          
  <div class="article-category">
    
    Category:
    
    <a class="article-category-link" href="/categories/DataScience/">DataScience</a>
  </div>


        </header>
        
        <div class="article-entry" itemprop="articleBody">
          
            <p>こんにちは、Strategic AI Group(SAIG)の田中、上野です。</p>
<p>少し前に<a href="/articles/20191210/">NeurIPSという学会に参加して来たことをご報告</a>しましたが、今回はNeurIPSで気になった論文をいくつか紹介したいと思います。</p>
<h1 id="画像認識・生成"><a href="#画像認識・生成" class="headerlink" title="画像認識・生成"></a>画像認識・生成</h1><p>まずは、上野からは画像認識・生成に関する下記2つの研究を取り上げます。</p>
<h2 id="This-Looks-Like-That-Deep-Learning-for-Interpretable-Image-Recognition"><a href="#This-Looks-Like-That-Deep-Learning-for-Interpretable-Image-Recognition" class="headerlink" title="This Looks Like That: Deep Learning for Interpretable Image Recognition"></a>This Looks Like That: Deep Learning for Interpretable Image Recognition</h2><ul>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/9095-this-looks-like-that-deep-learning-for-interpretable-image-recognition">https://papers.nips.cc/paper/9095-this-looks-like-that-deep-learning-for-interpretable-image-recognition</a></li>
</ul>
<p>Chaofan Chen(※1), Oscar Li(※1), Daniel Tao(※1), Alina Barnett(※1), Cynthia Rudin(※1), Jonathan K. Su(※2)</p>
<p>※1: Duku University<br>※2: MIT Lincoln Laboratory</p>
<h2 id="HYPE-A-Benchmark-for-Human-eYe-Perceptual-Evaluation-of-Generative-Models"><a href="#HYPE-A-Benchmark-for-Human-eYe-Perceptual-Evaluation-of-Generative-Models" class="headerlink" title="HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models"></a>HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models</h2><ul>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/8605-hype-a-benchmark-for-human-eye-perceptual-evaluation-of-generative-models">https://papers.nips.cc/paper/8605-hype-a-benchmark-for-human-eye-perceptual-evaluation-of-generative-models</a></li>
</ul>
<p>Sharon Zhou(※1), Mitchell Gordon(※1), Ranjay Krishna(※1), Austin Narcomey(※1), Li F. Fei-Fei(※1), Michael Bernstein(※1)</p>
<p>※1: Stanford University</p>
<p>一つ目はCNNの解釈性に関する研究で、二つ目は生成モデルの評価方法に関する研究です。<br>それぞれ、関連する研究を取り上げながら紹介します。</p>
<h2 id="CNNの解釈性に関する研究"><a href="#CNNの解釈性に関する研究" class="headerlink" title="CNNの解釈性に関する研究"></a>CNNの解釈性に関する研究</h2><p>Deep Learningの威力を世に知らしめた出来事の一つは、2012年のILSVRCという画像認識に関するコンペティションでした。2012年以降、画像認識の精度がどんどん向上し、ついには人間のレベルに匹敵するまでになりました。</p>
<p>画像認識では、Convolutional Neural Network(CNN)と呼ばれる技術が用いられますが、非線形な演算を何層にも渡って繰り返すため、CNNが画像のどこに着目して分類をしているかといった解釈が非常に難しくなってしまいます。</p>
<p>CNNの着目領域を可視化した初期の研究が、Class Activation Mapping(CAM) [1]です。</p>
<p>CAMでは、下図のようにGlobal Average Poolingをする直前のfeature mapに分類層の結合重みを使った線形和によって、CNNの判断根拠を可視化します。</p>
<img src="/images/20191227/CAM.png" loading="lazy">

<p>[1]のFigure2より引用</p>
<p>CAMは、conv feature maps → global average pooling → softmax layer という構成である必要がありましたが、Grad-CAM [2]では、勾配を用いてfeature mapの重み付けをすることで、ネットワーク構成の制約がなくなりました。</p>
<p>また、Attention Branch Network [3]では、Activation mapをAttentionに使う方法が提案されています。<br>Attention Branch Networkは、中部大学の研究グループが提案したこともあり、今夏に参加した日本の学会MIRUではよく見かけました。</p>
<p>NeurIPSでは、porototypical part network(ProtoPNet) [4]という手法が提案されました。ProtoPNetでは、Prototype layerによって、入力画像中のどの領域が、学習データのどの部分と類似しているかまでを判断することができます。</p>
<p>その結果、単に着目領域が可視化されるだけではなく、画像の部分ごとの判断根拠を組み合わせた、より詳細な推論の解釈を可能にします。</p>
<img src="/images/20191227/ProtoPNet.png" loading="lazy">

<p>[4]のFigure2より引用</p>
<p>NeurIPS最終日に行われたMedical Imaging meets NeurIPSというワークショップでは、CAMを医療画像へ適用した事例がポスター発表でありました。</p>
<p>HR-CAM [5]では、最後のfeature mapだけではなく、中間層のfeature mapも用いることで、より鮮明に判断根拠を可視化します。</p>
<img src="/images/20191227/HR-CAM.png" loading="lazy">
[5]のFigure5より引用

<p>また、初日のEXPOでは、Googleが”Interpretability - Now What?”というタイトルで解釈性に関する発表をしていました。<br>そこでは、Testing with Concept Activation Vectors(TCAV) [6]という手法が紹介されました。<br>TCAVは上記までの手法の流れとは少し異なり、概念的な重要度を抽出する方法を取っています。<br>画像認識の分野に限らず、解釈性に関する研究は近年、注目を高めている分野の一つです。</p>
<ul>
<li>[1] B.Zhou, et al., Learning Deep Features for Discriminative Localization, 2016.</li>
<li>[2] R.R.Selvaraju, et al., Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, 2017.</li>
<li>[3] H.Fukui, et al., Attention Branch Network: Learning of Attention Mechanism for Visual Explanation, 2018.</li>
<li>[4] C.Chen, et al., This Looks Like That: Deep Learning for Interpretable Image Recognition, 2019.</li>
<li>[5] S.Shinde, et al., HR-CAM: Precise Localization of pathology using multi-level learning in CNNs, 2019.</li>
<li>[6] B.Kim, et al., Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV), 2018.</li>
</ul>
<h2 id="生成モデルの評価方法に関する研究"><a href="#生成モデルの評価方法に関する研究" class="headerlink" title="生成モデルの評価方法に関する研究"></a>生成モデルの評価方法に関する研究</h2><p>近年、Generative Adversarial Network(GAN)をはじめとした深層生成モデルは目覚ましい発展を遂げ、本物と見間違える程きれいな画像を生成できるようになってきました。<br>しかし、生成された画像のクオリティを適切に評価することは、それほど簡単なことではありません。<br>よく使われる指標は、Inception Score [7]とFréchet Inception Distance [8] です。</p>
<p>Inception Scoreは、次式で計算されます。<br>p(y|x)は、ImageNetで学習済みのInception Modelで生成された画像を予測したときのラベルの分布、p(y)は予測ラベルの周辺分布であり、それらの分布間の距離をKullback–Leibler divergenceで測っています。</p>
<p>生成される画像が、識別が容易で、かつ、バリエーションが豊富であるほど、スコアが高くなるように設計されています。</p>
<img src="/images/20191227/IS.png" class="img-middle-size" loading="lazy">

<p>もう一つのFréchet Inception Distanceでは、実画像と生成画像でのInception Modelから得られる特徴ベクトルの距離を次式で測ります。</p>
<p>m_w, C_wは実画像から得られる特徴ベクトルの平均と共分散行列、m, Cは生成画像から得られる特徴ベクトルの平均と共分散行列であり、それぞれ多変量正規分布に従うと仮定し、Fréchet距離で分布間の距離を測ります。</p>
<img src="/images/20191227/FID.png" class="img-middle-size" loading="lazy">

<p>どちらの手法も、画像の「本物らしさ」をどのようにスコアするかや、ImageNetでの学習済みモデルに依存してしまっていることなどが課題としてあげられます。</p>
<p>NeurIPSでは、HYPE [9]というクラウドソーシングを利用して人の目で評価する手法が提案されました。<br>Amazon Mechanical Turkを利用したクラウドソーシングにより、実画像と生成画像の分類を人の目で行います。</p>
<p>論文では、心理物理学に基づいて評価者への画像の提示時間を制御する方法と、コストを抑えるために時間の制限を設けない方法の2つの手法が提案されています。</p>
<p>次図のように、HYPEのスコアによってモデルの善し悪しが判断できるような結果が得られています。</p>
<img src="/images/20191227/HYPE.png" loading="lazy">

<p>[9]のFigure1,Figure2より引用</p>
<p>HYPEを試すためには、<a target="_blank" rel="noopener" href="https://hype.stanford.edu/">https://hype.stanford.edu/</a> からAWSのS3の情報を送ると、<code>$60 ~ $100</code> 程度の値段でスコアが得られるようです。<br>NeurIPSは理論よりの研究が多いなかで、少し変わり種の発表に感じました。</p>
<ul>
<li>[7] T.salimans, et al., Improved Techniques for Training GANs, 2016.</li>
<li>[8] M.Heusel, et al., GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium, 2017.</li>
<li>[9] S.Zhou, et al., HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models, 2019.</li>
</ul>
<h1 id="言語・認知理解"><a href="#言語・認知理解" class="headerlink" title="言語・認知理解"></a>言語・認知理解</h1><p>続いて田中から言語理解や認知機能に関する研究を紹介します。</p>
<h2 id="SuperGLUE-A-Stickier-Benchmark-for-General-Purpose-Language-Understanding-Systems"><a href="#SuperGLUE-A-Stickier-Benchmark-for-General-Purpose-Language-Understanding-Systems" class="headerlink" title="SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"></a>SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</h2><ul>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems">https://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems</a></li>
</ul>
<p>Alex Wang(※1), Yada Pruksachatkun(※1), Nikita Nangia(※1), Amanpreet Singh(※2), Julian Michael(※3), Felix Hill(※4), Omer Levy(※2), Samuel R. Bowman(※1)</p>
<p>※1: New York University<br>※2: Facebook AI Research<br>※3: University of Washington<br>※4: DeepMind</p>
<p>GLUEを置き換える、言語理解タスク・転移学習のベンチマークに関する研究です。<br>GLUEベンチマークでは、システムの評価結果がヒトの評価結果を超えましたが、依然としてシステムの評価を行うために頑健な、単一の評価基準が必要です。<br>そこで、多くの学習データ/ジャンル/難易度をカバーした8つの言語理解タスク用ベンチマーク、SuperGLUEを提案しました。<br>新たな評価タスクとして、coreference resolutionとQAを追加し、トレーニングデータが比較的少ないタスクに重点を置いた設計になっています。リーダーボードや、詳細な分析を行うためのデータセットはGLUE同様に提供されています。</p>
<p>SuperGLUEは<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems">こちら</a>のリンクから使用できます。</p>
<h2 id="From-voxels-to-pixels-and-back-Self-supervision-in-natural-image-reconstruction-from-fMRI"><a href="#From-voxels-to-pixels-and-back-Self-supervision-in-natural-image-reconstruction-from-fMRI" class="headerlink" title="From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI"></a>From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI</h2><ul>
<li><a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/8879-from-voxels-to-pixels-and-back-self-supervision-in-natural-image-reconstruction-from-fmri">http://papers.nips.cc/paper/8879-from-voxels-to-pixels-and-back-self-supervision-in-natural-image-reconstruction-from-fmri</a></li>
</ul>
<p>Roman Beliy(※1), Guy Gaziv(※1), Assaf Hoogi(※1), Francesca Strappini(※1), Tal Golan(※2), Michal Irani(※1)</p>
<p>※1: The Weizmann Institute of Science<br>※2: Columbia University</p>
<p>fMRIからのNatural Image Reconstructionタスク(ヒトが何かしらの画像を思い浮かべている/見ている際にMRIで脳の活動を記録、脳のMRIデータから、思い浮かべていた/見ていた画像を再構築するタスク)において、fMRIデータと正解の画像ペアのデータ数が少なく、学習が十分にできない問題があり、単純に教師あり学習を行っても十分な精度がでない課題があります。<br>そこで、コーパス外の50000件の画像データと、正解ラベルの付いていないテスト用fMRIデータをそれぞれ用いて、事前にencoder-decoderを学習する方法を提案しました。結果として、state-of-the-art、もしくはそれに匹敵する精度を達成しました。</p>
<h1 id="最後に"><a href="#最後に" class="headerlink" title="最後に"></a>最後に</h1><p>NeurIPSは、EXPOからWorkshopまで入れると1週間ほどありました。<br>そこでは、多くの研究発表があり、ここでは取り上げきれないほど、おもしろい研究がたくさんありました。</p>
<p>みなさんもぜひ、興味のある分野を調べてみてください。</p>

          
        </div>
        <footer>
          <section class="social-area">
          <!-- シェアボタン START -->
  <ul class="social-button">
    
    <!-- Twitter -->
    <li>
      <a class="social-btn twitter-btn" target="_blank" href="https://twitter.com/share?url=https://future-architect.github.io/articles/20191227/&via=future_techblog&related=twitterapi%2Ctwitter&text=NeurIPS%202019%20%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B" rel="nofollow noopener">
        <i></i><span class="social-btn-label tw-btn-label">ツイート</span>
      </a>
    </li>
    <!-- Facebook -->
    <li>
      <a class="social-btn fb-btn" target="_blank" href="http://www.facebook.com/share.php?u=https://future-architect.github.io/articles/20191227/&t=NeurIPS%202019%20%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B" rel="nofollow noopener">
        <i></i><span class="social-btn-label fb-btn-label">シェア</span>
      </a>
    </li>
    <!-- hatebu -->
    <li>
      <a class="social-btn hatebu-btn" target="_blank" href="https://b.hatena.ne.jp/entry/s/future-architect.github.io/articles/20191227/" rel="nofollow noopener">
        <i></i><span class="social-btn-label hatebu-btn-label">はてな</span>
      </a>
    </li>
    <!-- pocket -->
    <li>
      <a class="social-btn pocket-btn" target="_blank" href="https://getpocket.com/save?url=https://future-architect.github.io/articles/20191227/" rel="nofollow noopener">
        <i></i><span class="social-btn-label pocket-btn-label">2</span>
      </a>
    </li>
    
  </ul>
<!-- シェアボタン END -->

          </section>
          <aside>
            <section class="related-post margin-bottom-20 nav">
              <h2>関連記事</h2>
              
  <div class="widget">
    <ul class="nav related-posts"><li class="related-posts-item"><span>2019.12.10</span><span class="snscount">&#9825;29</span><a class="related-posts-link" href=/articles/20191210/ title="こんにちは、Strategic AI Group(SAIG)の田中、上野です。私たちは現在、NeurIPSという学会に参加するためにカナダに来ています。NeurIPS(Conference on Neural Information Processing Systems)は、機械学習分野のトップカンファレンスで、今年が33回目の開催となります。">NeurIPS 2019 参加報告</a></li><li class="related-posts-item"><span>2019.06.27</span><span class="snscount">&#9825;8</span><a class="related-posts-link" href=/articles/20190627/ title="今年で33回目の開催で人工知能 (AI)の研究発表を行う学会で、機械学習から人工知能の応用の話まで幅広く発表があります。フューチャーは2017年からJSAIのプラチナスポンサーとなっており、年もスポンサーブースの出展、インダストリアルセッションでの発表を行ないました。">人工知能学会（JSAI2019） 参加報告</a></li><li class="related-posts-item"><span>2019.10.16</span><span class="snscount">&#9825;3</span><a class="related-posts-link" href=/articles/20191016/ title="2019年は幸運なことに、技術評論社のSoftware Designという雑誌に3回も機械学習関連の記事を執筆する機会を頂きました。本記事では3回分の記事を振り返りながら、機械学習を学ぶ際のちょっとしたTipsを紹介したいと思います。">Software Design 後記</a></li><li class="related-posts-item"><span>2019.06.17</span><span class="snscount">&#9825;191</span><a class="related-posts-link" href=/articles/20190617/ title="アメリカ ニューオーリンズで開催されたICLR(International Conference on Learning Representation)2019の参加報告です">ICLR2019 参加報告ブログ</a></li></ul>
  </div>
            </section>
            <section class="featured-post margin-bottom-20 nav">
              <h2>注目の記事</h2>
              <!-- BEGIN FEATURED POSTS -->
<div class="widget-wrap">
  
  <div class="widget">
    <ul class="nav featured-post-link">
      
    <li><span>2020.03.11</span><span class="snscount">&#9825;526</span> <a href="/articles/20200311/" title="Java to Go in-depth tutorialの日本語訳です。原文の著者に許諾を得て翻訳・公開いたします。このチュートリアルは、JavaプログラマーがすばやくGo言語にキャッチアップできるようにすることを目的としています。">JavaプログラマーのためのGo言語入門</a></li>

    <li><span>2020.03.11</span><span class="snscount">&#9825;526</span> <a href="/articles/20200311/" title="Java to Go in-depth tutorialの日本語訳です。原文の著者に許諾を得て翻訳・公開いたします。このチュートリアルは、JavaプログラマーがすばやくGo言語にキャッチアップできるようにすることを目的としています。">JavaプログラマーのためのGo言語入門</a></li>

    <li><span>2020.07.09</span><span class="snscount">&#9825;652</span> <a href="/articles/20200709/" title="最近Goで主にバックエンドのWebAPIや、AWS Lambdaで動くETLアプリ、たまにCLIツールを開発する時に、2回以上同じ指摘したコメントをまとめてます。Go言語特有ぽいところを中心にしています。レビュイーのスキルセットは..">GoでWebアプリ開発時にあるあるだったレビューコメント</a></li>

    <li><span>2020.06.09</span><span class="snscount">&#9825;139</span> <a href="/articles/20200609/" title="フューチャーではGraphQLの活用事例はまだ少なく、自分はまだお目にかかったことはないです。しかし、HeadlessCMS界隈を初めGraphQLのAPIを提供するサービスが増えてきました。今後もさらに需要が増えてきそうな予感がしたためGoとGraphQLを春の入門祭りのテーマにしました。学習する上でドキュメントを読み込むだけでは忘れがちです。手を動かしながらタイトルにある鉄道データ検索APIを開発していきましょう">作って学ぶGraphQL。gqlgenを用いて鉄道データ検索API開発入門</a></li>
    </ul>
  </div>
  
</div>
<!-- END FEATURED POSTS -->

            </section>
          </aside>
        </footer>
      </div>
    </article>
  </main>
  <aside class="col-md-3 col-sm-3 blog-sidebar">
    <!-- START SIDEBAR  -->

<section class="toc-section">
  <h2 class="margin-top-30">目次</h2>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98%E3%83%BB%E7%94%9F%E6%88%90"><span class="toc-text">画像認識・生成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#This-Looks-Like-That-Deep-Learning-for-Interpretable-Image-Recognition"><span class="toc-text">This Looks Like That: Deep Learning for Interpretable Image Recognition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HYPE-A-Benchmark-for-Human-eYe-Perceptual-Evaluation-of-Generative-Models"><span class="toc-text">HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E3%81%AE%E8%A7%A3%E9%87%88%E6%80%A7%E3%81%AB%E9%96%A2%E3%81%99%E3%82%8B%E7%A0%94%E7%A9%B6"><span class="toc-text">CNNの解釈性に関する研究</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E8%A9%95%E4%BE%A1%E6%96%B9%E6%B3%95%E3%81%AB%E9%96%A2%E3%81%99%E3%82%8B%E7%A0%94%E7%A9%B6"><span class="toc-text">生成モデルの評価方法に関する研究</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A8%80%E8%AA%9E%E3%83%BB%E8%AA%8D%E7%9F%A5%E7%90%86%E8%A7%A3"><span class="toc-text">言語・認知理解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SuperGLUE-A-Stickier-Benchmark-for-General-Purpose-Language-Understanding-Systems"><span class="toc-text">SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#From-voxels-to-pixels-and-back-Self-supervision-in-natural-image-reconstruction-from-fMRI"><span class="toc-text">From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E5%BE%8C%E3%81%AB"><span class="toc-text">最後に</span></a></li></ol>
</section>

<section class="category">
<h2 class="margin-top-30">カテゴリー</h2>

<div class="widget-wrap">
  <div class="widget">
    <ul class="nav sidebar-categories margin-bottom-40">
    
      <li class=""><a href="/categories/Culture/">Culture (52)</a></li>
    
      <li class=""><a href="/categories/IoT/">IoT (15)</a></li>
    
      <li class=""><a href="/categories/VR/">VR (10)</a></li>
    
      <li class=""><a href="/categories/Security/">Security (2)</a></li>
    
      <li class=""><a href="/categories/Design/">Design (8)</a></li>
    
      <li class=""><a href="/categories/Programming/">Programming (180)</a></li>
    
      <li class=""><a href="/categories/Management/">Management (9)</a></li>
    
      <li class=""><a href="/categories/Infrastructure/">Infrastructure (138)</a></li>
    
      <li class=""><a href="/categories/DB/">DB (14)</a></li>
    
      <li class=""><a href="/categories/DataScience/">DataScience (33)</a></li>
    
      <li class=""><a href="/categories/DevOps/">DevOps (4)</a></li>
    
      <li class=""><a href="/categories/%E8%AA%8D%E8%A8%BC%E8%AA%8D%E5%8F%AF/">認証認可 (7)</a></li>
    
      <li class=""><a href="/categories/Business/">Business (4)</a></li>
    
    </ul>
  </div>
</div>


</section>
<section class="other-blog-link">
<h2 class="margin-top-30">リンク</h2>
<div class="widget-wrap">
  <div class="widget">
    <ul class="nav">
      <li><a href="https://anchor.fm/futuretechcast" title="フューチャーグループの技術について深堀りするPodcastです" target="_blank" rel="noopener">ポッドキャスト</a></li>
      <li><a href="https://future.connpass.com/" title="経営とITをデザインするフューチャーの勉強会です" target="_blank" rel="noopener">勉強会Connpass</a></li>
      <li><a href="https://github.com/future-architect" title="Future's official open source repositories" target="_blank" rel="noopener">GitHub</a></li>
      <li><a href="https://qiita.com/organizations/future" title="フューチャーのQiita Organizationです" target="_blank" rel="noopener">Qiita</a></li>
      <li><a href="https://note.future.co.jp/" title="フューチャーの公式note" target="_blank" rel="noopener">未来報</a></li>
      <li><a href="https://future-fintech.github.io/" title="Future Fintech EyE - 金融の未来を語るブログ" target="_blank" rel="noopener">Future Fintech EYE</a></li>
    </ul>
  </div>
</div>

</section>
<section class="advent-calendar">
<h2 class="margin-top-30">アドベントカレンダー</h2>
<div class="widget-wrap">
  <div class="widget">
    <ul class="nav">
      <li><a href="http://qiita.com/advent-calendar/2020/future" title="フューチャー Advent Calendar 2020 #Qiita" target="_blank" rel="noopener">2020年</a></li>
      <li><a href="http://qiita.com/advent-calendar/2019/future" title="フューチャー Advent Calendar 2019 #Qiita" target="_blank" rel="noopener">2019年</a></li>
      <li><a href="http://qiita.com/advent-calendar/2018/future" title="フューチャー Advent Calendar 2018 #Qiita" target="_blank" rel="noopener">2018年</a></li>
      <li><a href="http://qiita.com/advent-calendar/2017/future" title="フューチャー Advent Calendar 2017 #Qiita" target="_blank" rel="noopener">2017年</a></li>
      <li><a href="http://qiita.com/advent-calendar/2016/future" title="フューチャー Advent Calendar 2016 #Qiita" target="_blank" rel="noopener">2016年</a></li>
      <li><a href="http://qiita.com/advent-calendar/2015/future" title="フューチャー Advent Calendar 2015 #Qiita" target="_blank" rel="noopener">2015年</a></li>
    </ul>
  </div>
</div>

</section>
<!-- END SIDEBAR -->

  </aside>
</div>

  </section>
</div>

      <!-- BEGIN PRE-FOOTER -->
    <footer>
      <div class="pre-footer">
        <div class="container">
          <div class="row">
            <!-- BEGIN BOTTOM ABOUT BLOCK -->
            <div class="col-md-4 col-sm-6 pre-footer-col">
              <h2>About Us</h2>
              <p>経営とITをデザインする、フューチャーの技術ブログです。業務で利用している幅広い技術について紹介します。記事についてのお問い合わせはTwitterのDMで連絡いただけると幸いです。<br /><br /><a target="_blank" rel="noopener" href="http://www.future.co.jp/">http://www.future.co.jp/</a></p>
            </div>
            <!-- END BOTTOM ABOUT BLOCK -->
            <!-- BEGIN BOTTOM CONTACTS -->
            <div class="col-md-4 col-sm-6 pre-footer-col">
              <h2>Contact</h2>
              <address class="margin-bottom-40">
                東京都品川区大崎1-2-2<br>
                アートヴィレッジ大崎セントラルタワー<br><br>
                Email: <a href="mailto:techblog@future.co.jp">techblog@future.co.jp</a><br>
              </address>
            </div>
            <!-- END BOTTOM CONTACTS -->
            <!-- Twitter Share. ページ毎に1度初期化できれば良い -->
            <script>window.twttr = (function(d, s, id) {
              var js, fjs = d.getElementsByTagName(s)[0], t = window.twttr || {};
              if (d.getElementById(id)) return t;
              js = d.createElement(s);
              js.id = id;
              js.src = "https://platform.twitter.com/widgets.js";
              fjs.parentNode.insertBefore(js, fjs);
              t._e = [];
              t.ready = function(f) {
                t._e.push(f);
              };
              return t;
            }(document, "script", "twitter-wjs"));</script>
            
              <!-- BEGIN TWITTER BLOCK -->
              <div class="col-md-4 col-sm-6 pre-footer-col">
                <!-- Twitterフォローボタン -->
                <div class="social-btn twitter-btn twitter-follow-btn">
                  <a href="https://twitter.com/intent/follow?screen_name=future_techblog " target="_blank" rel="nofollow noopener">
                    <i></i><span class="tw-btn-label">@future_techblogさんをフォロー</span>
                  </a>
                </div>
                <!-- Twitterタイムライン -->
                <a data-tweet-limit="1" class="twitter-timeline" target="_blank" rel="noopener" href="https://twitter.com/future_techblog?ref_src=twsrc%5Etfw" data-lang="ja" data-dnt="true">Tweets by @future_techblog</a>
              </div>
              <!-- END TWITTER BLOCK -->
            
          </div>
        </div>
      </div>
      <!-- END PRE-FOOTER -->
      <!-- BEGIN FOOTER -->
      <div class="footer">
        <div class="container">
          <div class="row">
            <!-- BEGIN COPYRIGHT -->
            <div class="col-md-6 col-sm-6 padding-top-10">
              &copy; 2021 フューチャー技術ブログ<br>
            </div>
          </div>
        </div>
      </div>
    </footer>
    <!-- END FOOTER -->

  <!-- START INTEGRATIONS -->

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-74047147-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->


<!-- END INTEGRATIONS -->

  </div>
</body>
</html>
