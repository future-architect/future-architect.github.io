<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <!--
    ███████╗██╗░░░██╗████████╗██╗░░░██╗██████╗░███████╗
    ██╔════╝██║░░░██║╚══██╔══╝██║░░░██║██╔══██╗██╔════╝
    █████╗░░██║░░░██║░░░██║░░░██║░░░██║██████╔╝█████╗░░
    ██╔══╝░░██║░░░██║░░░██║░░░██║░░░██║██╔══██╗██╔══╝░░
    ██║░░░░░╚██████╔╝░░░██║░░░╚██████╔╝██║░░██║███████╗
    ╚═╝░░░░░░╚═════╝░░░░╚═╝░░░░╚═════╝░╚═╝░░╚═╝╚══════╝
    ████████╗███████╗░█████╗░██╗░░██╗
    ╚══██╔══╝██╔════╝██╔══██╗██║░░██║
    ░░░██║░░░█████╗░░██║░░╚═╝███████║
    ░░░██║░░░██╔══╝░░██║░░██╗██╔══██║
    ░░░██║░░░███████╗╚█████╔╝██║░░██║
    ░░░╚═╝░░░╚══════╝░╚════╝░╚═╝░░╚═╝
    ██████╗░██╗░░░░░░█████╗░░██████╗░
    ██╔══██╗██║░░░░░██╔══██╗██╔════╝░
    ██████╦╝██║░░░░░██║░░██║██║░░██╗░
    ██╔══██╗██║░░░░░██║░░██║██║░░╚██╗
    ██████╦╝███████╗╚█████╔╝╚██████╔╝
    ╚═════╝░╚══════╝░╚════╝░░╚═════╝░
    Welcome engineer.
    https://www.future.co.jp/recruit/
  -->
  
  <title>AWS Glueで複雑な処理を開発するときのTips | フューチャー技術ブログ</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  
  <meta name="description" content="はじめにこんにちは。TIGの藤田です。 Python連載 の8日目として、PySparkを使用したGlueジョブ開発のお話をします。 ETLツールとして使用されるAWS Glueですが、業務バッチで行うような複雑な処理も実行できます。また、処理はGlueジョブとして、Apache Spark分散・並列処理のジョブフローに簡単に乗せることができます！ 特に複雑な処理は、やや割高な開発エンドポイン">
<meta property="og:type" content="article">
<meta property="og:title" content="AWS Glueで複雑な処理を開発するときのTips | フューチャー技術ブログ">
<meta property="og:url" content="https://future-architect.github.io/articles/20211011a/index.html">
<meta property="og:site_name" content="フューチャー技術ブログ">
<meta property="og:description" content="はじめにこんにちは。TIGの藤田です。 Python連載 の8日目として、PySparkを使用したGlueジョブ開発のお話をします。 ETLツールとして使用されるAWS Glueですが、業務バッチで行うような複雑な処理も実行できます。また、処理はGlueジョブとして、Apache Spark分散・並列処理のジョブフローに簡単に乗せることができます！ 特に複雑な処理は、やや割高な開発エンドポイン">
<meta property="og:locale" content="ja_JP">
<meta property="og:image" content="https://future-architect.github.io/images/20211011a/glue_python_spark.png">
<meta property="article:published_time" content="2021-10-10T15:00:00.000Z">
<meta property="article:modified_time" content="2022-07-04T14:47:53.566Z">
<meta property="article:tag" content="AWS">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Glue">
<meta property="article:tag" content="Athena">
<meta property="article:tag" content="PySpark">
<meta property="article:tag" content="SparkSQL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://future-architect.github.io/images/20211011a/glue_python_spark.png">
  
  <link rel="alternate" href="/atom.xml" title="フューチャー技術ブログ" type="application/atom+xml">
  
  <link rel="icon" href="/favicon.ico">
  <link rel="apple-touch-icon" sizes='180x180' href="/apple-touch-icon.png">
  <link rel="apple-touch-icon" sizes='57x57' href="/apple-touch-icon-57x57.png">
  <link rel="canonical" href="https://future-architect.github.io/articles/20211011a/">
  <meta content="AWS,Python,Glue,Athena,PySpark,SparkSQL" name="keywords">
  <meta content="藤田春佳" name="author">
  <link rel="preload" as="image" href="/banner.jpg" />
  <link rel='manifest' href='/manifest.webmanifest'/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css" integrity="sha384-F3w7mX95PdgyTmZZMECAngseQB83DfGTowi0iMjiWaeVhAn4FJkqJByhZMI3AhiU" crossorigin="anonymous">
  <link rel="stylesheet" href="/metronic/assets/style.css">
  <link rel="stylesheet" href="/css/theme-styles.css">
<meta name="generator" content="Hexo 5.4.2"></head>

<body class="corporate">
  <div class="wrap" itemscope itemtype="https://schema.org/TechArticle">
  <!-- BEGIN HEADER -->
<header class="header">
	<div class="header-overlay">
		<div class="header-menu"></div>
		<div class="header-title"><a href="/">Future Tech Blog</a></div>
		<div class="header-title-sub">フューチャー技術ブログ</div>
	</div>
</header>
<!-- Header END -->

  <div class="container">
  <ul class="breadcrumb">
    <li><a href="/">Home</a></li>
    <li><a href="/articles/">Blog</a></li>
    <li class="active">Post</li>
  </ul>
  <section id="main" class="margin-top-30">
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Infrastructure/">Infrastructureカテゴリ</a>
  </div>


    <h2 itemprop="name" class="article-title">AWS Glueで複雑な処理を開発するときのTips
  
  <a target="_blank" rel="noopener" href="https://github.com/future-architect/tech-blog/edit/master/source/_posts/20211011a_AWS_Glueで複雑な処理を開発するときのTips.md" title="Suggest Edits" class="github-edit"><i class="github-edit-icon"></i></a>
  
</h2>

    <div class="row">
  <main class="col-md-9 blog-posts">
    <article id="post-20211011a_AWS_Glueで複雑な処理を開発するときのTips" class="article article-type-post blog-item" itemscope itemprop="blogPost">
      <div class="article-inner">
        
        <header class="article-header">
          <ul class="blog-info">
            <li class="blog-info-item"><a href="/articles/2021/" class="publish-date"><time datetime="2021-10-10T15:00:00.000Z" itemprop="datePublished">2021.10.11</time></a>
</li>
            <li class="blog-info-item"><li><a href="/authors/%E8%97%A4%E7%94%B0%E6%98%A5%E4%BD%B3" title="藤田春佳さんの記事一覧へ" class="post-author">藤田春佳</a></li></li>
            <li class="blog-info-item">
  
    
    <a href="/tags/AWS/" title="AWSタグの記事へ" class="tag-list-link">AWS</a>
  
    
    <a href="/tags/Python/" title="Pythonタグの記事へ" class="tag-list-link">Python</a>
  
    
    <a href="/tags/Glue/" title="Glueタグの記事へ" class="tag-list-link">Glue</a>
  
    
    <a href="/tags/Athena/" title="Athenaタグの記事へ" class="tag-list-link">Athena</a>
  
    
    <a href="/tags/PySpark/" title="PySparkタグの記事へ" class="tag-list-link">PySpark</a>
  
    
    <a href="/tags/SparkSQL/" title="SparkSQLタグの記事へ" class="tag-list-link">SparkSQL</a>
  

</li>
          </ul>
          </header>
        
        <div class="article-entry" itemprop="articleBody">
          
            <img src="/images/20211011a/glue_python_spark.png" alt="" width="790" height="260" loading="lazy">

<h2 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h2><p>こんにちは。TIGの藤田です。</p>
<p><a href="/articles/20210927b/">Python連載</a> の8日目として、PySparkを使用したGlueジョブ開発のお話をします。</p>
<p>ETLツールとして使用されるAWS Glueですが、業務バッチで行うような複雑な処理も実行できます。また、処理はGlueジョブとして、Apache Spark分散・並列処理のジョブフローに簡単に乗せることができます！</p>
<p>特に複雑な処理は、やや割高な開発エンドポイントは使用せず、ローカル端末で、しっかり開発・テストを行いたいですよね。そのためのローカル開発Tipsをご紹介します。</p>
<h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ol>
<li><a href="#Glue%E3%82%B8%E3%83%A7%E3%83%96%E3%81%AE%E9%96%8B%E7%99%BA%E3%81%A8%E5%AE%9F%E8%A1%8C%E6%A6%82%E8%A6%81">Glueジョブの開発と実行概要</a></li>
<li><a href="#Tip1-%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%AB%E7%92%B0%E5%A2%83%E6%A7%8B%E7%AF%89">Tip1: ローカル環境構築</a></li>
<li><a href="#Tip2-pyspark-sparksql%E9%96%8B%E7%99%BA">Tip2: PySpark, SparkSQL開発</a></li>
<li><a href="#Tip3-%E5%8D%98%E4%BD%93%E3%83%86%E3%82%B9%E3%83%88-pytest">Tip3: 単体テスト(pytest)</a></li>
<li><a href="#Tip4-%E3%83%87%E3%83%BC%E3%82%BF%E3%82%AB%E3%82%BF%E3%83%AD%E3%82%B0%E3%81%A9%E3%81%86%E3%81%99%E3%82%8B%E5%95%8F%E9%A1%8C">Tip4: データカタログどうする問題</a></li>
</ol>
<h2 id="Glueジョブの開発と実行概要"><a href="#Glueジョブの開発と実行概要" class="headerlink" title="Glueジョブの開発と実行概要"></a>Glueジョブの開発と実行概要</h2><p>ローカル開発の前に、AWS Glueでのジョブ実行方法を簡単にお話します。複雑な処理をSparkジョブで実行するには、以下4ステップでOKです。</p>
<p>１）ジョブスクリプトを作成、S3に配置<br>２）ジョブ実行定義<br>３）「ワークフロー」によるジョブフロー定義<br>４）AWS Athenaを使った実行結果確認</p>
<p>３）のジョブフロー定義については、規模や構成によって他の方法を検討する余地が大きいですが、Glueの「ワークフロー」でも、以下のような機能は用意されています。</p>
<p>・画面GUIでのジョブフロー定義<br>・ジョブの並列実行、分岐、待合せ<br>・オンディマンド、スケジュール、EventBridgeイベントによるトリガ実行<br>・画面からの実行状態、結果、エラー確認、リトライ実行</p>
<p>４）について、<a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/ja_jp/athena/latest/ug/what-is.html">Athena</a>は、標準的なSQLを使用してS3のデータを直接分析できるサービスです。Athenaのクエリ実行には、AWS Glueデータカタログ（DatabaseやTable）の登録が必要ですが、これはAthenaのクエリエディタにDDLを実行すると簡単に行えます。（Glueのデータカタログ定義はTerraform等でも行えるので運用上は他の方法でもよいと思います。）</p>
<h2 id="Tip1-ローカル環境構築"><a href="#Tip1-ローカル環境構築" class="headerlink" title="Tip1: ローカル環境構築"></a>Tip1: ローカル環境構築</h2><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/jp/blogs/big-data/developing-aws-glue-etl-jobs-locally-using-a-container/">AWS公式にGlueコンテナが配布</a>されて、docker-composeによる環境構築が容易になりました。ローカル環境構築の詳細は、<a href="/articles/20210521a/">AWS Glueの開発環境の構築(2021)</a>を参照ください。</p>
<h2 id="Tip2-PySpark-SparkSQL開発"><a href="#Tip2-PySpark-SparkSQL開発" class="headerlink" title="Tip2: PySpark, SparkSQL開発"></a>Tip2: PySpark, SparkSQL開発</h2><p>Glueでは、3つのジョブタイプ、Python shell, Spark streaming, Spark script （Python, Scala）が選択できますが、今回はSpark script（PySpark, SparkSQL）を採用しました。<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/python/index.html">PySpark</a>は、<a target="_blank" rel="noopener" href="http://spark.apache.org/">Apache Spark</a>をPythonで呼出すライブラリです。<a target="_blank" rel="noopener" href="https://spark.apache.org/sql/">SparkSQL</a>は、Apache Sparkのモジュールの1つで、SQLとDataFrameによる構造化データの処理を可能にします。</p>
<p>複雑な業務処理の実装にも以下のメリットがありました。</p>
<ul>
<li>構造化データ（Table）をメモリ上のDataFrameに取込み効率的に加工できる。</li>
<li>データカタログ（Table定義）があれば、プログラム上データ取込用のモデル定義を別につくる必要がない。</li>
<li>SparkSQLにより、複数ファイル（Table）の結合を含む、<a target="_blank" rel="noopener" href="https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html">標準的なSQLによる操作が可能</a>。</li>
<li>SQL関数に含まれないPythonの関数やライブラリを使いたい場合にも、ユーザー定義関数 （UDF）を使えば、DataFrameの構造を維持したまま、特定のカラムに対してのみ処理を実行できる。</li>
</ul>
<p>以下、2ファイル(2 Tables)を結合してユーザー定義関数処理をするスクリプト例です。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> pyspark.context <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> awsglue.context <span class="keyword">import</span> GlueContext</span><br><span class="line"><span class="keyword">from</span> awsglue.utils <span class="keyword">import</span> getResolvedOptions</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> DecimalType</span><br><span class="line"><span class="keyword">from</span> decimal <span class="keyword">import</span> Decimal, ROUND_FLOOR, ROUND_HALF_UP, ROUND_CEILING</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">round_fraction</span>(<span class="params">target: Decimal, meth: <span class="built_in">str</span>, pos: <span class="built_in">str</span></span>):</span><br><span class="line">    p = &#123;</span><br><span class="line">        <span class="string">&quot;1&quot;</span>: <span class="string">&quot;1.&quot;</span>,</span><br><span class="line">        <span class="string">&quot;2&quot;</span>: <span class="string">&quot;0.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;3&quot;</span>: <span class="string">&quot;0.01&quot;</span>,</span><br><span class="line">        <span class="string">&quot;4&quot;</span>: <span class="string">&quot;0.001&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    methods = &#123;</span><br><span class="line">        <span class="string">&quot;1&quot;</span>: ROUND_FLOOR,</span><br><span class="line">        <span class="string">&quot;2&quot;</span>: ROUND_HALF_UP,</span><br><span class="line">        <span class="string">&quot;3&quot;</span>: ROUND_CEILING,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> meth <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> pos <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> meth == <span class="string">&quot;&quot;</span> <span class="keyword">or</span> pos == <span class="string">&quot;&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> target</span><br><span class="line">    <span class="keyword">return</span> target.quantize(Decimal(p[pos]), rounding=methods[meth])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">udf_round_fraction = F.udf(round_fraction, DecimalType(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">exec_sample</span>(<span class="params">glueContext, spark, input_dir, output_dir</span>):</span><br><span class="line">    data = [<span class="string">&quot;calc_source&quot;</span>, <span class="string">&quot;attributes&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        p = <span class="string">f&quot;s3://<span class="subst">&#123;input_dir&#125;</span>/<span class="subst">&#123;d&#125;</span>/&quot;</span></span><br><span class="line">        <span class="comment"># Sample to convert DynamicFrame to DataFrame then create temporary view</span></span><br><span class="line">        glueContext.create_dynamic_frame.from_options(</span><br><span class="line">            connection_type=<span class="string">&quot;s3&quot;</span>,</span><br><span class="line">            connection_options=&#123;<span class="string">&quot;paths&quot;</span>: [p]&#125;,</span><br><span class="line">            <span class="built_in">format</span>=<span class="string">&quot;parquet&quot;</span>,</span><br><span class="line">        ).toDF().createOrReplaceTempView(d)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SQL sample to join two tables</span></span><br><span class="line">    wk_main = spark.sql(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        SELECT</span></span><br><span class="line"><span class="string">            src.id</span></span><br><span class="line"><span class="string">        ,	src.number1</span></span><br><span class="line"><span class="string">        ,	src.number2</span></span><br><span class="line"><span class="string">        ,	src.position</span></span><br><span class="line"><span class="string">        ,	src.method</span></span><br><span class="line"><span class="string">        ,	src.group</span></span><br><span class="line"><span class="string">        ,	att.attribute1</span></span><br><span class="line"><span class="string">        FROM</span></span><br><span class="line"><span class="string">            calc_source	src</span></span><br><span class="line"><span class="string">        INNER JOIN</span></span><br><span class="line"><span class="string">            attributes	att</span></span><br><span class="line"><span class="string">        ON</span></span><br><span class="line"><span class="string">            src.group	=	att.group</span></span><br><span class="line"><span class="string">        WHERE</span></span><br><span class="line"><span class="string">            src.number1	IS	NOT NULL</span></span><br><span class="line"><span class="string">        AND	src.number2	IS	NOT NULL</span></span><br><span class="line"><span class="string">        AND	src.number2	&lt;&gt;	0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># UDF sample to calculate fractions</span></span><br><span class="line">    wk_main = wk_main.withColumn(</span><br><span class="line">        <span class="string">&quot;calc_result&quot;</span>,</span><br><span class="line">        udf_round_fraction(</span><br><span class="line">            F.col(<span class="string">&quot;number1&quot;</span>) / F.col(<span class="string">&quot;number2&quot;</span>),</span><br><span class="line">            F.col(<span class="string">&quot;method&quot;</span>),</span><br><span class="line">            F.col(<span class="string">&quot;position&quot;</span>))</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    wk_main.write.mode(<span class="string">&quot;overwrite&quot;</span>).<span class="built_in">format</span>(<span class="string">&quot;parquet&quot;</span>).save(<span class="string">f&quot;s3://<span class="subst">&#123;output_dir&#125;</span>/sample_out/&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    args = getResolvedOptions(sys.argv, [<span class="string">&quot;input_dir&quot;</span>, <span class="string">&quot;output_dir&quot;</span>])</span><br><span class="line">    glueContext = GlueContext(SparkContext.getOrCreate())</span><br><span class="line">    spark = glueContext.spark_session</span><br><span class="line">    <span class="comment"># Exec</span></span><br><span class="line">    exec_sample(glueContext, spark, args[<span class="string">&quot;input_dir&quot;</span>], args[<span class="string">&quot;output_dir&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="Tip3-単体テスト-pytest"><a href="#Tip3-単体テスト-pytest" class="headerlink" title="Tip3: 単体テスト(pytest)"></a>Tip3: 単体テスト(pytest)</h2><p>ローカル環境での、PySparkスクリプトの単体テストは<a target="_blank" rel="noopener" href="https://github.com/pytest-dev/pytest/">pytest</a>で可能です。方法は<a href="/articles/20191206/">AWS Glueの単体テスト環境の構築手順</a>を参照ください。実行結果のアサーションをファイル単位で行う場合は、DataFrameを比較できるツール（<a target="_blank" rel="noopener" href="https://github.com/MrPowers/chispa">chispa</a>など）を利用すると便利です。</p>
<h2 id="Tip4-データカタログどうする問題"><a href="#Tip4-データカタログどうする問題" class="headerlink" title="Tip4: データカタログどうする問題"></a>Tip4: データカタログどうする問題</h2><p>データカタログは、データのファイルシステムをDatabaseとTableのように定義して管理する<a target="_blank" rel="noopener" href="https://hive.apache.org/index.html">Hive</a>メタストア同様の機能を担っています。</p>
<p>データカタログは、上記Glueコンテナのディフォルト設定では呼出すことができず、CSVファイルを読込む際にデータ型定義ができない課題がありました。</p>
<p>CSVファイルをDataFrameに読込む際に、schema定義をかいてやることはできますが、ローカル環境でしか使わないコードを、対象データのカラムすべてに対して書くのは嬉しくありません。AWS環境のGlueデータカタログの定義と二重管理にもなります。そこで、2パタンの解決策をご紹介します。</p>
<ul>
<li>Tip4-1. AWS環境に接続してGlueデータカタログを使用する</li>
<li>Tip4-2. CSVではなく、Parquetファイルを使う</li>
</ul>
<h3 id="Tip4-1-AWS環境に接続してGlueデータカタログを使用する"><a href="#Tip4-1-AWS環境に接続してGlueデータカタログを使用する" class="headerlink" title="Tip4-1. AWS環境に接続してGlueデータカタログを使用する"></a>Tip4-1. AWS環境に接続してGlueデータカタログを使用する</h3><p>AWSアカウントの使える状態であれば、AWS環境のS3からGlueデータカタログを使用してファイルを読込むのが楽です。ローカル環境のGlueコンテナ内から、以下のようなコードが実行できます。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.context <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> awsglue.context <span class="keyword">import</span> GlueContext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">glueContext = GlueContext(SparkContext.getOrCreate())</span><br><span class="line">df = glueContext.create_dynamic_frame.from_catalog(</span><br><span class="line">    database=<span class="string">&quot;sampledb&quot;</span>,</span><br><span class="line">    table_name=<span class="string">&quot;departuredelays&quot;</span>,</span><br><span class="line">    push_down_predicate=<span class="string">&quot;(any==&#x27;sample&#x27;)&quot;</span>,</span><br><span class="line">).toDF()</span><br><span class="line"></span><br><span class="line">df.write.mode(<span class="string">&quot;overwrite&quot;</span>).<span class="built_in">format</span>(<span class="string">&quot;parquet&quot;</span>).save(</span><br><span class="line">    <span class="string">&quot;s3://bucket_name/departuredelays_out/any=sample/&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>このスクリプト実行のためには、DatabaseとTable定義を予めGlueデータカタログに登録しておく必要があります。Athenaから登録するには以下のようなDDLを使用します。読込みファイルがCSVの場合です。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE sampledb</span><br><span class="line">  LOCATION <span class="string">&#x27;s3://bucket_name/&#x27;</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> sampledb.departuredelays (</span><br><span class="line">  `<span class="type">date</span>` string,</span><br><span class="line">  `delay` <span class="type">int</span>,</span><br><span class="line">  `distance` <span class="type">int</span>,</span><br><span class="line">  `origin` string,</span><br><span class="line">  `destination` string)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (</span><br><span class="line">  `<span class="keyword">any</span>` string)</span><br><span class="line"><span class="type">ROW</span> FORMAT SERDE</span><br><span class="line">  <span class="string">&#x27;org.apache.hadoop.hive.serde2.OpenCSVSerde&#x27;</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">&#x27;separatorChar&#x27;</span><span class="operator">=</span><span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;escapeChar&#x27;</span><span class="operator">=</span><span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;quoteChar&#x27;</span><span class="operator">=</span><span class="string">&#x27;\&quot;&#x27;</span>)</span><br><span class="line">STORED <span class="keyword">AS</span> INPUTFORMAT</span><br><span class="line">  <span class="string">&#x27;org.apache.hadoop.mapred.TextInputFormat&#x27;</span></span><br><span class="line">OUTPUTFORMAT</span><br><span class="line">  <span class="string">&#x27;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#x27;</span></span><br><span class="line">LOCATION</span><br><span class="line">  <span class="string">&#x27;s3://bucket_name/departuredelays&#x27;</span></span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  <span class="string">&#x27;has_encrypted_data&#x27;</span><span class="operator">=</span><span class="string">&#x27;false&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;skip.header.line.count&#x27;</span><span class="operator">=</span><span class="string">&#x27;1&#x27;</span></span><br><span class="line">  )</span><br><span class="line">;</span><br></pre></td></tr></table></figure>

<p>おまけですが、出力結果をAthenaから確認するためには、出力ディレクトリのTable定義を登録します。今回出力ファイルはParquetなので、DDLは以下のようになります。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> sampledb.departuredelays_out (</span><br><span class="line">  `<span class="type">date</span>` string,</span><br><span class="line">  `delay` <span class="type">int</span>,</span><br><span class="line">  `distance` <span class="type">int</span>,</span><br><span class="line">  `origin` string,</span><br><span class="line">  `destination` string)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (</span><br><span class="line">  `<span class="keyword">any</span>` string)</span><br><span class="line"><span class="type">ROW</span> FORMAT SERDE</span><br><span class="line">  <span class="string">&#x27;org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe&#x27;</span></span><br><span class="line">STORED <span class="keyword">AS</span> INPUTFORMAT</span><br><span class="line">  <span class="string">&#x27;org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat&#x27;</span></span><br><span class="line">OUTPUTFORMAT</span><br><span class="line">  <span class="string">&#x27;org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat&#x27;</span></span><br><span class="line">LOCATION</span><br><span class="line">  <span class="string">&#x27;s3://bucket_name/departuredelays_out&#x27;</span></span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  <span class="string">&#x27;has_encrypted_data&#x27;</span><span class="operator">=</span><span class="string">&#x27;false&#x27;</span></span><br><span class="line">  )</span><br><span class="line">;</span><br></pre></td></tr></table></figure>

</br>
</br>

<h3 id="Tip4-2-CSVではなく、Parquetファイルを使う"><a href="#Tip4-2-CSVではなく、Parquetファイルを使う" class="headerlink" title="Tip4-2. CSVではなく、Parquetファイルを使う"></a>Tip4-2. CSVではなく、Parquetファイルを使う</h3><p>AWS環境の使えない状態でも、ファイルをParquetフォーマットで作成できれば、型の保存された状態で読込ができます。Parquetは、CSVよりも保存性や読書き性能の面で有利です（<a target="_blank" rel="noopener" href="https://databricks.com/jp/glossary/what-is-parquet">Apache Parquetについて</a>）。</p>
<p>Parquetファイルは直接開いて中が確認できないですが、上記のようにAthenaで確認できますし、ローカル環境でも、Jupyter Notebook上でDataFrameに読込んでschema表示・データ表示できます。</p>
<h2 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h2><p>AWS Glueで複雑な処理を開発するときのTipsをご紹介しました。複雑なロジック開発とテストにAWS Glue環境を用いるのは費用面で不利なので、ぜひローカル環境を活用したいところです。特にファイルIOについては、ローカル環境とAWS環境で同じコードで処理できるようにするのがポイントだと思います。Glueジョブ開発の一助になれば幸いです。</p>
<h2 id="参考リンク"><a href="#参考リンク" class="headerlink" title="参考リンク"></a>参考リンク</h2><ul>
<li><a href="/articles/20211006a/">AWS Glue Data CatalogでCSVを扱う - フューチャー技術ブログ</a><ul>
<li>AWS環境で、Glueデータカタログを使ってCSVファイルを扱う方法が紹介されています。</li>
</ul>
</li>
</ul>

          
        </div>
        <footer>
          <section class="social-area">
          <!-- シェアボタン START -->
  <ul class="social-button">
    
    <!-- Twitter -->
    <li>
      <a class="social-btn twitter-btn" target="_blank" href="https://twitter.com/share?url=https://future-architect.github.io/articles/20211011a/&related=twitterapi%2Ctwitter&text=AWS%20Glue%E3%81%A7%E8%A4%87%E9%9B%91%E3%81%AA%E5%87%A6%E7%90%86%E3%82%92%E9%96%8B%E7%99%BA%E3%81%99%E3%82%8B%E3%81%A8%E3%81%8D%E3%81%AETips%20%7C%20%E3%83%95%E3%83%A5%E3%83%BC%E3%83%81%E3%83%A3%E3%83%BC%E6%8A%80%E8%A1%93%E3%83%96%E3%83%AD%E3%82%B0" rel="nofollow noopener">
        <i></i><span class="social-btn-label">1</span>
      </a>
    </li>
    <!-- Facebook -->
    <li>
      <a class="social-btn fb-btn" target="_blank" href="http://www.facebook.com/share.php?u=https://future-architect.github.io/articles/20211011a/&t=AWS%20Glue%E3%81%A7%E8%A4%87%E9%9B%91%E3%81%AA%E5%87%A6%E7%90%86%E3%82%92%E9%96%8B%E7%99%BA%E3%81%99%E3%82%8B%E3%81%A8%E3%81%8D%E3%81%AETips" rel="nofollow noopener">
        <i></i><span class="social-btn-label">シェア</span>
      </a>
    </li>
    <!-- hatebu -->
    <li>
      <a class="social-btn hatebu-btn" target="_blank" href="https://b.hatena.ne.jp/entry/s/future-architect.github.io/articles/20211011a/" rel="nofollow noopener">
        <i></i><span class="social-btn-label">5</span>
      </a>
    </li>
    <!-- pocket -->
    <li>
      <a class="social-btn pocket-btn" target="_blank" href="https://getpocket.com/save?url=https://future-architect.github.io/articles/20211011a/" rel="nofollow noopener">
        <i></i><span class="social-btn-label">7</span>
      </a>
    </li>
    
  </ul>
<!-- シェアボタン END -->

          </section>
          <aside>
            <section class="related-post margin-bottom-40 nav">
              <h2 id="related"><a href="#related" class="headerlink" title="関連記事"></a>関連記事</h2>
              
  <div class="widget">
    <ul class="nav related-post-link"><li class="related-posts-item"><span>2021.10.06</span><span class="snscount">&#9825;7</span><a href=/articles/20211006a/ title="PySparkで予定しておりましたが、PySpark関連として、Glueを題材にさせていただきます。Glueといっても大きく下記の３種類、処理系をいれると4種類に分かれると思っていますが、それぞれ全く別のプロダクトという理解をしています。">AWS Glue Data CatalogでCSVを扱う</a></li><li class="related-posts-item"><span>2021.04.03</span><span class="snscount">&#9825;11</span><a href=/articles/20210403/ title="DynamoDBを頻繁に利用しており、連日DynamoDBコンソール画面と睨めっこをしています。DynamoDBのコンソール画面は特定のデータをピンポイントで探すには優秀ですが、データ集計には全く向いていません。">DynamoDBをS3へExportして、Glue+AthenaでSQLを実行する</a></li><li class="related-posts-item"><span>2019.12.06</span><span class="snscount">&#9825;10</span><a href=/articles/20191206/ title="当記事では、AWS Glue をローカル環境で単体テストするための環境構築方法についてまとめました。">AWS Glueの単体テスト環境の構築手順</a></li><li class="related-posts-item"><span>2022.04.28</span><span class="snscount">&#9825;5</span><a href=/articles/20220428a/ title="2021年の記事でもAWSの公式のDockerイメージを使って環境構築をする内容の記事があるのですが、Glue3.0の公式のDockerイメージがリリースされていたので、そちらを使って再度Glueのローカルでの開発環境構築の記事を書いてみようと思います。せっかくなので昨年の記事と少しコードを変えようと思い、AWSの公式ドキュメント[^2]に書かれたコードを基に解説します。">AWS Glueの開発環境の構築(2022)</a></li><li class="related-posts-item"><span>2021.05.21</span><span class="snscount">&#9825;15</span><a href=/articles/20210521a/ title="Glueの環境構築は以前の記事（[AWS Glueの単体テスト環境の構築手順、AWS Glueの開発エンドポイントがそこそこお高いのでローカル開発環境を用意しました）にあるのですが、公式のDocker imageが案内されているので改めて、構築してみます。なお、Glueの公式イメージでもJupyter Notebookは利用できるのですが、使い勝手を考慮し、Jupyterlabに差し替えています。">AWS Glueの開発環境の構築(2021)</a></li><li class="related-posts-item"><span>2020.12.06</span><span class="snscount">&#9825;6</span><a href=/articles/20201206/ title="AWS マネージド Airflow  が 2020/11/24 にリリースされました。 [Introducing Amazon Managed Workflows for Apache Airflow サービスを利用するにあたって知りたかったことを調査し、FAQ ベースで整理しましたので公開します。">AWSマネージドAirflow(MWAA)についてのFAQ</a></li></ul>
  </div>
            </section>
            <section class="reference-post margin-bottom-40 nav">
              
            </section>
          </aside>
        </footer>
      </div>
    </article>
  </main>
  <aside class="col-md-3 blog-sidebar">
    <!-- START SIDEBAR  -->


<section class="toc-section">
  <h2 class="margin-top-30">目次</h2>
  <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"><span class="toc-text">はじめに</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E5%AE%B9"><span class="toc-text">内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Glue%E3%82%B8%E3%83%A7%E3%83%96%E3%81%AE%E9%96%8B%E7%99%BA%E3%81%A8%E5%AE%9F%E8%A1%8C%E6%A6%82%E8%A6%81"><span class="toc-text">Glueジョブの開発と実行概要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tip1-%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%AB%E7%92%B0%E5%A2%83%E6%A7%8B%E7%AF%89"><span class="toc-text">Tip1: ローカル環境構築</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tip2-PySpark-SparkSQL%E9%96%8B%E7%99%BA"><span class="toc-text">Tip2: PySpark, SparkSQL開発</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tip3-%E5%8D%98%E4%BD%93%E3%83%86%E3%82%B9%E3%83%88-pytest"><span class="toc-text">Tip3: 単体テスト(pytest)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tip4-%E3%83%87%E3%83%BC%E3%82%BF%E3%82%AB%E3%82%BF%E3%83%AD%E3%82%B0%E3%81%A9%E3%81%86%E3%81%99%E3%82%8B%E5%95%8F%E9%A1%8C"><span class="toc-text">Tip4: データカタログどうする問題</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tip4-1-AWS%E7%92%B0%E5%A2%83%E3%81%AB%E6%8E%A5%E7%B6%9A%E3%81%97%E3%81%A6Glue%E3%83%87%E3%83%BC%E3%82%BF%E3%82%AB%E3%82%BF%E3%83%AD%E3%82%B0%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B"><span class="toc-text">Tip4-1. AWS環境に接続してGlueデータカタログを使用する</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tip4-2-CSV%E3%81%A7%E3%81%AF%E3%81%AA%E3%81%8F%E3%80%81Parquet%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E4%BD%BF%E3%81%86"><span class="toc-text">Tip4-2. CSVではなく、Parquetファイルを使う</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%81%BE%E3%81%A8%E3%82%81"><span class="toc-text">まとめ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E3%83%AA%E3%83%B3%E3%82%AF"><span class="toc-text">参考リンク</span></a></li></ol>
</section>

<section class="category">
<h2 class="margin-top-30">カテゴリー</h2>
<div class="widget">
  <ul class="nav sidebar-categories margin-bottom-40">
  
  <li class=""><a href="/categories/Programming/">Programming (342)</a></li>
<li class=""><a href="/categories/Infrastructure/">Infrastructure (198)</a></li>
<li class=""><a href="/categories/Culture/">Culture (84)</a></li>
<li class=""><a href="/categories/DataScience/">DataScience (44)</a></li>
<li class=""><a href="/categories/IoT/">IoT (31)</a></li>
<li class=""><a href="/categories/DB/">DB (20)</a></li>
<li class=""><a href="/categories/Business/">Business (19)</a></li>
<li class=""><a href="/categories/%E8%AA%8D%E8%A8%BC%E8%AA%8D%E5%8F%AF/">認証認可 (18)</a></li>
<li class=""><a href="/categories/Management/">Management (13)</a></li>
<li class=""><a href="/categories/VR/">VR (12)</a></li>
<li class=""><a href="/categories/DevOps/">DevOps (12)</a></li>
<li class=""><a href="/categories/Design/">Design (11)</a></li>
<li class=""><a href="/categories/Security/">Security (6)</a></li>

  </ul>
</div>

</section>
<section class="podcast-link">
<h2 class="margin-top-30">Tech Cast</h2>

  <div class="class="widget-wrap">
  <div class="widget">
    <ul class="nav techcast">
      <li><a href="https://anchor.fm/futuretechcast/episodes/35-MLOps-e1qe4st" title="フューチャーがお届けするポッドキャストです。#35 MLOpsエンジニアって何やるの？（後編）" target="_blank" rel="noopener"><span class="newitem">NEW</span> #35 MLOpsエンジニアって何やるの？（後編）</a></li>
<li><a href="https://anchor.fm/futuretechcast/episodes/34-MLOps-e1polbj" title="フューチャーがお届けするポッドキャストです。#34 MLOpsエンジニアって何やるの？（前編）" target="_blank" rel="noopener"> #34 MLOpsエンジニアって何やるの？（前編）</a></li>
<li><a href="https://anchor.fm/futuretechcast/episodes/33-IT-e1pcaon" title="フューチャーがお届けするポッドキャストです。#33 ヘルスケアグループリーダーの中元さんと語る「医療業界におけるITコンサルとビジネスイノベーション」（後編）" target="_blank" rel="noopener"> #33 ヘルスケアグループリーダーの中元さんと語る「医療業界におけるITコンサルとビジネスイノベーション」（後編）</a></li>
    </ul>
  </div>
  </div>
  
</section>
<section class="advent-calendar">
<h2 class="margin-top-30">アドベントカレンダー</h2>
<div class="widget">
  <ul class="nav-flex">
    <li><a href="http://qiita.com/advent-calendar/2022/future" title="フューチャー Advent Calendar 2022 #Qiita" target="_blank" rel="noopener">2022年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2021/future" title="フューチャー Advent Calendar 2021 #Qiita" target="_blank" rel="noopener">2021年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2020/future" title="フューチャー Advent Calendar 2020 #Qiita" target="_blank" rel="noopener">2020年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2019/future" title="フューチャー Advent Calendar 2019 #Qiita" target="_blank" rel="noopener">2019年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2018/future" title="フューチャー Advent Calendar 2018 #Qiita" target="_blank" rel="noopener">2018年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2017/future" title="フューチャー Advent Calendar 2017 #Qiita" target="_blank" rel="noopener">2017年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2016/future" title="フューチャー Advent Calendar 2016 #Qiita" target="_blank" rel="noopener">2016年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2015/future" title="フューチャー Advent Calendar 2015 #Qiita" target="_blank" rel="noopener">2015年</a></li>
  </ul>
</div>

</section>
<!-- END SIDEBAR -->

  </aside>
</div>

  </section>
</div>

      <!-- BEGIN PRE-FOOTER -->
    <footer>
      <div class="pre-footer">
        <div class="container">
          <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-6 col-6 pre-footer-col">
              <h2>About Us</h2>
              <p>経営とITをデザインする、フューチャーの技術ブログです。業務で利用している幅広い技術について紹介します。<br /><br /><a target="_blank" rel="noopener" href="http://www.future.co.jp/">http://www.future.co.jp/</a></p>
              <div class="social-btn twitter-btn twitter-follow-btn">
                <a href="https://twitter.com/intent/follow?screen_name=future_techblog " target="_blank" rel="nofollow noopener">
                  <i></i><span class="tw-btn-label">フューチャー技術ブログをフォロー</span>
                </a>
              </div>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-4 col-4 pre-footer-col">
              <h2>Contact</h2>
              <address class="margin-bottom-40">
                <a href="https://www.future.co.jp/recruit/recruit/rec-fresh/" title="新卒採用" target="_blank" rel="noopener">新卒採用</a><br>
                <a href="https://www.future.co.jp/recruit/recruit/rec-career/" title="キャリア採用" target="_blank" rel="noopener">キャリア採用</a><br>
                <a href="https://www.future.co.jp/contact_us/" title="お問い合わせページ" target="_blank" rel="noopener">お問い合わせ</a><br>
                <a href="https://www.future.co.jp/architect/socialmediapolicy/" title="ソーシャルメディアポリシー" target="_blank" rel="noopener">メディアポリシー</a><br><br>
                <a href="mailto:techblog@future.co.jp">techblog@future.co.jp</a>
              </address>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6 col-6 pre-footer-col">
              <h2>Contents</h2>
              <a href="https://future-architect.github.io/coding-standards/" title="Future Enterprise Coding Standards" target="_blank" rel="noopener">コーディング規約</a><br>
              <a href="https://future-architect.github.io/typescript-guide/" title="仕事ですぐに使えるTypeScript" target="_blank" rel="noopener">仕事ですぐに使えるTypeScript</a><br>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-3 col-3 pre-footer-col">
              <h2>Event</h2>
              <a href="https://future.connpass.com/" title="経営とITをデザインするフューチャーの勉強会です" target="_blank" rel="noopener">connpass</a><br>
              <a href="https://www.future.co.jp/futureinsightseminar/" title="フューチャーインサイトセミナー" target="_blank" rel="noopener">Webセミナー</a><br>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-3 col-3 pre-footer-col">
              <h2>SNS</h2>
              <a href="https://github.com/future-architect" title="Future's official open source repositories" target="_blank" rel="noopener">GitHub</a><br>
              <a href="https://qiita.com/organizations/future" title="フューチャーのQiita Organizationです" target="_blank" rel="noopener">Qiita</a><br>
              <a href="https://note.future.co.jp/" title="フューチャーの公式note" target="_blank" rel="noopener">未来報</a><br>
              <a href="https://www.youtube.com/channel/UCJUSwYYd0CkGgmEKAW7QVpw" title="フューチャーYoutubeチャネル" target="_blank" rel="noopener">Youtube</a>
            </div>
          </div>
        </div>
      </div>
      <div class="footer">
        <div class="container">
          <div class="row">
            <div class="col-md-6 col-sm-6 padding-top-10">
              &copy; 2022 フューチャー技術ブログ<br>
            </div>
          </div>
        </div>
      </div>
    </footer>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-X1C28R8H0M"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-X1C28R8H0M');
  gtag('config', 'UA-74047147-1'); // 過渡期対応
</script>

  </div>
</body>
</html>
